
@inproceedings{10.1145/2695664.2695902,
  title = {A Method to Support Search String Building in Systematic Literature Reviews through Visual Text Mining},
  booktitle = {Proceedings of the 30th Annual {{ACM}} Symposium on Applied Computing},
  author = {Mergel, Germano Duarte and Silveira, Milene Selbach and da Silva, Tiago Silva},
  date = {2015},
  pages = {1594--1601},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2695664.2695902},
  url = {https://doi.org/10.1145/2695664.2695902},
  isbn = {978-1-4503-3196-8},
  keywords = {information visualization,systematic literature review,visual text mining},
  numpages = {8},
  options = {useprefix=true},
  series = {{{SAC}} '15}
}

@inproceedings{10.1145/2915970.2916013,
  title = {Improvements in the {{StArt}} Tool to Better Support the Systematic Review Process},
  booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  author = {Fabbri, Sandra and Silva, Cleiton and Hernandes, Elis and Octaviano, F\'abio and Di Thommazo, Andr\'e and Belgamo, Anderson},
  date = {2016},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2915970.2916013},
  url = {https://doi.org/10.1145/2915970.2916013},
  articleno = {Article 21},
  isbn = {978-1-4503-3691-8},
  keywords = {evidence-based software engineering,StArt tool,systematic literature review,systematic review,tool support},
  numpages = {5},
  series = {{{EASE}} '16}
}

@software{2014,
  title = {Review {{Manager}} ({{RevMan}})},
  date = {2014},
  location = {{Copenhagen: The Nordic Cochrane Centre}},
  organization = {{The Cochrane Collaboration}},
  version = {5.3}
}

@online{2019,
  title = {How Does Machine Learning Work at Sysrev?},
  date = {2019-09-15T22:21:17.000Z},
  journaltitle = {Sysrev Blog},
  url = {http://blog.sysrev.com/machine-learning/},
  urldate = {2020-01-16},
  abstract = {Sysrev automatically builds machine learning models at every stage of ~review.

In each sysrev users can screen articles in their corpus by marking them as an
'include' or 'exclude'. ~While reviewing a sysrev screening model is silently
learning how to replicate reviewer decisions. ~Screening models can help
accelerate the review process and eventually automate reviews. ~Automated
reviews will be a key step in the updateable 'living' reviews talked that will
be the next frontier for document rev},
  file = {/Users/gerbrich/Zotero/storage/IS5JUAHZ/machine-learning.html},
  langid = {english}
}

@incollection{Aggarwal2012,
  title = {A {{Survey}} of {{Text Classification Algorithms}}},
  booktitle = {Mining {{Text Data}}},
  author = {Aggarwal, Charu C. and Zhai, ChengXiang},
  editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
  date = {2012},
  pages = {163--222},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4614-3223-4_6},
  url = {http://link.springer.com/10.1007/978-1-4614-3223-4_6},
  urldate = {2020-05-02},
  file = {/Users/gerbrich/Zotero/storage/NNF6BXHE/Aggarwal and Zhai - 2012 - A Survey of Text Classification Algorithms.pdf},
  isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
  langid = {english}
}

@article{Aphinyanaphongs2004,
  title = {Text {{Categorization Models}} for {{High}}-{{Quality Article Retrieval}} in {{Internal Medicine}}},
  author = {Aphinyanaphongs, Y.},
  date = {2004-11-23},
  journaltitle = {J Am Med Inform Assoc},
  volume = {12},
  pages = {207--216},
  issn = {1067-5027, 1527-974X},
  doi = {10/cpvp52},
  url = {https://academic.oup.com/jamia/article-lookup/doi/10.1197/jamia.M1641},
  urldate = {2020-05-02},
  abstract = {Measurements: The machine learning models were compared in each category with each other and with the clinical query filters using area under the receiver operating characteristic curves, 11-point average recall precision, and a sensitivity/specificity match method.
Results: In most categories, the data-induced models have better or comparable sensitivity, specificity, and precision than the clinical query filters. The polynomial support vector machine models perform the best among all learning methods in ranking the articles as evaluated by area under the receiver operating curve and 11-point average recall precision.
Conclusion: This research shows that, using machine learning methods, it is possible to automatically build models for retrieving high-quality, content-specific articles using inclusion or citation by the ACP Journal Club as a gold standard in a given time period in internal medicine that perform better than the 1994 PubMed clinical query filters.},
  file = {/Users/gerbrich/Zotero/storage/HG2Q28HD/Aphinyanaphongs - 2004 - Text Categorization Models for High-Quality Articl.pdf},
  langid = {english},
  number = {2}
}

@article{Appenzeller-Herzog2019,
  title = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-Analysis of Controlled Studies},
  shorttitle = {Comparative Effectiveness of Common Therapies for {{Wilson}} Disease},
  author = {Appenzeller-Herzog, Christian and Mathes, Tim and Heeres, Marlies L. S. and Weiss, Karl Heinz and Houwen, Roderick H. J. and Ewald, Hannah},
  date = {2019},
  journaltitle = {Liver Int.},
  volume = {39},
  pages = {2136--2152},
  issn = {1478-3231},
  doi = {10.1111/liv.14179},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/liv.14179},
  urldate = {2020-02-20},
  abstract = {Background \& aims Wilson disease (WD) is a rare disorder of copper metabolism. The objective of this systematic review was to determine the comparative effectiveness and safety of common treatments of WD. Methods We included WD patients of any age or stage and the study drugs D-penicillamine, zinc salts, trientine and tetrathiomolybdate. The control could be placebo, no treatment or any other treatment. We included prospective, retrospective, randomized and non-randomized studies. We searched Medline and Embase via Ovid, the Cochrane Central Register of Controlled Trials, and screened reference lists of included articles. Where possible, we applied random-effects meta-analyses. Results The 23 included studies reported on 2055 patients and mostly compared D-penicillamine to no treatment, zinc, trientine or succimer. One study compared tetrathiomolybdate and trientine. Post-decoppering maintenance therapy was addressed in one study only. Eleven of 23 studies were of low quality. When compared to no treatment, D-penicillamine was associated with a lower mortality (odds ratio 0.013; 95\% CI 0.0010 to 0.17). When compared to zinc, there was no association with mortality (odds ratio 0.73; 95\% CI 0.16 to 3.40) and prevention or amelioration of clinical symptoms (odds ratio 0.84; 95\% CI 0.48 to 1.48). Conversely, D-penicillamine may have a greater impact on side effects and treatment discontinuations than zinc. Conclusions There are some indications that zinc is safer than D-penicillamine therapy while being similarly effective in preventing or reducing hepatic or neurological WD symptoms. Study quality was low warranting cautious interpretation of our findings.},
  file = {/Users/gerbrich/Zotero/storage/XSIFPGCQ/Appenzeller‚ÄêHerzog et al. - 2019 - Comparative effectiveness of common therapies for .pdf;/Users/gerbrich/Zotero/storage/3BAL863U/liv.html},
  keywords = {hepatolenticular degeneration,meta-analysis,systematic review,Wilson disease},
  langid = {english},
  number = {11}
}

@dataset{Appenzeller-Herzog2020,
  title = {Data from {{Comparative}} Effectiveness of Common Therapies for {{Wilson}} Disease: {{A}} Systematic Review and Meta-analysis of Controlled Studies},
  author = {Appenzeller-Herzog, Christian},
  date = {2020-01},
  publisher = {{Zenodo}},
  url = {https://doi.org/10.5281/zenodo.3625931},
  keywords = {dataset,wilson}
}

@report{Aromataris2017,
  title = {Joanna {{Briggs Institute Reviewer}}'s {{Manual}}.},
  author = {Aromataris, E and Munn, Z},
  date = {2017},
  institution = {{The Joanna Briggs Institute}},
  url = {https://reviewersmanual.joannabriggs.org/}
}

@article{ASReview2020,
  title = {{{ASReview}}: {{Active}} Learning for Systematic Reviews},
  author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and Kramer, Bianca and Ferdinands, Gerbrich and Harkema, Albert and Fang, Qixiang and Oberski, Daniel},
  date = {2020-04},
  publisher = {{Zenodo}},
  doi = {10/ggssnj},
  abstractnote = {The Automated Systematic Review (ASReview) project implements learning algorithms that interactively query the researcher. This way of interactive training is known as Active Learning. ASReview offers support for classical learning algorithms and state-of-the-art learning algorithms like neural networks. The software ships with an user interface as well as a simulation mode.},
  options = {useprefix=true}
}

@inproceedings{Barn2014,
  title = {Slrtool: A Tool to Support Collaborative Systematic Literature Reviews},
  shorttitle = {Slrtool},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Enterprise Information Systems}}, {{Volume}} 2},
  author = {Barn, Balbir and Raimondi, Franco and Athiappan, Lalith and Clark, Tony},
  date = {2014},
  pages = {440--447},
  publisher = {{SCITEPRESS}},
  location = {{Lisbon, Portugal}},
  url = {http://dx.doi.org/10.5220/0004972204400447},
  urldate = {2020-02-07},
  abstract = {Systematic Literature Reviews (SLRs) are used in a number of fields to produce unbiased accounts of specific research topics. The SLR process is particularly well documented and regulated in the medical field, where it is accepted as the standard mechanism to assess, for instance, the benefits of drugs and treatments. SLRs and meta-analysis techniques are increasingly being used in other fields as well, from Social Sciences to Software Engineering.},
  eventtitle = {{{ICEIS}} 2014 - 16th {{International Conference}} on {{Enterprise Information Systems}}},
  file = {/Users/gerbrich/Zotero/storage/95IIU42B/Barn et al. - 2014 - Slrtool a tool to support collaborative systemati.pdf;/Users/gerbrich/Zotero/storage/MXNN29UK/14668.html},
  isbn = {978-989-758-028-4},
  langid = {english}
}

@article{Bergstra,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  pages = {25},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent ``High Throughput'' methods achieve surprising success\textemdash they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  file = {/Users/gerbrich/Zotero/storage/DKLNFYR6/Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf},
  keywords = {‚õî No DOI found},
  langid = {english}
}

@inproceedings{Bergstra2013,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  shorttitle = {Making a {{Science}} of {{Model Search}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Bergstra, James and Yamins, Daniel and Cox, David},
  date = {2013-02-13},
  pages = {115--123},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v28/bergstra13.html},
  urldate = {2020-07-23},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is of...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {/Users/gerbrich/Zotero/storage/H23BCAAZ/Bergstra et al. - 2013 - Making a Science of Model Search Hyperparameter O.pdf;/Users/gerbrich/Zotero/storage/6MW6B5DI/bergstra13.html},
  langid = {english}
}

@inproceedings{Bigendako2020,
  title = {Modeling a {{Tool}} for {{Conducting Systematic Reviews Iteratively}}},
  author = {Bigendako, Brice and Syriani, Eugene},
  date = {2020-02-05},
  pages = {552--559},
  url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006664405520559},
  urldate = {2020-02-05},
  abstract = {Digital Library},
  eventtitle = {6th {{International Conference}} on {{Model}}-{{Driven Engineering}} and {{Software Development}}},
  file = {/Users/gerbrich/Zotero/storage/8IK7WPAE/Link.html},
  isbn = {978-989-758-283-7},
  keywords = {relis}
}

@article{Borah2017,
  title = {Analysis of the Time and Workers Needed to Conduct Systematic Reviews of Medical Interventions Using Data from the {{PROSPERO}} Registry},
  author = {Borah, Rohit and Brown, Andrew W. and Capers, Patrice L. and Kaiser, Kathryn A.},
  date = {2017-02-01},
  journaltitle = {BMJ Open},
  volume = {7},
  pages = {e012545},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {2044-6055, 2044-6055},
  doi = {10/f9tf57},
  url = {https://bmjopen-bmj-com.proxy.library.uu.nl/content/7/2/e012545},
  urldate = {2020-04-21},
  abstract = {Objectives To summarise logistical aspects of recently completed systematic reviews that were registered in the International Prospective Register of Systematic Reviews (PROSPERO) registry to quantify the time and resources required to complete such projects.
Design Meta-analysis.
Data sources and study selection All of the 195 registered and completed reviews (status from the PROSPERO registry) with associated publications at the time of our search (1 July 2014).
Data extraction All authors extracted data using registry entries and publication information related to the data sources used, the number of initially retrieved citations, the final number of included studies, the time between registration date to publication date and number of authors involved for completion of each publication. Information related to funding and geographical location was also recorded when reported.
Results The mean estimated time to complete the project and publish the review was 67.3 weeks (IQR=42). The number of studies found in the literature searches ranged from 27 to 92 020; the mean yield rate of included studies was 2.94\% (IQR=2.5); and the mean number of authors per review was 5, SD=3. Funded reviews took significantly longer to complete and publish (mean=42 vs 26 weeks) and involved more authors and team members (mean=6.8 vs 4.8 people) than those that did not report funding (both p{$<$}0.001).
Conclusions Systematic reviews presently take much time and require large amounts of human resources. In the light of the ever-increasing volume of published studies, application of existing computing and informatics technology should be applied to decrease this time and resource burden. We discuss recently published guidelines that provide a framework to make finding and accessing relevant literature less burdensome.},
  eprint = {28242767},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/MPTZXFU9/Borah et al. - 2017 - Analysis of the time and workers needed to conduct.pdf;/Users/gerbrich/Zotero/storage/XAPPVH38/e012545.html},
  keywords = {metadata,PROSPERO registry,search methods,systematic reviews},
  langid = {english},
  number = {2}
}

@article{Bornmann2015,
  title = {Growth Rates of Modern Science: {{A}} Bibliometric Analysis Based on the Number of Publications and Cited References},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Mutz, R\"udiger},
  date = {2015},
  journaltitle = {J. Assoc. Inf. Sci. Technol.},
  volume = {66},
  pages = {2215--2222},
  issn = {2330-1643},
  doi = {10/gfj5zc},
  url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.23329},
  urldate = {2020-03-24},
  abstract = {Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique\textemdash segmented regression analysis\textemdash which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid-1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1\% up to the middle of the 18th century, to 2 to 3\% up to the period between the two world wars, and 8 to 9\% to 2010.},
  file = {/Users/gerbrich/Zotero/storage/GLB6RL2A/Bornmann and Mutz - 2015 - Growth rates of modern science A bibliometric ana.pdf;/Users/gerbrich/Zotero/storage/8IN8N5P2/asi.html},
  keywords = {bibliometrics},
  langid = {english},
  number = {11}
}

@article{Breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001-10-01},
  journaltitle = {Machine Learning},
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  url = {https://doi.org/10.1023/A:1010933404324},
  urldate = {2020-02-10},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {/Users/gerbrich/Zotero/storage/GXJE7SAK/Breiman - 2001 - Random Forests.pdf},
  langid = {english},
  number = {1}
}

@article{Carvallo,
  title = {Comparing {{Word Embeddings}} for {{Document Screening}} Based on {{Active Learning}}},
  author = {Carvallo, Andres and Parra, Denis},
  pages = {8},
  abstract = {Document screening is a fundamental task within Evidencebased Medicine (EBM), a practice that provides scientific evidence to support medical decisions. Several approaches are attempting to reduce the workload of physicians who need to screen and label hundreds or thousands of documents in order to answer specific clinical questions. Previous works have attempted to semi-automate document screening, reporting promising results, but their evaluation is conducted using small datasets, which hinders generalization. Moreover, some recent works have used recently introduced neural language models, but no previous work have compared, for this task, the performance of different language models based on neural word embeddings, which have reported good results in the latest years for several NLP tasks. In this work, we evaluate the performance of two popular neural word embeddings (Word2vec and GloVe) in an active learning-based setting for document screening in EBM, with the goal of reducing the number of documents that physicians need to label in order to answer clinical questions. We evaluate these methods in a small public dataset (HealthCLEF 2017) as well as a larger one (Epistemonikos). Our experiments indicate that Word2vec have less variance and better general performance than GloVe when using active learning strategies based on uncertainty sampling.},
  file = {/Users/gerbrich/Zotero/storage/W6EWM8CD/Carvallo and Parra - Comparing Word Embeddings for Document Screening b.pdf},
  langid = {english}
}

@incollection{Chalmers2007,
  title = {The Lethal Consequences of Failing to Make Full Use of All Relevant Evidence about the Effects of Medical Treatments: The Importance of Systematic Reviews},
  booktitle = {Treating Individuals - from Randomised Trials to Personalised Medicine.},
  author = {Chalmers, Iain},
  date = {2007},
  pages = {37--58},
  publisher = {{Lancet}},
  keywords = {‚õî No DOI found}
}

@article{Cheng2018,
  title = {Using Machine Learning to Advance Synthesis and Use of Conservation and Environmental Evidence},
  author = {Cheng, S. H. and Augustin, C. and Bethel, A. and Gill, D. and Anzaroot, S. and Brun, J. and DeWilde, B. and Minnich, R. C. and Garside, R. and Masuda, Y. J. and Miller, D. C. and Wilkie, D. and Wongbusarakum, S. and McKinnon, M. C.},
  date = {2018},
  journaltitle = {Conserv. Biol.},
  volume = {32},
  pages = {762--764},
  issn = {1523-1739},
  doi = {10.1111/cobi.13117},
  url = {https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/cobi.13117},
  urldate = {2020-02-05},
  abstract = {Article impact statement: Machine learning optimizes processes of systematic evidence synthesis and improves its utility for evidence-based conservation.},
  file = {/Users/gerbrich/Zotero/storage/KZKWUGRY/Cheng et al. - 2018 - Using machine learning to advance synthesis and us.pdf;/Users/gerbrich/Zotero/storage/G3QCVYE7/cobi.html},
  keywords = {colandr},
  number = {4}
}

@article{Claesen2015,
  title = {Hyperparameter {{Search}} in {{Machine Learning}}},
  author = {Claesen, Marc and De Moor, Bart},
  date = {2015-04-06},
  url = {http://arxiv.org/abs/1502.02127},
  urldate = {2020-05-08},
  abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
  archivePrefix = {arXiv},
  eprint = {1502.02127},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/ZDNTWGJ2/Claesen and De Moor - 2015 - Hyperparameter Search in Machine Learning.pdf;/Users/gerbrich/Zotero/storage/V42Z59GF/1502.html},
  keywords = {Computer Science - Machine Learning,G.1.6,I.2.6,I.2.8,I.5,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Cleo2019,
  title = {Usability and Acceptability of Four Systematic Review Automation Software Packages: A Mixed Method Design},
  shorttitle = {Usability and Acceptability of Four Systematic Review Automation Software Packages},
  author = {Cleo, Gina and Scott, Anna Mae and Islam, Farhana and Julien, Blair and Beller, Elaine},
  date = {2019-06-20},
  journaltitle = {Systematic Reviews},
  volume = {8},
  pages = {145},
  issn = {2046-4053},
  doi = {10.1186/s13643-019-1069-6},
  url = {https://doi.org/10.1186/s13643-019-1069-6},
  urldate = {2020-02-06},
  abstract = {New software packages help to improve the efficiency of conducting a systematic review through automation of key steps in the systematic review. The aim of this study was to gather qualitative data on the usability and acceptability of four systematic review automation software packages (Covidence, SRA-Helper for EndNote, Rayyan and RobotAnalyst) for the citation screening step of a systematic review.},
  file = {/Users/gerbrich/Zotero/storage/M7D3VD7L/Cleo et al. - 2019 - Usability and acceptability of four systematic rev.pdf;/Users/gerbrich/Zotero/storage/QMF2STW7/s13643-019-1069-6.html},
  keywords = {systrev},
  number = {1}
}

@article{Cohen2005,
  title = {A Survey of Current Work in Biomedical Text Mining},
  author = {Cohen, A. M.},
  date = {2005-01-01},
  journaltitle = {Brief. Bioinformatics.},
  volume = {6},
  pages = {57--71},
  issn = {1467-5463, 1477-4054},
  doi = {10/bnfhhs},
  url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/6.1.57},
  urldate = {2020-03-10},
  abstract = {The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5\textendash 10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.},
  file = {/Users/gerbrich/Zotero/storage/7PX3RNPM/Cohen - 2005 - A survey of current work in biomedical text mining.pdf},
  langid = {english},
  number = {1}
}

@article{Cohen2006,
  title = {Reducing {{Workload}} in {{Systematic Review Preparation Using Automated Citation Classification}}},
  author = {Cohen, A.M. and Hersh, W.R. and Peterson, K. and Yen, Po-Yin},
  date = {2006},
  journaltitle = {J Am Med Inform Assoc},
  volume = {13},
  pages = {206--219},
  issn = {1067-5027},
  doi = {10.1197/jamia.M1929},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447545/},
  urldate = {2020-02-24},
  abstract = {Objective: To determine whether automated classification of document citations can be useful in reducing the time spent by experts reviewing journal articles for inclusion in updating systematic reviews of drug class efficacy for treatment of disease., Design: A test collection was built using the annotated reference files from 15 systematic drug class reviews. A voting perceptron-based automated citation classification system was constructed to classify each article as containing high-quality, drug class\textendash specific evidence or not. Cross-validation experiments were performed to evaluate performance., Measurements: Precision, recall, and F-measure were evaluated at a range of sample weightings. Work saved over sampling at 95\% recall was used as the measure of value to the review process., Results: A reduction in the number of articles needing manual review was found for 11 of the 15 drug review topics studied. For three of the topics, the reduction was 50\% or greater., Conclusion: Automated document citation classification could be a useful tool in maintaining systematic reviews of the efficacy of drug therapy. Further work is needed to refine the classification system and determine the best manner to integrate the system into the production of systematic reviews.},
  eprint = {16357352},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/ELCDFZ4C/Cohen et al. - 2006 - Reducing Workload in Systematic Review Preparation.pdf},
  keywords = {simulation},
  number = {2},
  pmcid = {PMC1447545}
}

@article{Cohen2009,
  title = {Cross-{{Topic Learning}} for {{Work Prioritization}} in {{Systematic Review Creation}} and {{Update}}},
  author = {Cohen, Aaron M. and Ambert, Kyle and McDonagh, Marian},
  date = {2009-09-01},
  journaltitle = {J Am Med Inform Assoc},
  volume = {16},
  pages = {690--704},
  publisher = {{Oxford Academic}},
  issn = {1067-5027},
  doi = {10/c3shq2},
  url = {https://academic-oup-com.proxy.library.uu.nl/jamia/article/16/5/690/804676},
  urldate = {2020-04-24},
  abstract = {Abstract.  Objective: Machine learning systems can be an aid to experts performing systematic reviews (SRs) by automatically ranking journal articles for work-p},
  file = {/Users/gerbrich/Zotero/storage/VDIBS8ZF/Cohen et al. - 2009 - Cross-Topic Learning for Work Prioritization in Sy.pdf;/Users/gerbrich/Zotero/storage/I2CMFZV2/804676.html},
  langid = {english},
  number = {5}
}

@article{Cohen2011,
  title = {Performance of Support-Vector-Machine-Based Classification on 15 Systematic Review Topics Evaluated with the {{WSS}}@95 Measure},
  author = {Cohen, Aaron M},
  date = {2011},
  journaltitle = {J Am Med Inform Assoc},
  volume = {18},
  pages = {104},
  issn = {1067-5027},
  doi = {10/cskz4h},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3005879/},
  urldate = {2020-03-10},
  eprint = {21169622},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/JHQS5MV2/Cohen - 2011 - Performance of support-vector-machine-based classi.pdf},
  number = {1},
  pmcid = {PMC3005879}
}

@book{Cooper2015,
  title = {Research {{Synthesis}} and {{Meta}}-{{Analysis}}: {{A Step}}-by-{{Step Approach}}},
  shorttitle = {Research {{Synthesis}} and {{Meta}}-{{Analysis}}},
  author = {Cooper, Harris},
  date = {2015-12-24},
  publisher = {{SAGE Publications}},
  abstract = {The Fifth Edition of Harris Cooper's bestselling Research Synthesis and Meta-Analysis: A Step-by-Step Approach offers practical advice on how to conduct a synthesis of research in the social, behavioral, and health sciences. The book is written in plain language with four running examples drawn from psychology, education, and health science. With ample coverage of literature searching and the technical aspects of meta-analysis, this one-of-a-kind book applies the basic principles of sound data gathering to the task of producing a comprehensive assessment of existing research.},
  eprint = {IVhDCwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-4833-4705-9},
  keywords = {Reference / Research,Social Science / Research},
  langid = {english},
  pagetotal = {385}
}

@inproceedings{Cormack2014,
  title = {Evaluation of Machine-Learning Protocols for Technology-Assisted Review in Electronic Discovery},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval},
  author = {Cormack, Gordon V. and Grossman, Maura R.},
  date = {2014-07-03},
  pages = {153--162},
  publisher = {{Association for Computing Machinery}},
  location = {{Gold Coast, Queensland, Australia}},
  doi = {10.1145/2600428.2609601},
  url = {https://doi.org/10.1145/2600428.2609601},
  urldate = {2020-03-04},
  abstract = {Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P{$<$}0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P{$<$}0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of "stabilization" -- determining when training is adequate, and therefore may stop.},
  file = {/Users/gerbrich/Zotero/storage/WSWY6S52/Cormack and Grossman - 2014 - Evaluation of machine-learning protocols for techn.pdf},
  isbn = {978-1-4503-2257-7},
  keywords = {e-discovery,electronic discovery,predictive coding,technology-assisted review},
  series = {{{SIGIR}} '14}
}

@article{Cormack2015,
  title = {Autonomy and {{Reliability}} of {{Continuous Active Learning}} for {{Technology}}-{{Assisted Review}}},
  author = {Cormack, Gordon V. and Grossman, Maura R.},
  date = {2015-04-26},
  url = {http://arxiv.org/abs/1504.06868},
  urldate = {2020-04-29},
  abstract = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection.},
  archivePrefix = {arXiv},
  eprint = {1504.06868},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/DBT892QI/Cormack and Grossman - 2015 - Autonomy and Reliability of Continuous Active Lear.pdf;/Users/gerbrich/Zotero/storage/GGUBU6C4/1504.html},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{deVries2020,
  title = {Title, Abstract, and Keyword Searching Resulted in Poor Recovery of Articles in Systematic Reviews of Epidemiologic Practice},
  author = {de Vries, Bas B.L. Penning and van Smeden, Maarten and Rosendaal, Frits R. and Groenwold, Rolf H.H.},
  date = {2020},
  journaltitle = {J. Clin. Epidemiol.},
  volume = {121},
  pages = {55--61},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2020.01.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0895435619306018},
  abstract = {Objective Article full texts are often inaccessible via the standard search engines of biomedical literature, such as PubMed and Embase, which are commonly used for systematic reviews. Excluding the full-text bodies from a literature search may result in a small or selective subset of articles being included in the review because of the limited information that is available in only title, abstract, and keywords. This article describes a comparison of search strategies based on a systematic literature review of all articles published in 5 top-ranked epidemiology journals between 2000 and 2017. Study Design and Setting Based on a text-mining approach, we studied how nine different methodological topics were mentioned across text fields (title, abstract, keywords, and text body). The following methodological topics were studied: propensity score methods, inverse probability weighting, marginal structural modeling, multiple imputation, Kaplan-Meier estimation, number needed to treat, measurement error, randomized controlled trial, and latent class analysis. Results In total, 31,641 Hypertext Markup Language (HTML) files were downloaded from the journals' websites. For all methodological topics and journals, at most 50\% of articles with a mention of a topic in the text body also mentioned the topic in the title, abstract, or keywords. For several topics, a gradual decrease over calendar time was observed of reporting in the title, abstract, or keywords. Conclusion Literature searches based on title, abstract, and keywords alone may not be sufficiently sensitive for studies of epidemiological research practice. This study also illustrates the potential value of full-text literature searches, provided there is accessibility of full-text bodies for literature searches.},
  keywords = {Bibliometrics,Epidemiological methods,Statistical methods,Systematic literature review,Text mining},
  options = {useprefix=true}
}

@article{Doeleman2019,
  title = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis: A Systematic Review and Meta-Analysis},
  shorttitle = {Immunogenicity of Biologic Agents in Juvenile Idiopathic Arthritis},
  author = {Doeleman, Martijn J. H. and van Maarseveen, Erik M. and Swart, Joost F.},
  date = {2019-10-01},
  journaltitle = {Rheumatology (Oxford)},
  volume = {58},
  pages = {1839--1849},
  issn = {1462-0324},
  doi = {10.1093/rheumatology/kez030},
  url = {https://academic.oup.com/rheumatology/article/58/10/1839/5365497},
  urldate = {2020-02-20},
  abstract = {AbstractObjective.  The clinical impact of anti-drug antibodies (ADAbs) in paediatric patients with JIA remains unknown. This systematic review and meta-analysi},
  file = {/Users/gerbrich/Zotero/storage/55ZSQXSV/Doeleman et al. - 2019 - Immunogenicity of biologic agents in juvenile idio.pdf;/Users/gerbrich/Zotero/storage/HWXLZCEM/5365497.html},
  keywords = {review},
  langid = {english},
  number = {10},
  options = {useprefix=true}
}

@inproceedings{Felizardo2011,
  title = {Using {{Visual Text Mining}} to {{Support}} the {{Study Selection Activity}} in {{Systematic Literature Reviews}}},
  author = {Felizardo, Katia and Salleh, Norsaremah and Martins, Rafael and Mendes, Emilia and MacDonell, Stephen and Maldonado, Jos\'e},
  date = {2011-09-01},
  pages = {77--86},
  doi = {10.1109/ESEM.2011.16},
  abstract = {Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.},
  eventtitle = {International {{Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  file = {/Users/gerbrich/Zotero/storage/WRQ3UZ89/Felizardo et al. - 2011 - Using Visual Text Mining to Support the Study Sele.pdf},
  keywords = {revis}
}

@inproceedings{Fernandez-Saez2010,
  title = {{{SLR}}-{{Tool}} - {{A Tool}} for {{Performing Systematic Literature Reviews}}.},
  author = {Fern\'andez-S\'aez, Ana and Genero, Marcela and Romero, Francisco},
  date = {2010-01-01},
  pages = {157--166},
  abstract = {Systematic literature reviews (SLRs) have been gaining a significant amount of attention from Software Engineering researchers since 2004. SLRs are considered to be a new research methodology in Software Engineering, which allow evidence to be gathered with regard to the usefulness or effectiveness of the technology proposed in Software Engineering for the development and maintenance of software products.
This is demonstrated by the growing number of publications related to SLRs that have appeared in recent years. While some tools exist that can support some or all of the activities of the SLR processes defined in (Kitchenham \& Charters, 2007), these are not free. The objective of this paper is to present the SLR-Tool, which is a free tool and is available on the following website: http://alarcosj.esi.uclm.es/SLRTool/, to be used by researchers from any discipline, and not only Software Engineering. SLR-Tool not only supports the process of performing SLRs proposed in (Kitchenham \& Charters, 2007), but also provides additional functionalities such as: refining searches within the documents by applying text mining techniques; defining a classification schema in order to facilitate data synthesis; exporting the results obtained to the format of tables and charts; and exporting the references from the primary studies to the formats used in bibliographic
packages such as EndNote, BibTeX or Ris. This tool has, to date, been used by members of the Alarcos Research Group and PhD students, and their perception of it is that it is both highly necessary and useful. Our purpose now is to circulate the use of SLR-Tool throughout the entire research community in order to obtain feedback from other users.},
  file = {/Users/gerbrich/Zotero/storage/HF4M9SFC/Fern√°ndez-S√°ez et al. - 2010 - SLR-Tool - A Tool for Performing Systematic Litera.pdf}
}

@software{Freitas2020,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  date = {2020-02-02T14:50:52Z},
  origdate = {2013-07-25T00:27:21Z},
  url = {https://github.com/vitorfs/parsifal},
  urldate = {2020-02-05},
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@software{Freitas2020a,
  title = {Vitorfs/Parsifal},
  author = {Freitas, Vitor},
  date = {2020-02-02T14:50:52Z},
  origdate = {2013-07-25T00:27:21Z},
  url = {https://github.com/vitorfs/parsifal},
  urldate = {2020-02-07},
  abstract = {Parsifal is a tool to assist researchers to perform Systematic Literature Reviews},
  keywords = {academic,django,publishing,research,scientific-publications,systematic-literature-reviews}
}

@inproceedings{Fu2011,
  title = {Certainty-{{Enhanced Active Learning}} for {{Improving Imbalanced Data Classification}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Data Mining Workshops}}},
  author = {Fu, Jui Hsi and Lee, Sing Ling},
  date = {2011-12},
  pages = {405--412},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/ICDMW.2011.43},
  url = {http://ieeexplore.ieee.org/document/6137408/},
  urldate = {2020-04-28},
  abstract = {In active learning algorithms, informative samples are usually queried for true labels according to the disagreement of existing hypotheses. However we observed that, when the streaming dataset has skewed class membership, the imbalanced data classification problem is caused in active learning. The Minority class is overwhelmed by the majority class in generating the hypotheses. In this paper, for each unlabeled sample we propose to utilize only local behavior in the certainty-enhanced neighborhood, rather than the entire dataset, to generate the error minimization hypotheses. Consequently, our proposed method enhances the prediction of hypotheses and is able to determine the query probabilities properly. In our experiments, synthetic and real-world datasets are used for presenting the effectiveness of our active learning approach. It is shown that the proposed approach decreases the probability of querying a certain (majority) sample and has the ability of dealing with the imbalanced data classification problem in active learning.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  file = {/Users/gerbrich/Zotero/storage/3CI5WLFU/Fu and Lee - 2011 - Certainty-Enhanced Active Learning for Improving I.pdf},
  isbn = {978-1-4673-0005-6 978-0-7695-4409-0},
  langid = {english}
}

@article{GarciaAdeva2006,
  title = {Mining {{Text}} with {{Pimiento}}},
  author = {Garcia Adeva, J.J. and Calvo, R.},
  date = {2006-07},
  journaltitle = {IEEE Internet Comput.},
  volume = {10},
  pages = {27--35},
  issn = {1089-7801},
  doi = {10.1109/MIC.2006.85},
  url = {http://ieeexplore.ieee.org/document/1704753/},
  urldate = {2020-02-19},
  file = {/Users/gerbrich/Zotero/storage/C4J8FE54/Garcia Adeva and Calvo - 2006 - Mining Text with Pimiento.pdf},
  langid = {english},
  number = {4}
}

@article{Gates2018a,
  title = {Technology-Assisted Title and Abstract Screening for Systematic Reviews: A Retrospective Evaluation of the {{Abstrackr}} Machine Learning Tool},
  shorttitle = {Technology-Assisted Title and Abstract Screening for Systematic Reviews},
  author = {Gates, Allison and Johnson, Cydney and Hartling, Lisa},
  date = {2018-12},
  journaltitle = {Syst Rev},
  volume = {7},
  pages = {45},
  issn = {2046-4053},
  doi = {10/ggpsx4},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-018-0707-8},
  urldate = {2020-05-04},
  abstract = {Background: Machine learning tools can expedite systematic review (SR) processes by semi-automating citation screening. Abstrackr semi-automates citation screening by predicting relevant records. We evaluated its performance for four screening projects.
Methods: We used a convenience sample of screening projects completed at the Alberta Research Centre for Health Evidence, Edmonton, Canada: three SRs and one descriptive analysis for which we had used SR screening methods. The projects were heterogeneous with respect to search yield (median 9328; range 5243 to 47,385 records; interquartile range (IQR) 15,688 records), topic (Antipsychotics, Bronchiolitis, Diabetes, Child Health SRs), and screening complexity. We uploaded the records to Abstrackr and screened until it made predictions about the relevance of the remaining records. Across three trials for each project, we compared the predictions to human reviewer decisions and calculated the sensitivity, specificity, precision, false negative rate, proportion missed, and workload savings.
Results: Abstrackr's sensitivity was {$>$} 0.75 for all projects and the mean specificity ranged from 0.69 to 0.90 with the exception of Child Health SRs, for which it was 0.19. The precision (proportion of records correctly predicted as relevant) varied by screening task (median 26.6\%; range 14.8 to 64.7\%; IQR 29.7\%). The median false negative rate (proportion of records incorrectly predicted as irrelevant) was 12.6\% (range 3.5 to 21.2\%; IQR 12.3\%). The workload savings were often large (median 67.2\%, range 9.5 to 88.4\%; IQR 23.9\%). The proportion missed (proportion of records predicted as irrelevant that were included in the final report, out of the total number predicted as irrelevant) was 0.1\% for all SRs and 6.4\% for the descriptive analysis. This equated to 4.2\% (range 0 to 12. 2\%; IQR 7.8\%) of the records in the final reports.
Conclusions: Abstrackr's reliability and the workload savings varied by screening task. Workload savings came at the expense of potentially missing relevant records. How this might affect the results and conclusions of SRs needs to be evaluated. Studies evaluating Abstrackr as the second reviewer in a pair would be of interest to determine if concerns for reliability would diminish. Further evaluations of Abstrackr's performance and usability will inform its refinement and practical utility.},
  file = {/Users/gerbrich/Zotero/storage/KNNG3A4H/Gates et al. - 2018 - Technology-assisted title and abstract screening f.pdf},
  langid = {english},
  number = {1}
}

@article{Gates2019,
  title = {Performance and Usability of Machine Learning for Screening in Systematic Reviews: A Comparative Evaluation of Three Tools},
  shorttitle = {Performance and Usability of Machine Learning for Screening in Systematic Reviews},
  author = {Gates, Allison and Guitard, Samantha and Pillay, Jennifer and Elliott, Sarah A. and Dyson, Michele P. and Newton, Amanda S. and Hartling, Lisa},
  date = {2019-11-15},
  journaltitle = {Systematic Reviews},
  volume = {8},
  pages = {278},
  issn = {2046-4053},
  doi = {10/ggzgs6},
  url = {https://doi.org/10.1186/s13643-019-1222-2},
  urldate = {2020-06-05},
  abstract = {We explored the performance of three machine learning tools designed to facilitate title and abstract screening in systematic reviews (SRs) when used to (a) eliminate irrelevant records (automated simulation) and (b) complement the work of a single reviewer (semi-automated simulation). We evaluated user experiences for each tool.},
  file = {/Users/gerbrich/Zotero/storage/97RM9LLL/Gates et al. - 2019 - Performance and usability of machine learning for .pdf;/Users/gerbrich/Zotero/storage/JHX3WL9C/s13643-019-1222-2.html},
  number = {1}
}

@article{Glujovsky2011,
  title = {{{PRM2 EROS}}: {{A New Software For Early Stage Of Systematic REVIEWS}}},
  shorttitle = {{{PRM2 EROS}}},
  author = {Glujovsky, D. and Bardach, A. and Mart\'i, S. Garc\'ia and Comand\'e, D. and Ciapponi, A.},
  date = {2011-11-01},
  journaltitle = {Value in Health},
  volume = {14},
  pages = {A564},
  issn = {1098-3015, 1524-4733},
  doi = {10.1016/j.jval.2011.08.1689},
  url = {https://www.valueinhealthjournal.com/article/S1098-3015(11)03251-7/abstract},
  urldate = {2020-02-07},
  abstract = {The workload of the initial phases of the process of developing a systematic review
(SR) is often underestimated. The screening and quality assessment of studies, usually
done by pairs of independent reviewers, is not only time-consuming, but it also is
complicated, tiresome, and prone to mistakes. A computer-software designed to cope
with the initial phases of a SR would be of great help. There is a generalized lack
of development in this regard, and the available options are not very accessible or
affordable.},
  eprint = {21669381},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/IK4HEMZJ/Glujovsky et al. - 2011 - PRM2 EROS A New Software For Early Stage Of Syste.pdf;/Users/gerbrich/Zotero/storage/NBIWRX58/fulltext.html},
  langid = {english},
  number = {7}
}

@article{Gough2002,
  title = {Systematic {{Research Synthesis}} to {{Inform Policy}}, {{Practice}} and {{Democratic Debate}}},
  author = {Gough, David and Elbourne, Diana},
  date = {2002-07},
  journaltitle = {Soc. Policy Soc.},
  volume = {1},
  pages = {225--236},
  publisher = {{Cambridge University Press}},
  issn = {1475-3073, 1474-7464},
  doi = {10/bdmp7h},
  url = {http://www.cambridge.org/core/journals/social-policy-and-society/article/systematic-research-synthesis-to-inform-policy-practice-and-democratic-debate/0A84765767F7ED99E55EA1B30C642D73},
  urldate = {2020-04-21},
  abstract = {Issues surrounding the role and provision of evidence to inform policy and practice have become topical and problematic. The context of these controversies is discussed, with particular emphasis on systematic approaches to synthesising research evidence. We contrast the `positivist' emphasis with interpretative qualitative synthesis, and suggest that many of the viewpoints have become unnecessarily and unhelpfully polarised. The methods for systematic research syntheses will vary as they depend on the question being asked. The process is transparent, allowing readers to see how conclusions have been reached, and forms the basis of reviews which can be updated to help provide sustainable and relevant evidence.},
  file = {/Users/gerbrich/Zotero/storage/GZZLYTSQ/Gough and Elbourne - 2002 - Systematic Research Synthesis to Inform Policy, Pr.pdf;/Users/gerbrich/Zotero/storage/VXUHBTFJ/0A84765767F7ED99E55EA1B30C642D73.html},
  langid = {english},
  number = {3}
}

@incollection{Gough2018,
  title = {Systematic Reviews},
  booktitle = {Advanced Research Methods for Applied Psychology},
  author = {Gough, David and Richardson, Michelle},
  date = {2018},
  pages = {75--87},
  publisher = {{Routledge}},
  file = {/Users/gerbrich/Zotero/storage/BTMZ4UG3/Brough - 2018 - Advanced research methods for applied psychology .pdf}
}

@article{Hall2012,
  title = {A {{Systematic Literature Review}} on {{Fault Prediction Performance}} in {{Software Engineering}}},
  author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
  date = {2012-11},
  journaltitle = {IEEE Trans. Softw. Eng.},
  volume = {38},
  pages = {1276--1304},
  issn = {2326-3881},
  doi = {10.1109/TSE.2011.103},
  abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  file = {/Users/gerbrich/Zotero/storage/BZZDJVA3/Hall et al. - 2012 - A Systematic Literature Review on Fault Prediction.pdf;/Users/gerbrich/Zotero/storage/J6R99SUF/6035727.html},
  keywords = {Analytical models,Bayes methods,Context modeling,contextual information,cost reduction,Data models,Fault diagnosis,fault prediction models,fault prediction performance,fault prediction study,feature selection,independent variables,logistic regression,methodological information,naive Bayes,Predictive models,predictive performance,regression analysis,reliable methodology,simple modeling techniques,software engineering,software fault prediction,software fault tolerance,software quality,Software testing,systematic literature review,Systematic literature review,Systematics},
  number = {6}
}

@unpublished{Harkema,
  title = {Replacing {{Scientists}} by {{Machines}}.},
  author = {Harkema, A},
  file = {/Users/gerbrich/Zotero/storage/PY6JF9QJ/Harkema - Replacing Scientists by Machines..pdf},
  langid = {english}
}

@article{Harrison2020,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  shorttitle = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and Usher-Smith, Juliet A.},
  date = {2020-01-13},
  journaltitle = {BMC Med. Res. Methodol.},
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-0897-3},
  url = {https://doi.org/10.1186/s12874-020-0897-3},
  urldate = {2020-01-16},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  file = {/Users/gerbrich/Zotero/storage/7UYH9YX8/12874_2020_897_MOESM3_ESM.docx;/Users/gerbrich/Zotero/storage/NMB789XG/Harrison et al. - 2020 - Software tools to support title and abstract scree.pdf;/Users/gerbrich/Zotero/storage/CSUQM7G7/s12874-020-0897-3.html},
  number = {1}
}

@article{Harrison2020b,
  title = {Software Tools to Support Title and Abstract Screening for Systematic Reviews in Healthcare: An Evaluation},
  author = {Harrison, Hannah and Griffin, Simon J. and Kuhn, Isla and Usher-Smith, Juliet A.},
  date = {2020-01-13},
  journaltitle = {BMC Medical Research Methodology},
  volume = {20},
  pages = {7},
  issn = {1471-2288},
  doi = {10/gghppf},
  url = {https://doi.org/10.1186/s12874-020-0897-3},
  abstract = {Systematic reviews are vital to the pursuit of evidence-based medicine within healthcare. Screening titles and abstracts (T\&Ab) for inclusion in a systematic review is an intensive, and often collaborative, step. The use of appropriate tools is therefore important. In this study, we identified and evaluated the usability of software tools that support T\&Ab screening for systematic reviews within healthcare research.},
  number = {1}
}

@article{Hashimoto2016,
  title = {Topic Detection Using Paragraph Vectors to Support Active Learning in Systematic Reviews},
  author = {Hashimoto, Kazuma and Kontonatsios, Georgios and Miwa, Makoto and Ananiadou, Sophia},
  date = {2016-08-01},
  journaltitle = {J Biomed Inform},
  volume = {62},
  pages = {59--65},
  issn = {1532-0464},
  doi = {10/brgm},
  url = {http://www.sciencedirect.com/science/article/pii/S1532046416300442},
  urldate = {2020-05-06},
  abstract = {Systematic reviews require expert reviewers to manually screen thousands of citations in order to identify all relevant articles to the review. Active learning text classification is a supervised machine learning approach that has been shown to significantly reduce the manual annotation workload by semi-automating the citation screening process of systematic reviews. In this paper, we present a new topic detection method that induces an informative representation of studies, to improve the performance of the underlying active learner. Our proposed topic detection method uses a neural network-based vector space model to capture semantic similarities between documents. We firstly represent documents within the vector space, and cluster the documents into a predefined number of clusters. The centroids of the clusters are treated as latent topics. We then represent each document as a mixture of latent topics. For evaluation purposes, we employ the active learning strategy using both our novel topic detection method and a baseline topic model (i.e., Latent Dirichlet Allocation). Results obtained demonstrate that our method is able to achieve a high sensitivity of eligible studies and a significantly reduced manual annotation cost when compared to the baseline method. This observation is consistent across two clinical and three public health reviews. The tool introduced in this work is available from https://nactem.ac.uk/pvtopic/.},
  file = {/Users/gerbrich/Zotero/storage/76UT6YZI/Hashimoto et al. - 2016 - Topic detection using paragraph vectors to support.pdf;/Users/gerbrich/Zotero/storage/7L6EGQIU/S1532046416300442.html},
  keywords = {Active learning,Citation screening,Document embeddings,Paragraph vectors,Systematic reviews,Topic modelling},
  langid = {english}
}

@article{Holzinger2016,
  title = {Interactive Machine Learning for Health Informatics: When Do We Need the Human-in-the-Loop?},
  shorttitle = {Interactive Machine Learning for Health Informatics},
  author = {Holzinger, Andreas},
  date = {2016-06},
  journaltitle = {Brain Inf.},
  volume = {3},
  pages = {119--131},
  issn = {2198-4018, 2198-4026},
  doi = {10/gddxqv},
  url = {http://link.springer.com/10.1007/s40708-016-0042-6},
  urldate = {2020-05-19},
  file = {/Users/gerbrich/Zotero/storage/HUF44PU7/Holzinger - 2016 - Interactive machine learning for health informatic.pdf},
  langid = {english},
  number = {2}
}

@article{Howard2016,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review},
  shorttitle = {{{SWIFT}}-{{Review}}},
  author = {Howard, Brian E. and Phillips, Jason and Miller, Kyle and Tandon, Arpit and Mav, Deepak and Shah, Mihir R. and Holmgren, Stephanie and Pelch, Katherine E. and Walker, Vickie and Rooney, Andrew A. and Macleod, Malcolm and Shah, Ruchir R. and Thayer, Kristina},
  date = {2016-05-23},
  journaltitle = {Syst Rev},
  volume = {5},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0263-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877757/},
  urldate = {2020-02-05},
  abstract = {Background
There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus.

Methods
Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95~\% of known relevant studies and (2) the ``Work Saved over Sampling'' (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics.

Results
Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50~\% of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation.

Conclusions
Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.

Electronic supplementary material
The online version of this article (doi:10.1186/s13643-016-0263-z) contains supplementary material, which is available to authorized users.},
  eprint = {27216467},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/QM5IJC8Z/Howard et al. - 2016 - SWIFT-Review a text-mining workbench for systemat.pdf},
  keywords = {swift-review},
  pmcid = {PMC4877757}
}

@software{J.Thomas2010,
  title = {{{EPPI}}-{{Reviewer}} 4: Software for Research Synthesis.},
  author = {{J. Thomas} and {J. Brunton} and {S. Graziosi}},
  date = {2010},
  location = {{London: Social Science Research Unit}},
  organization = {{UCL Institute of Education}}
}

@article{Jelihovschi2014,
  title = {{{ScottKnott}}: {{A Package}} for {{Performing}} the {{Scott}}-{{Knott Clustering Algorithm}} in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, Enio and Faria, Jos\'e},
  date = {2014-03-05},
  journaltitle = {TEMA (S\~ao Carlos)},
  volume = {15},
  doi = {10.5540/tema.2014.015.01.0003},
  abstract = {Scott-Knott is an hierarchical clustering algorithm used in the application of ANOVA, when the researcher is comparing treatment means, with a very important characteristic: it does not present any overlapping in its grouping results. We wrote a code, in R, that performs this algorithm starting from vectors, matrix, data.frame, aov or aov.list objects. The results are presented with letters representing groups, as well as through graphics using different colors to differentiate distinct groups. This R package, named ScottKnott is the main topic of this article.},
  file = {/Users/gerbrich/Zotero/storage/2GAVUFKT/Jelihovschi and Faria - 2014 - ScottKnott A Package for Performing the Scott-Kno.pdf}
}

@article{Jelihovschi2014a,
  title = {{{ScottKnott}}: A Package for Performing the {{Scott}}-{{Knott}} Clustering Algorithm in {{R}}},
  shorttitle = {{{ScottKnott}}},
  author = {Jelihovschi, E. G. and Faria, J. C. and Allaman, I. B.},
  date = {2014-04},
  journaltitle = {TEMA S\~ao Carlos},
  volume = {15},
  pages = {3--17},
  publisher = {{Sociedade Brasileira de Matem\'atica Aplicada e Computacional}},
  issn = {2179-8451},
  doi = {10.5540/tema.2014.015.01.0003},
  url = {http://www.scielo.br/scielo.php?script=sci_abstract&pid=S2179-84512014000100002&lng=en&nrm=iso&tlng=en},
  urldate = {2020-03-03},
  file = {/Users/gerbrich/Zotero/storage/BH7YUCIH/Jelihovschi et al. - 2014 - ScottKnott a package for performing the Scott-Kno.pdf;/Users/gerbrich/Zotero/storage/DX32ASY6/scielo.html},
  langid = {english},
  number = {1}
}

@article{joia2011local,
  title = {Local Affine Multidimensional Projection},
  author = {Joia, Paulo and Coimbra, Danilo and Cuminato, Jose A and Paulovich, Fernando V and Nonato, Luis G},
  date = {2011},
  journaltitle = {IEEE Trans. Vis. Comput. Graph.},
  volume = {17},
  pages = {2563--2571},
  publisher = {{IEEE}},
  number = {12}
}

@article{Kilicoglu2009,
  title = {Towards {{Automatic Recognition}} of {{Scientifically Rigorous Clinical Research Evidence}}},
  author = {Kilicoglu, H. and Demner-Fushman, D. and Rindflesch, T. C. and Wilczynski, N. L. and Haynes, R. B.},
  date = {2009-01-01},
  journaltitle = {J Am Med Inform Assn},
  volume = {16},
  pages = {25--31},
  issn = {1067-5027, 1527-974X},
  doi = {10/bjkhh9},
  url = {https://academic.oup.com/jamia/article-lookup/doi/10.1197/jamia.M2996},
  urldate = {2020-03-10},
  file = {/Users/gerbrich/Zotero/storage/NY5RBQ5D/Kilicoglu et al. - 2009 - Towards Automatic Recognition of Scientifically Ri.pdf},
  langid = {english},
  number = {1}
}

@article{Kohl2018,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  shorttitle = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  date = {2018-02-01},
  journaltitle = {Environmental Evidence},
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  url = {https://doi.org/10.1186/s13750-018-0115-5},
  urldate = {2020-02-07},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K\"uhn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  file = {/Users/gerbrich/Zotero/storage/XZX9XDTZ/Kohl et al. - 2018 - Online tools supporting the conduct and reporting .pdf;/Users/gerbrich/Zotero/storage/ECBH5U46/s13750-018-0115-5.html},
  keywords = {cadima},
  number = {1}
}

@article{Kohl2018a,
  title = {Online Tools Supporting the Conduct and Reporting of Systematic Reviews and Systematic Maps: A Case Study on {{CADIMA}} and Review of Existing Tools},
  author = {Kohl, Christian and McIntosh, Emma J. and Unger, Stefan and Haddaway, Neal R. and Kecke, Steffen and Schiemann, Joachim and Wilhelm, Ralf},
  date = {2018-02-01},
  journaltitle = {Environmental Evidence},
  volume = {7},
  pages = {8},
  issn = {2047-2382},
  doi = {10.1186/s13750-018-0115-5},
  url = {https://doi.org/10.1186/s13750-018-0115-5},
  abstract = {Systematic reviews and systematic maps represent powerful tools to identify, collect, evaluate and summarise primary research pertinent to a specific research question or topic in a highly standardised and reproducible manner. Even though they are seen as the ``gold standard'' when synthesising primary research, systematic reviews and maps are typically resource-intensive and complex activities. Thus, managing the conduct and reporting of such reviews can become a time consuming and challenging task. This paper introduces the open access online tool CADIMA, which was developed through a collaboration between the Julius K\"uhn-Institut and the Collaboration for Environmental Evidence, in order to increase the efficiency of the evidence synthesis process and facilitate reporting of all activities to maximise methodological rigour. Furthermore, we analyse how CADIMA compares with other available tools by providing a comprehensive summary of existing software designed for the purposes of systematic review management. We show that CADIMA is the only available open access tool that is designed to: (1) assist throughout the systematic review/map process; (2) be suited to reviews broader than medical sciences; (3) allow for offline data extraction; and, (4) support working as a review team.},
  number = {1}
}

@article{Kremer2014,
  title = {Active Learning with Support Vector Machines},
  author = {Kremer, Jan and Steenstrup Pedersen, Kim and Igel, Christian},
  date = {2014},
  journaltitle = {WIREs Data Min. Knowl. Discov.},
  volume = {4},
  pages = {313--326},
  doi = {10/f6fss7},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1132},
  abstract = {In machine learning, active learning refers to algorithms that autonomously select the data points from which they will learn. There are many data mining applications in which large amounts of unlabeled data are readily available, but labels (e.g., human annotations or results coming from complex experiments) are costly to obtain. In such scenarios, an active learning algorithm aims at identifying data points that, if labeled and used for training, would most improve the learned model. Labels are then obtained only for the most promising data points. This speeds up learning and reduces labeling costs. Support vector machine (SVM) classifiers are particularly well-suited for active learning due to their convenient mathematical properties. They perform linear classification, typically in a kernel-induced feature space, which makes expressing the distance of a data point from the decision boundary straightforward. Furthermore, heuristics can efficiently help estimate how strongly learning from a data point influences the current model. This information can be used to actively select training samples. After a brief introduction to the active learning problem, we discuss different query strategies for selecting informative data points and review how these strategies give rise to different variants of active learning with SVMs. This article is categorized under: Technologies \textquestiondown{} Machine Learning},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1132},
  number = {4}
}

@article{Kwok2020,
  title = {Virus {{Metagenomics}} in {{Farm Animals}}: {{A Systematic Review}}},
  shorttitle = {Virus {{Metagenomics}} in {{Farm Animals}}},
  author = {Kwok, Kirsty T. T. and Nieuwenhuijse, David F. and Phan, My V. T. and Koopmans, Marion P. G.},
  date = {2020-01},
  journaltitle = {Viruses},
  volume = {12},
  pages = {107},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/v12010107},
  url = {https://www.mdpi.com/1999-4915/12/1/107},
  urldate = {2020-03-24},
  abstract = {A majority of emerging infectious diseases are of zoonotic origin. Metagenomic Next-Generation Sequencing (mNGS) has been employed to identify uncommon and novel infectious etiologies and characterize virus diversity in human, animal, and environmental samples. Here, we systematically reviewed studies that performed viral mNGS in common livestock (cattle, small ruminants, poultry, and pigs). We identified 2481 records and 120 records were ultimately included after a first and second screening. Pigs were the most frequently studied livestock and the virus diversity found in samples from poultry was the highest. Known animal viruses, zoonotic viruses, and novel viruses were reported in available literature, demonstrating the capacity of mNGS to identify both known and novel viruses. However, the coverage of metagenomic studies was patchy, with few data on the virome of small ruminants and respiratory virome of studied livestock. Essential metadata such as age of livestock and farm types were rarely mentioned in available literature, and only 10.8\% of the datasets were publicly available. Developing a deeper understanding of livestock virome is crucial for detection of potential zoonotic and animal pathogens and One Health preparedness. Metagenomic studies can provide this background but only when combined with essential metadata and following the \&ldquo;FAIR\&rdquo; (Findable, Accessible, Interoperable, and Reusable) data principles.},
  file = {/Users/gerbrich/Zotero/storage/PB55QXJP/Kwok et al. - 2020 - Virus Metagenomics in Farm Animals A Systematic R.pdf;/Users/gerbrich/Zotero/storage/VZ8PYV4C/107.html},
  issue = {1},
  keywords = {animal reservoir,deep sequencing,emerging infectious diseases,high-throughput sequencing,livestock,NGS,one health,viral metagenomics,virome,zoonosis},
  langid = {english},
  number = {1}
}

@article{Lajeunesse2016,
  title = {Facilitating Systematic Reviews, Data Extraction, and Meta-Analysis with the Metagear Package for {{R}}},
  author = {Lajeunesse, Marc J.},
  date = {2016},
  journaltitle = {Methods Ecol. Evol.},
  volume = {7},
  pages = {323--330}
}

@article{Lau2019,
  title = {Editorial: {{Systematic}} Review Automation Thematic Series},
  shorttitle = {Editorial},
  author = {Lau, Joseph},
  date = {2019-03-11},
  journaltitle = {Syst Rev},
  volume = {8},
  pages = {70},
  issn = {2046-4053},
  doi = {10/ggsmwf},
  url = {https://doi.org/10.1186/s13643-019-0974-z},
  urldate = {2020-04-21},
  file = {/Users/gerbrich/Zotero/storage/RADW6V27/Lau - 2019 - Editorial Systematic review automation thematic s.pdf;/Users/gerbrich/Zotero/storage/PNTJE2Y7/s13643-019-0974-z.html},
  number = {1}
}

@article{Le2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  date = {2014-05-22},
  url = {http://arxiv.org/abs/1405.4053},
  urldate = {2020-02-04},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ``powerful,'' ``strong'' and ``Paris'' are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archivePrefix = {arXiv},
  eprint = {1405.4053},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/IH9XHJUV/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,doc2vec,feature_extraction},
  langid = {english},
  primaryClass = {cs}
}

@incollection{Lewis1994,
  title = {Heterogeneous {{Uncertainty Sampling}} for {{Supervised Learning}}},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Lewis, David D. and Catlett, Jason},
  editor = {Cohen, William W. and Hirsh, Haym},
  date = {1994-01-01},
  pages = {148--156},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco (CA)}},
  doi = {10.1016/B978-1-55860-335-6.50026-X},
  url = {http://www.sciencedirect.com/science/article/pii/B978155860335650026X},
  urldate = {2020-02-04},
  abstract = {Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.},
  file = {/Users/gerbrich/Zotero/storage/V8XP4FY3/Lewis and Catlett - 1994 - Heterogeneous Uncertainty Sampling for Supervised .pdf;/Users/gerbrich/Zotero/storage/GFRUCYX6/B978155860335650026X.html},
  isbn = {978-1-55860-335-6},
  keywords = {query_strategy},
  langid = {english}
}

@article{Marshall2019,
  title = {Toward Systematic Review Automation: A Practical Guide to Using Machine Learning Tools in Research Synthesis},
  shorttitle = {Toward Systematic Review Automation},
  author = {Marshall, Iain J. and Wallace, Byron C.},
  date = {2019-12},
  journaltitle = {Syst Rev},
  volume = {8},
  pages = {163, s13643-019-1074-9},
  issn = {2046-4053},
  doi = {10/ggnnp5},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1074-9},
  urldate = {2020-03-10},
  abstract = {Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice.},
  file = {/Users/gerbrich/Zotero/storage/9T5MUH8Q/Marshall and Wallace - 2019 - Toward systematic review automation a practical g.pdf},
  langid = {english},
  number = {1}
}

@article{Marshall2020,
  title = {Semi-{{Automated}} Evidence Synthesis in Health Psychology: Current Methods and Future Prospects},
  author = {Marshall, Iain J. and Johnson, Blair T. and Wang, Zigeng and Rajasekaran, Sanguthevar and Wallace, Byron C.},
  date = {2020},
  journaltitle = {Health Psychol. Rev.},
  volume = {14},
  pages = {145--158},
  publisher = {{Routledge}},
  doi = {10/ggjv98},
  url = {https://doi.org/10.1080/17437199.2020.1716198},
  eprint = {https://doi.org/10.1080/17437199.2020.1716198},
  file = {/Users/gerbrich/Zotero/storage/ZYRX9353/Marshall et al. - 2020 - Semi-Automated evidence synthesis in health psycho.pdf},
  number = {1}
}

@article{Matwin2010,
  title = {A New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews},
  author = {Matwin, Stan and Kouznetsov, Alexandre and Inkpen, Diana and Frunza, Oana and O'Blenis, Peter},
  date = {2010-07-01},
  journaltitle = {J Am Med Inform Assoc},
  volume = {17},
  pages = {446--453},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004325},
  url = {https://academic.oup.com/jamia/article/17/4/446/867054},
  urldate = {2020-02-06},
  abstract = {Abstract.  Objective To determine whether a factorized version of the complement na\"ive Bayes (FCNB) classifier can reduce the time spent by experts reviewing jo},
  file = {/Users/gerbrich/Zotero/storage/D2P8QY98/Matwin et al. - 2010 - A new algorithm for reducing the workload of exper.pdf;/Users/gerbrich/Zotero/storage/7R2UK3MA/867054.html},
  langid = {english},
  number = {4}
}

@inproceedings{Miller2016,
  title = {{{SWIFT}}-{{Active Screener}}: Reducing Literature Screening Effort through Machine Learning for Systematic Reviews},
  booktitle = {Abstracts of the 24th {{Cochrane Colloquium}}},
  author = {Miller, K. and J, Phillips and {M. Shah} and {B. Howard} and {D. Mav} and {K. Thayer} and {R. Shah}},
  date = {2016-23/0027-10},
  publisher = {{John Wiley \& Sons}},
  location = {{Seoul, Korea}},
  eventtitle = {Challenges to Evidence-Based Health Care and {{Cochrane}}.}
}

@article{Mitchell1997,
  title = {Does {{Machine Learning Really Work}}?},
  author = {Mitchell, Tom M.},
  date = {1997-09-15},
  journaltitle = {AI Mag.},
  volume = {18},
  pages = {11--11},
  issn = {2371-9621},
  doi = {10/gg4g34},
  url = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1303},
  urldate = {2020-07-10},
  abstract = {Does machine learning really work? Yes. Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value. Machine-learning algorithms have now learned to detect credit card fraud by mining data on past transactions, learned to steer vehicles driving autonomously on public highways at 70 miles an hour, and learned the reading interests of many individuals to assemble personally customized electronic newsAbstracts. A new computational theory of learning is beginning to shed light on fundamental issues, such as the trade-off among the number of training examples available, the number of hypotheses considered, and the likely accuracy of the learned hypothesis. Newer research is beginning to explore issues such as long-term learning of new representations, the integration of Bayesian inference and induction, and life-long cumulative learning. This article, based on the keynote talk presented at the Thirteenth National Conference on Artificial Intelligence, samples a number of recent accomplishments in machine learning and looks at where the field might be headed. [Copyright restrictions preclude electronic publication of this article.]},
  file = {/Users/gerbrich/Zotero/storage/TDME67HK/Mitchell - 1997 - Does Machine Learning Really Work.pdf;/Users/gerbrich/Zotero/storage/3AJVWGS7/1303.html},
  issue = {3},
  langid = {english},
  number = {3}
}

@article{Miwa2014,
  title = {Reducing Systematic Review Workload through Certainty-Based Screening},
  author = {Miwa, Makoto and Thomas, James and O'Mara-Eves, Alison and Ananiadou, Sophia},
  date = {2014-10-01},
  journaltitle = {J Biomed Inform},
  volume = {51},
  pages = {242--253},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2014.06.005},
  url = {http://www.sciencedirect.com/science/article/pii/S1532046414001439},
  urldate = {2020-03-04},
  abstract = {In systematic reviews, the growing number of published studies imposes a significant screening workload on reviewers. Active learning is a promising approach to reduce the workload by automating some of the screening decisions, but it has been evaluated for a limited number of disciplines. The suitability of applying active learning to complex topics in disciplines such as social science has not been studied, and the selection of useful criteria and enhancements to address the data imbalance problem in systematic reviews remains an open problem. We applied active learning with two criteria (certainty and uncertainty) and several enhancements in both clinical medicine and social science (specifically, public health) areas, and compared the results in both. The results show that the certainty criterion is useful for finding relevant documents, and weighting positive instances is promising to overcome the data imbalance problem in both data sets. Latent dirichlet allocation (LDA) is also shown to be promising when little manually-assigned information is available. Active learning is effective in complex topics, although its efficiency is limited due to the difficulties in text classification. The most promising criterion and weighting method are the same regardless of the review topic, and unsupervised techniques like LDA have a possibility to boost the performance of active learning without manual annotation.},
  file = {/Users/gerbrich/Zotero/storage/D36HMHQV/Miwa et al. - 2014 - Reducing systematic review workload through certai.pdf;/Users/gerbrich/Zotero/storage/JNGPDR3A/S1532046414001439.html},
  keywords = {Active learning,Certainty,Systematic reviews,Text mining},
  langid = {english}
}

@article{modAL2018,
  title = {{{modAL}}: {{A}} Modular Active Learning Framework for {{Python}}},
  author = {Danka, Tivadar and Horvath, Peter},
  url = {https://github.com/cosmic-cortex/modAL}
}

@article{Molleri,
  title = {AUTOMATIZA\c{C}\~AO DO PROCESSO DE CONDU\c{C}\~AO DE REVIS\~OES SISTEM\'ATICAS DA LITERATURA EM ENGENHARIA DE SOFTWARE},
  author = {Moll\'eri, Jefferson Seide},
  pages = {192},
  file = {/Users/gerbrich/Zotero/storage/4VDSH5NP/Moll√©ri - AUTOMATIZA√á√ÉO DO PROCESSO DE CONDU√á√ÉO DE REVIS√ïES .pdf},
  langid = {portuguese}
}

@book{Molnar2020,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  date = {2020-02-28},
  publisher = {{Lulu.com}},
  abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
  eprint = {jBm3DwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-244-76852-2},
  keywords = {Computers / General},
  langid = {english},
  pagetotal = {320}
}

@article{Nagtegaal2019,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  date = {2019},
  journaltitle = {J. Behav. Public Adm.},
  volume = {2},
  doi = {doi.org/10.30636/jbpa.22.71},
  file = {/Users/gerbrich/Zotero/storage/PJ8QLKFJ/Nudging healthcare professionals towards evidence-.pdf;/Users/gerbrich/Zotero/storage/RFLHKMUP/71.html},
  keywords = {nudging,review},
  number = {2}
}

@data{Nagtegaal2019a,
  title = {Nudging Healthcare Professionals towards Evidence-Based Medicine: {{A}} Systematic Scoping Review},
  author = {Nagtegaal, Rosanna and Tummers, Lars and Noordegraaf, Mirko and Bekkers, Victor},
  date = {2019},
  publisher = {{Harvard Dataverse}},
  url = {https://doi.org/10.7910/DVN/WMGPGZ},
  unf = {UNF:6:xUGLGKnjKj5IOIlZQXTwDg==},
  version = {V1}
}

@incollection{Ng2002,
  title = {On {{Discriminative}} vs. {{Generative Classifiers}}: {{A}} Comparison of Logistic Regression and Naive {{Bayes}}},
  shorttitle = {On {{Discriminative}} vs. {{Generative Classifiers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Ng, Andrew Y. and Jordan, Michael I.},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {841--848},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf},
  urldate = {2020-04-05},
  file = {/Users/gerbrich/Zotero/storage/PK9GJT6G/Ng and Jordan - 2002 - On Discriminative vs. Generative Classifiers A co.pdf;/Users/gerbrich/Zotero/storage/BEMP5GP7/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-.html}
}

@article{Norman2020,
  title = {Systematic Review Automation Methods},
  author = {Norman, C. R.},
  date = {2020},
  url = {https://dare.uva.nl/search?identifier=20331344-af76-4f0f-8094-7423a13a28ce},
  urldate = {2020-07-23},
  file = {/Users/gerbrich/Zotero/storage/9BK3S42V/search.html},
  langid = {english}
}

@article{OConnor2019,
  title = {A Question of Trust: Can We Build an Evidence Base to Gain Trust in Systematic Review Automation Technologies?},
  shorttitle = {A Question of Trust},
  author = {O'Connor, Annette M. and Tsafnat, Guy and Thomas, James and Glasziou, Paul and Gilbert, Stephen B. and Hutton, Brian},
  date = {2019-12},
  journaltitle = {Syst Rev},
  volume = {8},
  pages = {143},
  issn = {2046-4053},
  doi = {10/ggsmwh},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-1062-0},
  urldate = {2020-04-21},
  abstract = {Background: Although many aspects of systematic reviews use computational tools, systematic reviewers have been reluctant to adopt machine learning tools. Discussion: We discuss that the potential reason for the slow adoption of machine learning tools into systematic reviews is multifactorial. We focus on the current absence of trust in automation and set-up challenges as major barriers to adoption. It is important that reviews produced using automation tools are considered non-inferior or superior to current practice. However, this standard will likely not be sufficient to lead to widespread adoption. As with many technologies, it is important that reviewers see ``others'' in the review community using automation tools. Adoption will also be slow if the automation tools are not compatible with workflows and tasks currently used to produce reviews. Many automation tools being developed for systematic reviews mimic classification problems. Therefore, the evidence that these automation tools are non-inferior or superior can be presented using methods similar to diagnostic test evaluations, i.e., precision and recall compared to a human reviewer. However, the assessment of automation tools does present unique challenges for investigators and systematic reviewers, including the need to clarify which metrics are of interest to the systematic review community and the unique documentation challenges for reproducible software experiments.},
  file = {/Users/gerbrich/Zotero/storage/B2LVV5F8/O‚ÄôConnor et al. - 2019 - A question of trust can we build an evidence base.pdf},
  langid = {english},
  number = {1}
}

@article{OMara-Eves2015,
  title = {Using Text Mining for Study Identification in Systematic Reviews: A Systematic Review of Current Approaches},
  shorttitle = {Using Text Mining for Study Identification in Systematic Reviews},
  author = {O'Mara-Eves, Alison and Thomas, James and McNaught, John and Miwa, Makoto and Ananiadou, Sophia},
  date = {2015-01-14},
  journaltitle = {Syst Rev},
  volume = {4},
  pages = {5},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-5},
  url = {https://doi.org/10.1186/2046-4053-4-5},
  urldate = {2020-02-11},
  abstract = {The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities.},
  file = {/Users/gerbrich/Zotero/storage/WWD9DBD2/O‚ÄôMara-Eves et al. - 2015 - Using text mining for study identification in syst.pdf;/Users/gerbrich/Zotero/storage/HMT2W9P7/2046-4053-4-5.html},
  keywords = {systrev},
  number = {1}
}

@article{Ouzzani2016,
  title = {Rayyan\textemdash a Web and Mobile App for Systematic Reviews},
  author = {Ouzzani, Mourad and Hammady, Hossam and Fedorowicz, Zbys and Elmagarmid, Ahmed},
  date = {2016},
  journaltitle = {Syst. Rev.},
  volume = {5},
  pages = {210},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0384-4},
  url = {http://dx.doi.org/10.1186/s13643-016-0384-4},
  abstract = {Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making.},
  number = {1}
}

@article{Oxman1993,
  title = {The {{Science}} of {{Reviewing Researcha}}},
  author = {Oxman, Andrew D. and Guyatt, Gordon H.},
  date = {1993},
  journaltitle = {Ann. N. Y. Acad. Sci.},
  volume = {703},
  pages = {125--134},
  issn = {1749-6632},
  doi = {10/d2qzcj},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb26342.x},
  urldate = {2020-04-17},
  file = {/Users/gerbrich/Zotero/storage/5KL5XV4Y/j.1749-6632.1993.tb26342.html},
  langid = {english},
  number = {1}
}

@article{PRISMA-PGroup2015,
  title = {Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols ({{PRISMA}}-{{P}}) 2015 Statement},
  author = {{PRISMA-P Group} and Moher, David and Shamseer, Larissa and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A},
  date = {2015-12},
  journaltitle = {Syst Rev},
  volume = {4},
  pages = {1},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-1},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-4-1},
  urldate = {2020-02-04},
  abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
  file = {/Users/gerbrich/Zotero/storage/PT8NAI48/PRISMA-P Group et al. - 2015 - Preferred reporting items for systematic review an.pdf},
  langid = {english},
  number = {1}
}

@article{Przybyla2018,
  title = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}: {{A}} User Study},
  shorttitle = {Prioritising References for Systematic Reviews with {{RobotAnalyst}}},
  author = {Przyby\l a, Piotr and Brockmeier, Austin J. and Kontonatsios, Georgios and Pogam, Marie-Annick Le and McNaught, John and {Erik von Elm} and Nolan, Kay and Ananiadou, Sophia},
  date = {2018},
  journaltitle = {Res. Synth. Methods},
  volume = {9},
  pages = {470--488},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1311},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1311},
  urldate = {2020-02-05},
  abstract = {Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95\% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.},
  file = {/Users/gerbrich/Zotero/storage/7FPGPSK2/Przyby≈Ça et al. - 2018 - Prioritising references for systematic reviews wit.pdf;/Users/gerbrich/Zotero/storage/U7KPBBJ4/jrsm.html},
  langid = {english},
  number = {3}
}

@inproceedings{Ramos2003,
  title = {Using Tf-Idf to Determine Word Relevance in Document Queries},
  booktitle = {Proceedings of the First Instructional Conference on Machine Learning},
  author = {Ramos, Juan and others},
  date = {2003},
  volume = {242},
  pages = {133--142},
  file = {/Users/gerbrich/Zotero/storage/YUB2N9D9/Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf},
  organization = {{Piscataway, NJ}}
}

@article{Rathbone2015,
  title = {Faster Title and Abstract Screening? {{Evaluating Abstrackr}}, a Semi-Automated Online Screening Program for Systematic Reviewers},
  shorttitle = {Faster Title and Abstract Screening?},
  author = {Rathbone, John and Hoffmann, Tammy and Glasziou, Paul},
  date = {2015-06-15},
  journaltitle = {Systematic Reviews},
  volume = {4},
  pages = {80},
  issn = {2046-4053},
  doi = {10/f7ms4w},
  url = {https://doi.org/10.1186/s13643-015-0067-6},
  urldate = {2020-05-10},
  abstract = {Citation screening is time consuming and inefficient. We sought to evaluate the performance of Abstrackr, a semi-automated online tool for predictive title and abstract screening.},
  file = {/Users/gerbrich/Zotero/storage/WZJ458FZ/Rathbone et al. - 2015 - Faster title and abstract screening Evaluating Ab.pdf;/Users/gerbrich/Zotero/storage/KU5AMTKQ/s13643-015-0067-6.html},
  number = {1}
}

@book{RCoreTeam2019,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2019},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@inproceedings{Rehurek2010,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {\v{R}eh\r{u}\v{r}ek, Radim and Sojka, Petr},
  date = {2010-05-22},
  pages = {45--50},
  publisher = {{ELRA}},
  location = {{Valletta, Malta}},
  langid = {english}
}

@article{Reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2019-08-27},
  url = {http://arxiv.org/abs/1908.10084},
  urldate = {2020-02-10},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archivePrefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/QWTQF9NF/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/gerbrich/Zotero/storage/SI4PQX5T/1908.html},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@online{Scholar,
  title = {{{CORD}}-19 | {{Semantic Scholar}}},
  author = {Scholar, Semantic},
  url = {https://pages.semanticscholar.org/coronavirus-research},
  urldate = {2020-03-25},
  abstract = {CORD-19 (COVID-19 Open Research Dataset) is a free resource of over 44,000 scholarly articles about COVID-19 and related coronaviruses.},
  file = {/Users/gerbrich/Zotero/storage/CT5QCGZM/coronavirus-research.html},
  langid = {english}
}

@article{scikit-learn,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  date = {2011},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2825--2830}
}

@report{Settles2009,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  date = {2009},
  institution = {{University of Wisconsin-Madison Department of Computer Sciences}},
  url = {https://minds.wisconsin.edu/handle/1793/60660},
  urldate = {2020-05-07},
  abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain. 
 
This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
  file = {/Users/gerbrich/Zotero/storage/LTW675GJ/Settles - 2009 - Active Learning Literature Survey.pdf;/Users/gerbrich/Zotero/storage/LDQNIUHV/60660.html},
  langid = {english},
  type = {Technical Report}
}

@article{Settles2012,
  title = {Active {{Learning}}},
  author = {Settles, Burr},
  date = {2012-06-30},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {6},
  pages = {1--114},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00429ED1V01Y201207AIM018},
  url = {http://www.morganclaypool.com/doi/abs/10.2200/S00429ED1V01Y201207AIM018},
  urldate = {2020-02-04},
  file = {/Users/gerbrich/Zotero/storage/2BSTA3X9/Settles - 2012 - Active Learning.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{Shahmirzadi2019,
  title = {Text {{Similarity}} in {{Vector Space Models}}: {{A Comparative Study}}},
  shorttitle = {Text {{Similarity}} in {{Vector Space Models}}},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Shahmirzadi, Omid and Lugowski, Adam and Younge, Kenneth},
  date = {2019-12},
  pages = {659--666},
  doi = {10/gg54m9},
  url = {10.1109/ICMLA.2019.00120},
  abstract = {Automatic measurement of semantic text similarity is an important task in natural language processing. In this paper, we evaluate the performance of different vector space models to perform this task. We address the real-world problem of modeling patent-to-patent similarity and compare TFIDF (and related extensions), topic models (e.g., latent semantic indexing), and neural models (e.g., paragraph vectors). Contrary to expectations, the added computational cost of text embedding methods is justified only when: 1) the target text is condensed; and 2) the similarity comparison is trivial. Otherwise, TFIDF performs surprisingly well in other cases: in particular for longer and more technical texts or for making finer-grained distinctions between nearest neighbours. Unexpectedly, extensions to the TFIDF method, such as adding noun phrases or calculating term weights incrementally, were not helpful in our context.},
  eventtitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  keywords = {automatic measurement,Computational modeling,Context modeling,latent semantic indexing,natural language processing,nearest neighbour methods,nearest neighbours,neural models,paragraph vectors,patent-to-patent similarity,patents,Patents,Predictive models,semantic text similarity,Semantics,Task analysis,technical texts,text analysis,text embedding methods,text similarity ; vector space model ; text embedding ; patent ; big data,TFIDF method,vector space models,vectors,Vocabulary}
}

@software{Shapiro2018,
  title = {Shapiromatron/Hawc: 2018-{{Q3}}},
  shorttitle = {Shapiromatron/Hawc},
  author = {Shapiro, Andy and Addington, Josh and Thacker, Shane and Comeaux, Justin},
  date = {2018-09-13},
  doi = {10.5281/zenodo.1414622},
  url = {https://zenodo.org/record/1414622#.Xjx5zxNKjOQ},
  urldate = {2020-02-06},
  abstract = {2018 Q3 release.},
  file = {/Users/gerbrich/Zotero/storage/ESIYK5HH/1414622.html},
  organization = {{Zenodo}}
}

@article{Shemilt2014,
  title = {Pinpointing Needles in Giant Haystacks: Use of Text Mining to Reduce Impractical Screening Workload in Extremely Large Scoping Reviews},
  shorttitle = {Pinpointing Needles in Giant Haystacks},
  author = {Shemilt, Ian and Simon, Antonia and Hollands, Gareth J. and Marteau, Theresa M. and Ogilvie, David and O'Mara-Eves, Alison and Kelly, Michael P. and Thomas, James},
  date = {2014},
  journaltitle = {Res. Synth. Methods},
  volume = {5},
  pages = {31--49},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1093},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1093},
  urldate = {2020-02-11},
  abstract = {In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved {$>$}800,000 (CA) and {$>$}1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90\% (CA) and 88\% (EE) compared with conventional screening (absolute reductions of {$\approx$}430 000 (CA) and {$\approx$}378 000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions. \textcopyright{} 2013 The Authors. Research Synthesis Methods published by John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/C42P4W4M/Shemilt et al. - 2014 - Pinpointing needles in giant haystacks use of tex.pdf;/Users/gerbrich/Zotero/storage/5QFKVQ8Z/jrsm.html},
  keywords = {scoping review methods,study selection,systematic review methods,text mining},
  langid = {english},
  number = {1}
}

@article{Shemilt2016,
  title = {Use of Cost-Effectiveness Analysis to Compare the Efficiency of Study Identification Methods in Systematic Reviews},
  author = {Shemilt, Ian and Khan, Nada and Park, Sophie and Thomas, James},
  date = {2016-08-17},
  journaltitle = {Syst Rev},
  volume = {5},
  pages = {140},
  issn = {2046-4053},
  doi = {10.1186/s13643-016-0315-4},
  url = {https://doi.org/10.1186/s13643-016-0315-4},
  urldate = {2020-03-03},
  abstract = {Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/CZQI8A6B/Shemilt et al. - 2016 - Use of cost-effectiveness analysis to compare the .pdf},
  keywords = {systrev},
  langid = {english},
  number = {1}
}

@online{Shperber2019,
  title = {A Gentle Introduction to {{Doc2Vec}}},
  author = {Shperber, Gidi},
  date = {2019-11-05T14:00:53.950Z},
  journaltitle = {Medium},
  url = {https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e},
  urldate = {2020-02-04},
  abstract = {TL;DR},
  file = {/Users/gerbrich/Zotero/storage/MKWGDB73/a-gentle-introduction-to-doc2vec-db3e8c0cce5e.html},
  langid = {english}
}

@article{Singh2018,
  title = {Improving {{Active Learning}} in {{Systematic Reviews}}},
  author = {Singh, Gaurav and Thomas, James and Shawe-Taylor, John},
  date = {2018-01-29},
  url = {http://arxiv.org/abs/1801.09496},
  urldate = {2020-05-06},
  abstract = {Systematic reviews are essential to summarizing the results of different clinical and social science studies. The first step in a systematic review task is to identify all the studies relevant to the review. The task of identifying relevant studies for a given systematic review is usually performed manually, and as a result, involves substantial amounts of expensive human resource. Lately, there have been some attempts to reduce this manual effort using active learning. In this work, we build upon some such existing techniques, and validate by experimenting on a larger and comprehensive dataset than has been attempted until now. Our experiments provide insights on the use of different feature extraction models for different disciplines. More importantly, we identify that a naive active learning based screening process is biased in favour of selecting similar documents. We aimed to improve the performance of the screening process using a novel active learning algorithm with success. Additionally, we propose a mechanism to choose the best feature extraction method for a given review.},
  archivePrefix = {arXiv},
  eprint = {1801.09496},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/6UJCQSKC/Singh et al. - 2018 - Improving Active Learning in Systematic Reviews.pdf;/Users/gerbrich/Zotero/storage/33LSMEGV/1801.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{Stansfield2013,
  title = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research: A Case Study},
  shorttitle = {`{{Clustering}}' Documents Automatically to Support Scoping Reviews of Research},
  author = {Stansfield, Claire and Thomas, James and Kavanagh, Josephine},
  date = {2013},
  journaltitle = {Res. Synth. Methods},
  volume = {4},
  pages = {230--241},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1082},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1082},
  urldate = {2020-02-06},
  abstract = {Background Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. Aims To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. Methods A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. Results The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. Conclusions Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  file = {/Users/gerbrich/Zotero/storage/B92KBE6S/jrsm.html},
  keywords = {automatic clustering,automation,information storage and retrieval,lingo,mapping,methods,scoping reviews,text mining},
  langid = {english},
  number = {3}
}

@article{Thomas2011,
  title = {Applications of Text Mining within Systematic Reviews},
  author = {Thomas, James and McNaught, John and Ananiadou, Sophia},
  date = {2011},
  journaltitle = {Res. Synth. Methods},
  volume = {2},
  pages = {1--14},
  doi = {10.1002/jrsm.27},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.27},
  abstract = {Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.27},
  keywords = {automatic summarization,document classification,document clustering,research synthesis,screening,searching,systematic review,term recognition,text mining},
  number = {1}
}

@article{Thomas2017,
  title = {Living Systematic Reviews: 2. {{Combining}} Human and Machine Effort},
  shorttitle = {Living Systematic Reviews},
  author = {Thomas, James and Noel-Storr, Anna and Marshall, Iain and Wallace, Byron and McDonald, Steven and Mavergames, Chris and Glasziou, Paul and Shemilt, Ian and Synnot, Anneliese and Turner, Tari and Elliott, Julian and Agoritsas, Thomas and Hilton, John and Perron, Caroline and Akl, Elie and Hodder, Rebecca and Pestridge, Charlotte and Albrecht, Lauren and Horsley, Tanya and Platt, Joanne and Armstrong, Rebecca and Nguyen, Phi Hung and Plovnick, Robert and Arno, Anneliese and Ivers, Noah and Quinn, Gail and Au, Agnes and Johnston, Renea and Rada, Gabriel and Bagg, Matthew and Jones, Arwel and Ravaud, Philippe and Boden, Catherine and Kahale, Lara and Richter, Bernt and Boisvert, Isabelle and Keshavarz, Homa and Ryan, Rebecca and Brandt, Linn and Kolakowsky-Hayner, Stephanie A. and Salama, Dina and Brazinova, Alexandra and Nagraj, Sumanth Kumbargere and Salanti, Georgia and Buchbinder, Rachelle and Lasserson, Toby and Santaguida, Lina and Champion, Chris and Lawrence, Rebecca and Santesso, Nancy and Chandler, Jackie and Les, Zbigniew and Sch\"unemann, Holger J. and Charidimou, Andreas and Leucht, Stefan and Shemilt, Ian and Chou, Roger and Low, Nicola and Sherifali, Diana and Churchill, Rachel and Maas, Andrew and Siemieniuk, Reed and Cnossen, Maryse C. and MacLehose, Harriet and Simmonds, Mark and Cossi, Marie-Joelle and Macleod, Malcolm and Skoetz, Nicole and Counotte, Michel and Marshall, Iain and Soares-Weiser, Karla and Craigie, Samantha and Marshall, Rachel and Srikanth, Velandai and Dahm, Philipp and Martin, Nicole and Sullivan, Katrina and Danilkewich, Alanna and Mart\'inez Garc\'ia, Laura and Synnot, Anneliese and Danko, Kristen and Mavergames, Chris and Taylor, Mark and Donoghue, Emma and Maxwell, Lara J. and Thayer, Kris and Dressler, Corinna and McAuley, James and Thomas, James and Egan, Cathy and McDonald, Steve and Tritton, Roger and Elliott, Julian and McKenzie, Joanne and Tsafnat, Guy and Elliott, Sarah A. and Meerpohl, Joerg and Tugwell, Peter and Etxeandia, Itziar and Merner, Bronwen and Turgeon, Alexis and Featherstone, Robin and Mondello, Stefania and Turner, Tari and Foxlee, Ruth and Morley, Richard and van Valkenhoef, Gert and Garner, Paul and Munafo, Marcus and Vandvik, Per and Gerrity, Martha and Munn, Zachary and Wallace, Byron and Glasziou, Paul and Murano, Melissa and Wallace, Sheila A. and Green, Sally and Newman, Kristine and Watts, Chris and Grimshaw, Jeremy and Nieuwlaat, Robby and Weeks, Laura and Gurusamy, Kurinchi and Nikolakopoulou, Adriani and Weigl, Aaron and Haddaway, Neal and Noel-Storr, Anna and Wells, George and Hartling, Lisa and O'Connor, Annette and Wiercioch, Wojtek and Hayden, Jill and Page, Matthew and Wolfenden, Luke and Helfand, Mark and Pahwa, Manisha and Yepes Nu\~nez, Juan Jos\'e and Higgins, Julian and Pardo, Jordi Pardo and Yost, Jennifer and Hill, Sophie and Pearson, Leslea},
  date = {2017-11-01},
  journaltitle = {Journal of Clinical Epidemiology},
  volume = {91},
  pages = {31--37},
  issn = {0895-4356},
  doi = {10/gcqcxv},
  url = {http://www.sciencedirect.com/science/article/pii/S0895435617306042},
  urldate = {2020-03-10},
  abstract = {New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (``crowds'') as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential\textemdash and limitations\textemdash of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine ``technologies'' are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.},
  file = {/Users/gerbrich/Zotero/storage/ESEJPVK8/Thomas et al. - 2017 - Living systematic reviews 2. Combining human and .pdf;/Users/gerbrich/Zotero/storage/J9B379VJ/S0895435617306042.html},
  keywords = {Automation,Citizen science,Crowdsourcing,Machine learning,Systematic review,Text mining},
  langid = {english},
  options = {useprefix=true}
}

@inproceedings{Tomassetti2011,
  title = {Linked {{Data}} Approach for Selection Process Automation in {{Systematic Reviews}}},
  author = {Tomassetti, Federico Cesare Argentino and Rizzo, Giuseppe and Vetro', Antonio and Ardito, Luca and Torchiano, Marco and Morisio, Maurizio},
  date = {2011},
  pages = {31--35},
  publisher = {{IEE}},
  doi = {10.1049/ic.2011.0004},
  url = {https://iris.polito.it/handle/11583/2381987#.Xj0tAhNKjOQ},
  urldate = {2020-02-07},
  abstract = {Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20\% with respect to the work load needed in the fully manually selection, with a recall of 100\%. Conclusions: the extraction of knowledge from scientific studies through Linked Data and text mining techniques could be used in the selection phase of the systematic review process to reduce the work load and subjectivity bias.},
  eventtitle = {15th {{Annual Conference}} on {{Evaluation}} \& {{Assessment}} in {{Software Engineering}} ({{EASE}} 2011)},
  file = {/Users/gerbrich/Zotero/storage/8PJ2V6EQ/Tomassetti et al. - 2011 - Linked Data approach for selection process automat.pdf;/Users/gerbrich/Zotero/storage/8PT4A537/2381987.html},
  isbn = {978-1-84919-509-6},
  keywords = {dbpedia},
  langid = {english}
}

@article{Tong2001,
  title = {Support Vector Machine Active Learning with Applications to Text Classification},
  author = {Tong, Simon and Koller, Daphne},
  date = {2001},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {2},
  pages = {45--66},
  file = {/Users/gerbrich/Zotero/storage/V4WAE7M6/Tong and Koller - 2001 - Support vector machine active learning with applic.pdf},
  issue = {Nov},
  keywords = {‚õî No DOI found}
}

@article{vandeSchoot2017,
  title = {The {{GRoLTS}}-{{Checklist}}: {{Guidelines}} for Reporting on Latent Trajectory Studies},
  author = {van de Schoot, Rens and Sijbrandij, Marit and Winter, Sonja D. and Depaoli, Sarah and Vermunt, Jeroen K.},
  date = {2017},
  journaltitle = {Struct. Equ. Model. Multidiscip. J.},
  volume = {24},
  pages = {451--467},
  publisher = {{Routledge}},
  doi = {10/gdpcw9},
  url = {https://doi.org/10.1080/10705511.2016.1247646},
  eprint = {https://doi.org/10.1080/10705511.2016.1247646},
  file = {/Users/gerbrich/Zotero/storage/6VNXSNFK/van de Schoot et al. - 2017 - The GRoLTS-Checklist Guidelines for reporting on .pdf},
  number = {3},
  options = {useprefix=true}
}

@article{vandeSchoot2018,
  title = {Bayesian {{PTSD}}-{{Trajectory Analysis}} with {{Informed Priors Based}} on a {{Systematic Literature Search}} and {{Expert Elicitation}}},
  author = {van de Schoot, Rens and Sijbrandij, Marit and Depaoli, Sarah and Winter, Sonja D. and Olff, Miranda and van Loey, Nancy E.},
  date = {2018-03-04},
  journaltitle = {Multivar. Behav. Res.},
  volume = {53},
  pages = {267--291},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1080/00273171.2017.1412293},
  url = {https://doi.org/10.1080/00273171.2017.1412293},
  urldate = {2020-03-04},
  abstract = {There is a recent increase in interest of Bayesian analysis. However, little effort has been made thus far to directly incorporate background knowledge via the prior distribution into the analyses. This process might be especially useful in the context of latent growth mixture modeling when one or more of the latent groups are expected to be relatively small due to what we refer to as limited data. We argue that the use of Bayesian statistics has great advantages in limited data situations, but only if background knowledge can be incorporated into the analysis via prior distributions. We highlight these advantages through a data set including patients with burn injuries and analyze trajectories of posttraumatic stress symptoms using the Bayesian framework following the steps of the WAMBS-checklist. In the included example, we illustrate how to obtain background information using previous literature based on a systematic literature search and by using expert knowledge. Finally, we show how to translate this knowledge into prior distributions and we illustrate the importance of conducting a prior sensitivity analysis. Although our example is from the trauma field, the techniques we illustrate can be applied to any field.},
  eprint = {29324055},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/MXLVZIJR/Schoot et al. - 2018 - Bayesian PTSD-Trajectory Analysis with Informed Pr.pdf;/Users/gerbrich/Zotero/storage/3DLPQCD2/00273171.2017.html},
  keywords = {Bayesian statistics,Latent class analysis,Latent growth models,Mixture modeling,PTSD},
  number = {2},
  options = {useprefix=true}
}

@article{vandeSchoot2020,
  title = {{{ASReview}}: {{Open}} Source Software for Efficient and Transparent Active Learning for Systematic Reviews},
  author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and de Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Tummers, Lars and Oberski, Daniel},
  date = {2020},
  archivePrefix = {arXiv},
  eprint = {2006.12166},
  eprinttype = {arxiv},
  file = {/Users/gerbrich/Zotero/storage/EB5VHH7W/van de Schoot et al. - 2020 - ASReview Open source software for efficient and t.pdf},
  keywords = {‚õî No DOI found},
  options = {useprefix=true},
  primaryClass = {cs.IR}
}

@software{VeritasHealthInnovation,
  title = {Covidence Systematic Review Software},
  location = {{Melbourne, Australia}},
  url = {www.covidence.org},
  editora = {{Veritas Health Innovation}},
  editoratype = {collaborator}
}

@article{Vries2016,
  title = {Innovation in the {{Public Sector}}: {{A Systematic Review}} and {{Future Research Agenda}}},
  shorttitle = {Innovation in the {{Public Sector}}},
  author = {Vries, Hanna De and Bekkers, Victor and Tummers, Lars},
  date = {2016},
  journaltitle = {Public Adm.},
  volume = {94},
  pages = {146--166},
  issn = {1467-9299},
  doi = {10/gc5rs6},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/padm.12209},
  urldate = {2020-03-24},
  abstract = {This article brings together empirical academic research on public sector innovation. Via a systematic literature review, we investigate 181 articles and books on public sector innovation, published between 1990 and 2014. These studies are analysed based on the following themes: (1) the definitions of innovation, (2) innovation types, (3) goals of innovation, (4) antecedents of innovation and (5) outcomes of innovation. Based upon this analysis, we develop an empirically based framework of potentially important antecedents and effects of public sector innovation. We put forward three future research suggestions: (1) more variety in methods: moving from a qualitative dominance to using other methods, such as surveys, experiments and multi-method approaches; (2) emphasize theory development and testing as studies are often theory-poor; and (3) conduct more cross-national and cross-sectoral studies, linking for instance different governance and state traditions to the development and effects of public sector innovation.},
  file = {/Users/gerbrich/Zotero/storage/LSLQMUJ7/Vries et al. - 2016 - Innovation in the Public Sector A Systematic Revi.pdf;/Users/gerbrich/Zotero/storage/UYJREEFI/padm.html},
  langid = {english},
  number = {1}
}

@article{Wallace2010,
  title = {Semi-Automated Screening of Biomedical Citations for Systematic Reviews},
  author = {Wallace, Byron C. and Trikalinos, Thomas A. and Lau, Joseph and Brodley, Carla and Schmid, Christopher H.},
  date = {2010-01-26},
  journaltitle = {BMC Bioinform},
  volume = {11},
  pages = {55},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-55},
  url = {https://doi.org/10.1186/1471-2105-11-55},
  urldate = {2020-03-04},
  abstract = {Systematic reviews address a specific clinical question by unbiasedly assessing and analyzing the pertinent literature. Citation screening is a time-consuming and critical step in systematic reviews. Typically, reviewers must evaluate thousands of citations to identify articles eligible for a given review. We explore the application of machine learning techniques to semi-automate citation screening, thereby reducing the reviewers' workload.},
  file = {/Users/gerbrich/Zotero/storage/SHT7NWMP/Wallace et al. - 2010 - Semi-automated screening of biomedical citations f.pdf;/Users/gerbrich/Zotero/storage/7RW8GCQ2/1471-2105-11-55.html},
  keywords = {abstrackr},
  number = {1}
}

@inproceedings{Wallace2012,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  date = {2012-01-28},
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  location = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  url = {https://doi.org/10.1145/2110363.2110464},
  urldate = {2020-02-05},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/JHVMNGCY/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {abstrackr,active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@inproceedings{Wallace2012b,
  title = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center: Abstrackr},
  shorttitle = {Deploying an Interactive Machine Learning System in an Evidence-Based Practice Center},
  booktitle = {Proceedings of the 2nd {{ACM SIGHIT International Health Informatics Symposium}}},
  author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Lau, Joseph and Trikalinos, Thomas A.},
  date = {2012-01-28},
  pages = {819--824},
  publisher = {{Association for Computing Machinery}},
  location = {{Miami, Florida, USA}},
  doi = {10.1145/2110363.2110464},
  url = {https://doi.org/10.1145/2110363.2110464},
  urldate = {2020-02-26},
  abstract = {Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.},
  file = {/Users/gerbrich/Zotero/storage/NPDGJ92Z/Wallace et al. - 2012 - Deploying an interactive machine learning system i.pdf},
  isbn = {978-1-4503-0781-9},
  keywords = {active learning,applications,evidence-based medicine,machine learning,medical,text classification},
  series = {{{IHI}} '12}
}

@article{Wang2020,
  title = {Error Rates of Human Reviewers during Abstract Screening in Systematic Reviews},
  author = {Wang, Zhen and Nayfeh, Tarek and Tetzlaff, Jennifer and O'Blenis, Peter and Murad, Mohammad Hassan},
  date = {2020-01-14},
  journaltitle = {PLOS ONE},
  volume = {15},
  pages = {e0227742},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10/ggpsxj},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227742},
  urldate = {2020-03-24},
  abstract = {Background Automated approaches to improve the efficiency of systematic reviews are greatly needed. When testing any of these approaches, the criterion standard of comparison (gold standard) is usually human reviewers. Yet, human reviewers make errors in inclusion and exclusion of references. Objectives To determine citation false inclusion and false exclusion rates during abstract screening by pairs of independent reviewers. These rates can help in designing, testing and implementing automated approaches. Methods We identified all systematic reviews conducted between 2010 and 2017 by an evidence-based practice center in the United States. Eligible reviews had to follow standard systematic review procedures with dual independent screening of abstracts and full texts, in which citation inclusion by one reviewer prompted automatic inclusion through the next level of screening. Disagreements between reviewers during full text screening were reconciled via consensus or arbitration by a third reviewer. A false inclusion or exclusion was defined as a decision made by a single reviewer that was inconsistent with the final included list of studies. Results We analyzed a total of 139,467 citations that underwent 329,332 inclusion and exclusion decisions from 86 unique reviewers. The final systematic reviews included 5.48\% of the potential references identified through bibliographic database search (95\% confidence interval (CI): 2.38\% to 8.58\%). After abstract screening, the total error rate (false inclusion and false exclusion) was 10.76\% (95\% CI: 7.43\% to 14.09\%). Conclusions This study suggests important false inclusion and exclusion rates by human reviewers. When deciding the validity of a future automated study selection algorithm, it is important to keep in mind that the gold standard is not perfect and that achieving error rates similar to humans may be adequate and can save resources and time.},
  file = {/Users/gerbrich/Zotero/storage/TGXVDG4J/Wang et al. - 2020 - Error rates of human reviewers during abstract scr.pdf;/Users/gerbrich/Zotero/storage/BWRDVMGJ/article.html},
  keywords = {Automation,Cardiovascular medicine,Citation analysis,Database searching,Distillation,Health screening,Mental health and psychiatry,Systematic reviews},
  langid = {english},
  number = {1}
}

@article{Westgate2019,
  title = {Revtools: {{An R}} Package to Support Article Screening for Evidence Synthesis},
  author = {Westgate, Martin J.},
  date = {2019},
  journaltitle = {Res. Synth. Methods},
  doi = {10.1002/jrsm.1374}
}

@data{Winter2020,
  title = {Additional {{Information}}: {{Bayesian PTSD}}-{{Trajectory Analysis}} with {{Informed Priors}}},
  author = {Winter, Sonja D. and van de Schoot, Rens},
  date = {2020-02},
  publisher = {{OSF}},
  url = {osf.io/vk4be},
  options = {useprefix=true}
}

@article{Wolfenden2016,
  title = {Time to Consider Sharing Data Extracted from Trials Included in Systematic Reviews},
  author = {Wolfenden, Luke and Grimshaw, Jeremy and Williams, Christopher M. and Yoong, Sze Lin},
  date = {2016-11-03},
  journaltitle = {Systematic Reviews},
  volume = {5},
  pages = {185},
  issn = {2046-4053},
  doi = {10/ggb857},
  url = {https://doi.org/10.1186/s13643-016-0361-y},
  urldate = {2020-06-05},
  abstract = {While the debate regarding shared clinical trial data has shifted from whether such data should be shared to how this is best achieved, the sharing of data collected as part of systematic reviews has received little attention. In this commentary, we discuss the potential benefits of coordinated efforts to share data collected as part of systematic reviews.},
  file = {/Users/gerbrich/Zotero/storage/EY83NG3T/Wolfenden et al. - 2016 - Time to consider sharing data extracted from trial.pdf;/Users/gerbrich/Zotero/storage/RXIGV9N9/s13643-016-0361-y.html},
  number = {1}
}

@article{Wynants2020,
  title = {Prediction Models for Diagnosis and Prognosis of Covid-19: Systematic Review and Critical Appraisal},
  shorttitle = {Prediction Models for Diagnosis and Prognosis of Covid-19},
  author = {Wynants, Laure and Calster, Ben Van and Collins, Gary S. and Riley, Richard D. and Heinze, Georg and Schuit, Ewoud and Bonten, Marc M. J. and Damen, Johanna A. A. and Debray, Thomas P. A. and Vos, Maarten De and Dhiman, Paula and Haller, Maria C. and Harhay, Michael O. and Henckaerts, Liesbet and Kreuzberger, Nina and Lohmann, Anna and Luijken, Kim and Ma, Jie and Navarro, Constanza L. Andaur and Reitsma, Johannes B. and Sergeant, Jamie C. and Shi, Chunhu and Skoetz, Nicole and Smits, Luc J. M. and Snell, Kym I. E. and Sperrin, Matthew and Spijker, Ren\'e and Steyerberg, Ewout W. and Takada, Toshihiko and van Kuijk, Sander M. J. and van Royen, Florien S. and Wallisch, Christine and Hooft, Lotty and Moons, Karel G. M. and van Smeden, Maarten},
  date = {2020-04-07},
  journaltitle = {BMJ},
  volume = {369},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {1756-1833},
  doi = {10/ggr2qk},
  url = {http://www.bmj.com/content/369/bmj.m1328},
  urldate = {2020-06-29},
  abstract = {Objective To review and critically appraise published and preprint reports of prediction models for diagnosing coronavirus disease 2019 (covid-19) in patients with suspected infection, for prognosis of patients with covid-19, and for detecting people in the general population at increased risk of becoming infected with covid-19 or being admitted to hospital with the disease.
Design Living systematic review and critical appraisal.
Data sources PubMed and Embase through Ovid, Arxiv, medRxiv, and bioRxiv up to 7 April 2020.
Study selection Studies that developed or validated a multivariable covid-19 related prediction model.
Data extraction At least two authors independently extracted data using the CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using PROBAST (prediction model risk of bias assessment tool).
Results 4909 titles were screened, and 51 studies describing 66 prediction models were included. The review identified three models for predicting hospital admission from pneumonia and other events (as proxy outcomes for covid-19 pneumonia) in the general population; 47 diagnostic models for detecting covid-19 (34 were based on medical imaging); and 16 prognostic models for predicting mortality risk, progression to severe disease, or length of hospital stay. The most frequently reported predictors of presence of covid-19 included age, body temperature, signs and symptoms, sex, blood pressure, and creatinine. The most frequently reported predictors of severe prognosis in patients with covid-19 included age and features derived from computed tomography scans. C index estimates ranged from 0.73 to 0.81 in prediction models for the general population, from 0.65 to more than 0.99 in diagnostic models, and from 0.85 to 0.99 in prognostic models. All models were rated at high or unclear risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and vague reporting. Most reports did not include any description of the study population or intended use of the models, and calibration of the model predictions was rarely assessed.
Conclusion Prediction models for covid-19 are quickly entering the academic literature to support medical decision making at a time when they are urgently needed. This review indicates that proposed models are poorly reported, at high risk of bias, and their reported performance is probably optimistic. Hence, we do not recommend any of these reported prediction models to be used in current practice. Immediate sharing of well documented individual participant data from covid-19 studies and collaboration are urgently needed to develop more rigorous prediction models, and validate promising ones. The predictors identified in included models should be considered as candidate predictors for new models. Methodological guidance should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. Finally, studies should adhere to the TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline.
Systematic review registration Protocol https://osf.io/ehc47/, registration https://osf.io/wy245.
Readers' note This article is a living systematic review that will be updated to reflect emerging evidence. Updates may occur for up to two years from the date of original publication. This version is update 1 of the original article published on 7 April 2020 (BMJ 2020;369:m1328), and previous updates can be found as data supplements (https://www-bmj-com.proxy.library.uu.nl/content/369/bmj.m1328/related\#datasupp).},
  eprint = {32265220},
  eprinttype = {pmid},
  file = {/Users/gerbrich/Zotero/storage/F9VTBBM8/Wynants et al. - 2020 - Prediction models for diagnosis and prognosis of c.pdf;/Users/gerbrich/Zotero/storage/4KWALXTC/bmj.html},
  langid = {english}
}

@article{Yang2018,
  title = {A Benchmark and Comparison of Active Learning for Logistic Regression},
  author = {Yang, Yazhou and Loog, Marco},
  date = {2018-11-01},
  journaltitle = {Pattern Recognition},
  volume = {83},
  pages = {401--415},
  issn = {0031-3203},
  doi = {10/ggb483},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320318302140},
  urldate = {2020-07-27},
  abstract = {Logistic regression is by far the most widely used classifier in real-world applications. In this paper, we benchmark the state-of-the-art active learning methods for logistic regression and discuss and illustrate their underlying characteristics. Experiments are carried out on three synthetic datasets and 44 real-world datasets, providing insight into the behaviors of these active learning methods with respect to the area of the learning curve (which plots classification accuracy as a function of the number of queried examples) and their computational costs. Surprisingly, one of the earliest and simplest suggested active learning methods, i.e., uncertainty sampling, performs exceptionally well overall. Another remarkable finding is that random sampling, which is the rudimentary baseline to improve upon, is not overwhelmed by individual active learning techniques in many cases.},
  keywords = {Active learning,Benchmark,Experimental design,Logistic regression,Preference maps},
  langid = {english}
}

@thesis{Yimin2007,
  title = {Text {{Classification}} on {{Imbalanced Data}}: {{Application}} to {{Systematic Reviews Automation}}},
  author = {Yimin, Ma},
  date = {2007-05},
  institution = {{University of Ottawa}},
  url = {http://dx.doi.org/10.20381/ruor-12126},
  file = {/Users/gerbrich/Zotero/storage/9XQI6EK3/MR34085.pdf}
}

@article{Yu2008a,
  title = {{{GAPscreener}}: {{An}} Automatic Tool for Screening Human Genetic Association Literature in {{PubMed}} Using the Support Vector Machine Technique},
  author = {Yu, Wei and Clyne, Melinda and Dolan, Siobhan M. and Yesupriya, Ajay and Wulf, Anja and Liu, Tiebin and Khoury, Muin J. and Gwinn, Marta},
  date = {2008-04-22},
  journaltitle = {BMC Bioinformatics},
  volume = {9},
  pages = {205},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-9-205},
  url = {https://doi.org/10.1186/1471-2105-9-205},
  abstract = {Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies.},
  number = {1}
}

@dataset{Yu2017,
  title = {Data Sets for {{FASTREAD}}},
  author = {Yu, Zhe and Kraft, Nicholas and Menzies, Tim},
  date = {2017-08},
  publisher = {{Zenodo}},
  url = {https://doi.org/10.5281/zenodo.1162952},
  version = {v1.3}
}

@article{Yu2018,
  title = {Finding Better Active Learners for Faster Literature Reviews},
  author = {Yu, Zhe and Kraft, Nicholas A. and Menzies, Tim},
  date = {2018-03},
  journaltitle = {Empir. Softw. Eng.},
  volume = {23},
  pages = {3161--3186},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1573-7616},
  doi = {10.1007/s10664-017-9587-0},
  url = {http://dx.doi.org/10.1007/s10664-017-9587-0},
  file = {/Users/gerbrich/Zotero/storage/ACKMBK39/Yu et al. - 2018 - Finding better active learners for faster literatu.pdf},
  number = {6}
}

@article{Yu2019,
  title = {{{FAST2}}: {{An}} Intelligent Assistant for Finding Relevant Papers},
  shorttitle = {{{FAST2}}},
  author = {Yu, Zhe and Menzies, Tim},
  date = {2019-04-15},
  journaltitle = {Expert Syst. Appl},
  volume = {120},
  pages = {57--71},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.11.021},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417418307413},
  urldate = {2020-03-03},
  abstract = {Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. Finding relevant papers can be hard due to the huge amount of candidates provided by search. FAST2 is a novel tool for assisting the researchers to find the next promising paper to read. This paper describes FAST2 and tests it on four large systematic literature review datasets. We show that FAST2 robustly optimizes the human effort to find most (95\%) of the relevant software engineering papers while also compensating for the errors made by humans during the review process. The effectiveness of FAST2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers\textemdash which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found\textemdash which helps the reviewer decide when to stop the review; (3) a novel human error correction algorithm\textemdash which corrects a majority of human misclassifications (labeling relevant papers as non-relevant or vice versa) without imposing too much extra human effort.},
  file = {/Users/gerbrich/Zotero/storage/R65D4TB5/Yu and Menzies - 2019 - FAST2 An intelligent assistant for finding releva.pdf;/Users/gerbrich/Zotero/storage/ITB4FS2M/S0957417418307413.html},
  keywords = {Active learning,Literature reviews,Relevance feedback,Selection process,Semi-supervised learning,Text mining},
  langid = {english}
}

@inproceedings{Zhang2004,
  title = {The {{Optimality}} of {{Naive Bayes}}},
  author = {Zhang, Harry},
  date = {2004-01-01},
  volume = {2},
  abstract = {Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classifica- tion is surprising, because the conditional independence assumption on which it is based, is rarely true in real- world applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classification? In this paper, we propose a novel explanation on the superb classification performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependen- cies of all nodes work together, consistently (support- ing a certain classification) or inconsistently (cancel- ing each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences can- cel each other out. We propose and prove a sufficient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
  eventtitle = {Proceedings of the {{Seventeenth International Florida Artificial Intelligence Research Society Conference}}, {{FLAIRS}} 2004},
  file = {/Users/gerbrich/Zotero/storage/UP2XUMJR/Zhang - The Optimality of Naive Bayes.pdf},
  keywords = {model}
}

@article{Zhang2011,
  title = {A Comparative Study of {{TF}}*{{IDF}}, {{LSI}} and Multi-Words for Text Classification},
  author = {Zhang, Wen and Yoshida, Taketoshi and Tang, Xijin},
  date = {2011-03-01},
  journaltitle = {Expert Syst. Appl},
  volume = {38},
  pages = {2758--2765},
  issn = {0957-4174},
  doi = {10/dp7268},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417410008626},
  urldate = {2020-05-06},
  abstract = {One of the main themes in text mining is text representation, which is fundamental and indispensable for text-based intellegent information processing. Generally, text representation inludes two tasks: indexing and weighting. This paper has comparatively studied TF*IDF, LSI and multi-word for text representation. We used a Chinese and an English document collection to respectively evaluate the three methods in information retreival and text categorization. Experimental results have demonstrated that in text categorization, LSI has better performance than other methods in both document collections. Also, LSI has produced the best performance in retrieving English documents. This outcome has shown that LSI has both favorable semantic and statistical quality and is different with the claim that LSI can not produce discriminative power for indexing.},
  file = {/Users/gerbrich/Zotero/storage/GC77RTAU/Zhang et al. - 2011 - A comparative study of TFIDF, LSI and multi-words.pdf;/Users/gerbrich/Zotero/storage/4NRAVP6S/S0957417410008626.html},
  keywords = {Information retrieval,LSI,Multi-word,Text categorization,Text classification,Text representation,TF*IDF},
  langid = {english},
  number = {3}
}

@online{zotero-254,
  title = {Machine Learning Algorithms for Systematic Review: Reducing Workload in a Preclinical Review of Animal Studies and Reducing Human Screening Error | {{Systematic Reviews}} | {{Full Text}}},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-019-0942-7},
  urldate = {2020-01-16},
  file = {/Users/gerbrich/Zotero/storage/FTXIT6LW/s13643-019-0942-7.html}
}

@online{zotero-263,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
  url = {https://pandoc.org/MANUAL.html#definition-lists},
  urldate = {2020-01-20},
  file = {/Users/gerbrich/Zotero/storage/GYHDPIKS/MANUAL.html}
}

@online{zotero-298,
  title = {Is It Time to Trust the Robots? {{The}} Reliability and Usability of Machine Learning Tools for Screening in Systematic Reviews | {{Colloquium Abstracts}}},
  shorttitle = {Is It Time to Trust the Robots?},
  url = {/2019-santiago/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening},
  urldate = {2020-02-05},
  file = {/Users/gerbrich/Zotero/storage/S2WIJV9A/it-time-trust-robots-reliability-and-usability-machine-learning-tools-screening.html},
  langid = {english}
}

@online{zotero-349,
  title = {{{SWIFT}}-{{Review}}: A Text-Mining Workbench for Systematic Review | {{Systematic Reviews}} | {{Full Text}}},
  url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-016-0263-z},
  urldate = {2020-02-06},
  file = {/Users/gerbrich/Zotero/storage/HC8TLNZ2/s13643-016-0263-z.html}
}

@online{zotero-351,
  title = {Completing a Full Systematic Review in under Two Weeks: Processes, Barriers and Facilitators | {{The}} 26th {{Cochrane Colloquium}}},
  url = {https://colloquium2019.cochrane.org/abstracts/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators},
  urldate = {2020-02-06},
  file = {/Users/gerbrich/Zotero/storage/6ZPXMXSL/completing-full-systematic-review-under-two-weeks-processes-barriers-and-facilitators.html},
  keywords = {2weeks}
}

@online{zotero-358,
  title = {{{MDX SLR Tool}}},
  url = {http://ta.mdx.ac.uk/slr/about/},
  urldate = {2020-02-07},
  file = {/Users/gerbrich/Zotero/storage/P6GXB2WY/about.html}
}

@online{zotero-383,
  title = {{{SyRF}}: {{Systematic Review Facility}} | {{SyRF}}: {{Systematic Review Facility}}},
  url = {http://syrf.org.uk/},
  urldate = {2020-02-07},
  file = {/Users/gerbrich/Zotero/storage/8ALRBI87/syrf.org.uk.html}
}

@online{zotero-389,
  title = {6.2. {{Feature}} Extraction \textemdash{} Scikit-Learn 0.22.1 Documentation},
  url = {https://scikit-learn.org/stable/modules/feature_extraction.html},
  urldate = {2020-02-10},
  file = {/Users/gerbrich/Zotero/storage/6Y956NI9/feature_extraction.html}
}

@software{zotero-436,
  title = {{{DistillerSR}}},
  location = {{Ottowa, Canada}},
  organization = {{Evidence Partners}}
}

@online{zotero-5773,
  title = {({{PDF}}) {{How}} to {{Read Less}}: {{Better Machine Assisted Reading Methods}} for {{Systematic Literature Reviews}}},
  url = {https://www.researchgate.net/publication/311586326_How_to_Read_Less_Better_Machine_Assisted_Reading_Methods_for_Systematic_Literature_Reviews},
  urldate = {2020-02-21},
  file = {/Users/gerbrich/Zotero/storage/5MHXQG65/311586326_How_to_Read_Less_Better_Machine_Assisted_Reading_Methods_for_Systematic_Literature_Re.html}
}

@online{zotero-6400,
  title = {New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews | {{Journal}} of the {{American Medical Informatics Association}} | {{Oxford Academic}}},
  url = {https://academic-oup-com.proxy.library.uu.nl/jamia/article/17/4/446/867054},
  urldate = {2020-05-10},
  file = {/Users/gerbrich/Zotero/storage/W2R5QXF6/867054.html}
}

@online{zotero-6401,
  title = {New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews | {{Journal}} of the {{American Medical Informatics Association}} | {{Oxford Academic}}},
  url = {https://academic-oup-com.proxy.library.uu.nl/jamia/article/17/4/446/867054},
  urldate = {2020-05-10},
  file = {/Users/gerbrich/Zotero/storage/5IKA7INL/new algorithm for reducing the workload of experts.PDF;/Users/gerbrich/Zotero/storage/HNCKXE3R/867054.html}
}

@online{zotero-6402,
  title = {New Algorithm for Reducing the Workload of Experts in Performing Systematic Reviews | {{Journal}} of the {{American Medical Informatics Association}} | {{Oxford Academic}}},
  url = {https://academic-oup-com.proxy.library.uu.nl/jamia/article/17/4/446/867054},
  urldate = {2020-05-10},
  file = {/Users/gerbrich/Zotero/storage/KH6DN4GG/867054.html}
}


