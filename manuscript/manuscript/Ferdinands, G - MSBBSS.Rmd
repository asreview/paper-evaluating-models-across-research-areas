---
title: Active learning for screening prioritization in systematic reviews
subtitle: A simulation study
author: 
    - Gerbrich Ferdinands
    - Raoul Schram
    - Jonathan de Bruin
    - Ayoub Bagheri
    - Daniel Oberski
    - Lars Tummers
    - Rens van de Schoot
date: '`r format(Sys.time(), "%d %B, %Y")`'
header-includes:
    - \usepackage{setspace}\doublespacing

output:
  bookdown::pdf_document2:
    extra_dependencies: subfig, algorithm2e, float, lineno, setspace, amssymb
    number_sections: no
    toc: no
  word_document: default
bibliography: asreview.bib
link-citations: yes
csl: systematic-reviews.csl
---

<!-- --- -->
<!-- title: Interactive screening prioritization in systematic reviews by employing active learning -->
<!-- subtitle: A comprehensive study -->
<!-- author:  -->
<!--     - Gerbrich Ferdinands -->
<!--     - Raoul Schram -->
<!--     - Jonathan de Bruin -->
<!--     - Ayoub Bagheri -->
<!--     - Daniel Oberski -->
<!--     - Lars Tummers -->
<!--     - Rens van de Schoot -->
<!-- date: '`r format(Sys.time(), "%d %B, %Y")`' -->
<!-- header-includes: -->
<!--     - \usepackage{setspace}\doublespacing -->
<!-- output: -->
<!--   word_document: default -->
<!--   bookdown::pdf_document2: -->
<!--     extra_dependencies: subfig, algorithm2e, float, lineno, setspace, amssymb -->
<!--     number_sections: no -->
<!--     toc: no -->
<!-- bibliography: asreview.bib -->
<!-- link-citations: yes -->
<!-- csl: systematic-reviews.csl -->
<!-- --- -->


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.ncol = 2, out.width = "50%", fig.align = "center", dev = "png", fig.pos = "H") 
options(tinytex.verbose = TRUE)

library(tidyverse)
library(knitr)
library(xtable)
library(kableExtra)
library(captioner)
library(glue)
library(rbbt)
library(officer)
library(flextable)
# load model parameters 
source("asr-parameters.R")

#pathRdata <- "data/"
pathRdata <- "../../datasets/data_statistics/"
```


```{r}
# configurations table 
tab <- lapply(words, FUN = function(x) paste(x, collapse = ", "))
tab <- do.call(rbind, tab)
# remove row training data (28.02)
colnames(tab) <- c("Configurations")
tab <- tab[-4,]
```

```{r}
# combinations table
params[[4]] <- NULL # remove training data
combins <- expand.grid(params) %>%
  arrange(model, feature_extraction)

# rows to drop
droprows <- combins$model == "nb" & combins$feature_extraction != "tfidf"
combins <- combins[!droprows, ]
``` 


```{r all tables, results = 'hide'}
table_nums <- captioner(prefix = "Table")
# table  on datasets statistics
table_nums(name = "datasets", caption = "Statistics on the datasets obtained from six original systematic reviews.")
table_nums(name = "atd", caption = "ATD values ($\\bar x (\\hat s)$) for all model-dataset combinations. For every dataset, the best results are in bold. Median (MAD) is given for all datasets.")
table_nums(name = "wss", caption = "WSS@95 values ($\\bar x (\\hat s)$) for all model-dataset combinations. For every dataset, the best results are in bold. Median (MAD) is given for all datasets.")
table_nums(name = "rrf", caption = "RRF@10 values ($\\bar x, (\\hat s)$) for all model-dataset combinations. For every dataset, the best results are in bold. Median (MAD) is given for all datasets.")

```

\linenumbers
# Abstract
#### Background
Conducting a systematic review requires great screening effort. Various tools have been proposed to speed up the process of screening thousands of titles and abstracts by engaging in active learning. In active learning, the reviewer interacts with machine learning software to identify relevant publications as early as possible. To gain a comprehensive understanding of active learning models for reducing workload in systematic reviews, the current study provides an methodical overview of such models. Active learning models were evaluated across four different classification techniques (naive Bayes, logistic regression, support vector machines, and random forest) and two different feature extraction strategies (TF-IDF and Doc2vec). Moreover, models were evaluated across six systematic review datasets from various research areas to assess generalizability of active learning models across different research contexts. 

#### Methods
Performance of the models were assessed by conducting simulations on six systematic review datasets. We defined desirable model performance as maximizing recall while minimizing the number of publications needed to screen. Model performance was evaluated by recall curves, WSS@95, RRF@10, and ATD. 

#### Results
Within all datasets, the model performance exceeded screening at random order to a great degree.
THe models reduced the number of publications needed to screen by 91.7% to 63.9%. 

#### Conclusions
Active learning models for screening prioritization show great potential in reducing the workload in systematic reviews.
Overall, the Naive Bayes + TF-IDF model performed the best. 

<!-- Model performance differs vastly across datasets, and the naive Bayes classifier is a  -->
<!-- The results shed new light on the performance of different classification techniques, indicating that the NB classification technique is superior to the widely used SVM. As model performance differs vastly across datasets,  -->
<!-- To facilitate the usability of active learning models in systematic review practice, it is essential to identify how dataset characteristics relate to model performance. -->

__keywords:__ systematic reviews, active learning, screening prioritization, researcher-in-the-loop, title-and-abstract screening

<!-- #### Background  -->
<!-- the context and purpose of the study -->

<!-- context: -->
<!-- active learning models for reducing workload in title and abstract screening in systematic reviews. -->

<!-- This study adds to existing literature by evaluating models across four different classification techniques (Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest) and two different feature extraction strategies (TF-IDF and Doc2vec). Moreover, as previous studies have paid little attention to the generalizability of active learning models across different research contexts, models were evaluated across six systematic review datasets from various research areas. -->
<!-- purpose  -->

<!-- The purpose of the current paper is to increase the evidence base on active learning models for reducing workload in title and abstract screening in systematic reviews.  -->

<!-- for the purpose of maximizing the number of identified relevant publications, while minimizing the number of publications needed to screen.  -->

<!--  We adopt four different classification techniques (Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest) and two different feature extraction strategies (TF-IDF and Doc2vec) for the purpose of maximizing the number of identified relevant publications, while minimizing the number of publications needed to screen.  -->

<!-- #### Methods -->
<!-- how the study was performed and statistical tests used -->

<!-- Models were assessed by conducting a simulation on six systematic review datasets. -->
<!-- To assess generalizability of the models across research contexts, datasets were collected from the fields of medical science, computer science, humanities and social science,  -->

<!-- Desirable model performance was defined as maximizing recall while minimizing the number of publications needed to screen. (WSS@95 recall curves RRF@10 ATD) -->

<!-- #### Results -->
<!-- the main findings -->


<!-- Across all classification techniques, feature extraction strategies and review contexts, active learning models exceeded screening at random order by far.  -->

<!-- #### Conclusions -->
<!-- brief summary and potential implications -->

<!-- As all models exceeded screening at random order by far, they show great potential in reducing workload of systematic reviewers.  -->

<!-- implications: researchers can use this  -->

\newpage 

# Background
Systematic reviews are top of the bill in research. A systematic review brings together all available studies relevant to answer a specific research question [@PRISMA-PGroup2015]. Systematic reviews inform practice and policy [@Gough2002] and are key in developing clinical guidelines [@Chalmers2007]. However, systematic reviews are costly because to identify publications relevant to answering the research question, they among else involve the manual screening of thousands of titles and abstracts.

<!-- An experienced reviewer takes on average 30 seconds to screen one title and abstract, whereas an inexperienced reviewer takes even longer [@Wallace2010]. A typical review of 5000 publications would require over 40 hours of uninterrupted screening, while typically only a fraction of the publications is relevant (mean = 2.94%, IQR=2.5 @Borah2017]). -->
Conducting a systematic review typically requires over a year of work by a team of researchers [@Borah2017]. Nevertheless, systematic reviewers are often bound to a limited budget and timeframe. Currently, the demand for systematic reviews exceeds the available time and resources by far [@Lau2019]. Especially when answering the research question at hand is urgent, it is extremely challenging to provide a review that is both timely and comprehensive.

To ensure a timely review, reducing the workload in systematic reviews is essential. With advances in machine learning (ML), there has been wide interest in tools to reduce the workload in systematic reviews [@Harrison2020]. Various ML models have been proposed, aiming to predict whether a given publication is relevant or irrelevant to the systematic review. Previous findings suggest that such models potentially reduce the workload with 30-70% at the cost of losing 5% of relevant publications, in else, a 95% recall [@OMara-Eves2015].
<!-- To date, several studies have begun to examine the use of learning algorithms for screening prioritization. -->

A well-established approach to increase the efficiency of title and abstract screening is screening prioritization [@Cohen2009; @Shemilt2014]. In screening prioritization, the ML model presents the reviewer with the publications that are most likely to be relevant first, thereby expediting the process of finding all of the relevant publications. Such an approach allows for substantial time-savings in the screening process as the reviewer can decide to stop screening after a sufficient number of relevant publications have been found [@Yu2019]. Moreover, the early retrieval of relevant publications facilitates a faster transition of those publications to the next steps in the review process [@Cohen2009]. 
<!-- Additionally, several studies report increasing efficiency beyond saving time [@OMara-Eves2015].  -->

<!-- assist the reviewer in the screening process -->
<!-- Several studies have proposed active learning models for screening prioritization -->
Recent studies have demonstrated the effectiveness of screening prioritization by means of active learning models [@Yu2018; @Yu2019; @Miwa2014; @Cormack2014; @Cormack2015; @Wallace2010; @Gates2018a]. With active learning, the ML model can iteratively improve its predictions on unlabeled data by allowing the model to select the records from which it wants to learn [@Settles2012]. The model proposes these records to a human annotator who provides the records with labels, which the model then uses to update its predictions. The general assumption is that by letting the model select which records are labelled, the model can achieve higher accuracy more quickly while requiring the human annotator to label as few records as possible [@Settles2009]. Active learning has proven to be an efficient strategy in large unlabeled datasets where labels are expensive to obtain [@Settles2009]. This makes the screening phase in systematic reviewing an ideal candidate solution for such models, because typically labelling a large number of publications is very costly. 

When active learning is applied in the screening phase, the reviewer screens publications that are suggested by an active learning model. Subsequently, the active learning model learns from the reviewers' decision ('relevant', 'irrelevant') and uses this knowledge to update its predictions and to select the next publication to be screened by the reviewer. 

<!-- Combined with some sort of stopping criterion, the reviewer c -->
<!-- _Adopting some sort of stopping criterion (outside scope of the current thesis) the reviewer can quit reviewing after having read only a fraction of candidate publications. meaning the screening process can be finished after reading a fraction of all candidate publications. Saving hours of time and resources._ -->

<!-- , the complex nature of the field is making it difficult to draw overarching conclusions about best practice [@OMara-Eves2015]. -->

The application of active learning models in systematic reviews has been extensively studied [@Yu2018; @Yu2019; @Miwa2014; @Wallace2010; @Gates2018a]. While previous studies have evaluated active learning models in many forms and shapes [@Yu2018; @Yu2019; @Miwa2014; @Cormack2014; @Cormack2015; @Wallace2010; @Singh2018; @Carvallo; @Yimin2007], ready-to-use software tools implementing such models (Abstrackr [@Wallace2012b], Colandr [@Cheng2018], FASTREAD [@Yu2018], Rayyan [@Ouzzani2016], and RobotAnalyst [@Przybyla2018]) currently use the same classification technique to predict relevance of publications, namely support vector machines (SVM). It was found [@Kilicoglu2009; @Aphinyanaphongs2004] that different classification techniques can serve different needs in the retrieval of relevant publications, for example the desired balance between recall and precision. Therefore, it is essential to evaluate different classification techniques in the context of active learning models. The current study investigates active learning models adopting four classification techniques: naive Bayes (NB), logistic regression (LR), SVM, and random forest (RF). These are widely adopted techniques in text classification [@Aggarwal2012] and are fit for software tools to be used in scientific practice due to their relatively short computation time.

<!-- for creating interpretable ML models [@Molnar2020]. Moreover,  -->

<!-- as they allow the reviewer to be timely presented with the next publication.  -->

<!-- low computation time makes these techniques fit to practial use in a software tool when run on a personal computer, as retraining a model can be done in a timely manner. as default of asreview is on local computer to be applicable -->

<!-- Whilst the performance of several classification techniques has been investigated in the ML-aided title-and-abstract screening field in general [@Kilicoglu2009; @Aphinyanaphongs2004], the relatively new subfield of active learning for screening prioritization has not yet studied the performance of classifiers other than SVMs [@Yu2018; @Yu2019; @Cormack2014; @Cormack2015; @Miwa2014; @Wallace2010]. -->

<!-- no study in this area has made a comparison between models adopting different classification techniques. -->
<!-- Since a plethora of techniques exist [@Aggarwal2012], it would be of interest to incorporate such models to this area as well.  -->
Another component that influences model performance is how the textual content of titles and abstracts is represented in a model, called the feature extraction strategy [@Zhang2011; @Carvallo; @Singh2018]. One of the more sophisticated feature extraction strategies is Doc2vec (D2V), also known as paragraph vectors [@Le2014]. D2V learns continuous distributed vector representations for pieces of text. In distributed text representations, words are assumed to appear in the same context when they are similar in terms of a latent space, the “embedding”. A word embedding is simply a vector of scores estimated from a corpus for each word; D2V is an extension of this idea to document embeddings. Embeddings can sometimes outperform simpler feature extraction strategies such as term frequency-inverse document frequency (TF-IDF). They can be trained on large corpora to capture wider semantics and subsequently applied in a specific systematic reviewing application [@Le2014]. Therefore, it is interesting to compare models adopting D2V to models adopting TF-IDF.  

<!-- [@Yu2018; @Yu2019; @Miwa2014; @Wallace2010], -->

Lastly, previous studies have mainly focussed on reviews from a single scientific field, like medicine [@Wallace2010; @Gates2018a] or computer science [@Yu2018, @Yu2019]. To draw conclusions about the general effectiveness of active learning models, it is essential to evaluate models on reviews from varying research contexts [@OMara-Eves2015; @Marshall2020]. To our knowledge, Miwa et al [@Miwa2014] were the only researchers to make a direct comparison between systematic reviews from different research areas, such as the social and the medical sciences. They found that the application of active learning to systematic reviews was more difficult on a systematic review from the social sciences due to the different nature of the vocabularies used. Thus, it is of interest to evaluate model performance across different research contexts, namely social science, medical science and computer science.  

Taken together, for a more comprehensive understanding of active learning models in the context of systematic reviewing, a methodical evaluation of such models is required. The current study aims to address this issue by answering the following research questions:

<!-- (1) across different classification techniques, (2) feature extraction strategies, and (3) review contexts.  -->

<!-- Therefore, the current study aims to address these issues by answering the following research questions: -->

__RQ1__ What is the performance of active learning models across four classification techniques?
__RQ2__ What is the performance of active learning models across two feature extraction strategies? 
__RQ3__ Does the performance of active learning models differ across six systematic reviews from four research areas?

The purpose of this paper is to show the usefulness of active learning models for reducing workload in title and abstract screening in systematic reviews. We adopt four different classification techniques (NB, LR, SVM, and RF) and two different feature extraction strategies (TF-IDF and D2V) for the purpose of maximizing the number of identified relevant publications, while minimizing the number of publications needed to screen. Models were assessed by conducting a simulation on six systematic review datasets. To assess generalizability of the models across research contexts, datasets containing previous systeamtic reviews were collected from the fields of medical science [@Cohen2006; @Appenzeller-Herzog2019; @Kwok2020], computer science [@Yu2018], and social science [@Nagtegaal2019; @vandeSchoot2017]. The models, datasets and simulations are implemented in a pipeline of active learning for screening prioritization, called $\texttt{ASReview}$ [@vandeSchoot2020]. $\texttt{ASReview}$ is a generic open source tool, encouraging fellow researchers to replicate findings from previous studies.
<!-- in which users can adapt and add modules as they like,  -->
To facilitate usability and acceptability of ML-assisted title and abstract screening in the field of systematic review our scripts and data used are openly available.   

<!-- The remaining part of this paper is organized as follows. The methods section describes the study that was designed to answer the research questions and elaborates on the technical details of active learning models for identifying relevant publications in the context of systematic reviews. The findings of the simulation study are reported in the results section. The implications of the findings in context of previous research are discussed in the discussion section, followed by this study's main conclusions in the conclusion section.  -->

# Methods 

## Technical details 
<!-- What follows is a more detailed account of the active learning models. The structure and functions of the key components of the models are introduced to clarify the choices made in the design of the current study.   -->
What follows is a more detailed account of the active learning models to clarify the choices made in the design of the current study.

### Task description
The screening process of a systematic review starts with all publications obtained in the search. The task is to identify which of these publications are relevant, by screening their titles and abstracts. In _active learning_ for screening prioritization, the screening process proceeds as follows:

<!-- Just a sample algorithmn -->
<!-- \begin{algorithm}[H] -->
<!-- \DontPrintSemicolon -->
<!-- \SetAlgoLined -->
<!-- \KwResult{Write here the result} -->
<!-- \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output} -->
<!-- \Input{Record IDs in screening order} -->
<!-- \Output{Average Time to Discovery} -->
<!-- \BlankLine -->
<!--  # inclusions -->
<!--  one_labels = np.where(labels == 1)[0] -->
<!--  # store discovery time -->
<!--  time_results = {label: [] for label in one_labels} -->

<!-- \While{While condition}{ -->
<!--     instructions\; -->
<!--     \eIf{condition}{ -->
<!--         instructions1\; -->
<!--         instructions2\; -->
<!--     }{ -->
<!--         instructions3\; -->
<!--     } -->
<!-- } -->
<!-- \caption{While loop with If/Else condition} -->
<!-- \end{algorithm} -->

- Start with the set of all unlabeled records (titles and abstracts), $\mathcal{U}$.
- The reviewer provides a label for a few, e.g. 5-10 records $x \in \mathcal{U}$, creating a set of labelled records $x \in \mathcal{L}$ such that $x \notin \mathcal{U}$. The label can be either Relevant $\langle x, \texttt{R}\rangle$ or Irrelevant $\langle x, \texttt{I}\rangle$.
- The active learning cycle starts: 

  1. A classifier $C$ is trained on the labelled records $\mathcal{L}$; $C = \texttt{train}(\mathcal{L})$
    
  2. The classifier predicts relevancy scores for all unlabeled records $\mathcal{U}$, $C(\mathcal{U})$
    
  3. Based on the predictions by $C$, the model selects the most relevant record $x^* \in \mathcal{U}$
    
  4. The model requests the reviewer to screen this record, $\langle x^*, \texttt{?}\rangle$
    
  5. The reviewer screens the record and provides a label, $\langle x^*, \texttt{R}\rangle$ or $\langle x^*, \texttt{I}\rangle$
    
  6. The newly labelled record is added to the training data, such that $x^* \in \mathcal{L}$ and $x^* \notin \mathcal{U}$
  
  7. Back to step 1
  
  8. Repeat this cycle until the reviewer decides to stop [@Yu2019] or until all records have been labelled, $\mathcal{U} = \varnothing$. 

In this active learning cycle, the model incrementally improves its predictions on the remaining unlabeled title and abstracts. Relevant titles and abstracts are identified as early in the process as possible. 

This case is an example of pool-based active learning, as the next record to be queried is selected by predicting relevancy for all records in a fixed pool [@Settles2012]. Another form of active learning is stream-based active learning, in which the data is regarded as a stream instead of a fixed pool, in which the model selects one record at a time and then decides whether or not to query this record. This approach of active learning is preferred when it is expensive or impossible to exhaustively search the data for selecting the next query. A possible application of stream-based active learning is living systematic reviews, as the review is continually updated as new evidence becomes available. For an example see the study by Wynants et al @Wynants2020. 

### Class imbalance problem
<!-- There are two classes in the dataset: relevant and irrelevant publications.  -->
Typically, only a fraction of the publications belong to the relevant class (2.94%, [@Borah2017]). To some extent, this fraction is under the control of the researcher through the search criteria: if the researcher narrows the search query, it will generally result in a higher proportion of relevant publications. However, in most applications this practice would yield an unacceptable number of false negatives (erroneously excluded papers) in the querying phase of the review process. For this reason, the query phase in most practical applications would typically yield a very low percentage of relevant publications. 
<!-- In the discussion, we will provide some further remarks on the interplay among query scope and subsequent active machine learning in the abstract screening phase.  -->
Because there are genearlly far fewer examples of relevant than irrelevant publications to train on, the class imbalance causes the classifier to miss relevant publications [@OMara-Eves2015]. Moreover, classifiers can achieve high accuracy but still fail to identify any of the relevant publications [@Wallace2010]. 

<!-- This is evident in the case of a systematic review dataset where only three percent of publications are relevant.  -->

<!-- A model would achieve 97% accuracy when classifying all publications as irrelevant, even though none of the relevant papers would have been correctly identified.  -->

Previous studies have addressed the class imbalance problem by rebalancing the training data in various ways [@OMara-Eves2015]. To decrease the class imbalance in the training data, we rebalance the training set by a technique we propose to call "dynamic resampling" (DR). DR undersamples the number of irrelevant publications in the training data, whereas the number of relevant publications are oversampled such that the size of the training data remains the same. The ratio between relevant and irrelevant publications in the rebalanced training data is not fixed, but dynamically updated and depends on the number of publications in the available training data, the total number of publications in the dataset, and the ratio between relevant and irrelevant publications in the available training data. Additional file 1 provides a detailed script to perform DR. 

<!-- This works because..  -->
<!-- This technique is related to aggressive undersampling in such a way that.  -->

### Classification
To make relevancy predictions on the unlabeled publications, a classifier is trained on features from the training data. The performance of the following four classifiers is explored:
<!-- The classifier predicts class of a given unlabeled publication.  -->
<!-- Whilst the SVM is a well-known technique, also classifiers have been employed in class prediction tasks in systematic reviews.  -->
<!-- In fact, research in this area to date has not yet determined the performance of classifiers other than SVMs. -->

 - Support vector machines (SVM) - SVMs separate the data into classes by finding a multidimensional hyperplane [@Tong2001; @Kremer2014]. 
 
 - L2-regularized logistic regression (LR) - models the probabilities describing the possible outcomes by a logistic function. The classifier uses regularization, shrinking coefficients of features with small contributions to the solution towards zero. 

<!-- A penalty is imposed on the size of the coefficients, shrinking coefficients of features with a minor contribution to the solution towards zero. -->

 - Naive Bayes (NB) is a supervised learning algorithm often used in text classification. Based on Bayes' theorem, with the 'naive' assumption that all features are independent given the class value [@Zhang2004].
 
- Random forests (RF) is a supervised learning algorithm where a large number of decision trees are fit on samples obtained from the original data by sampling both rows (bootstrapped samples) and columns (feature samples). In prediction mode, each tree casts a vote on the class, and the final prediction is the class that received the most votes [@Breiman2001]. 

### Feature extraction
To predict relevance of a given publication, the classifier uses information from the publications in the dataset. Examples of information are titles and abstracts. However, a model cannot make predictions from the titles and abstracts as they are; their textual content needs to be represented numerically as feature vectors. This process of numerically representing textual content is referred to as 'feature extraction'. 

TF-IDF is a specific way of assigning scores to the cells of the “document-term matrix” used in all bag-of-words representations. That is, the rows of the document-term matrix represent the documents (titles and abstracts) and the columns represent all words in the dictionary. Instead of simply counting the number of times each word occurred in the given document, TF-IDF assigns a score to a word relative to the number of documents the word occurs.
<!-- assigns high scores only when a word occurs especially often, relative to its overall frequency across all documents.  -->
The idea behind weighting words by their rarity is that surprising word choices should subsequently make for more discriminative features [@Ramos2003]. A disadvantage of TF-IDF and other bag-of-words methods is that they do not take the ordering of words into account, thereby ignoring syntax. However, in practice, TF-IDF is often found to be a strong baseline [@Shahmirzadi2019].

In recent years, a range of modern methods have been developed that often outperform bag-of-words approaches. Here, we consider doc2vec, an extension of the classic word2vec embedding [@Le2014]. In word embedding models, whether a word did or did not happen to appear in a specific context is predicted by its similarity to that context in a latent space - the “embedding”. The context is usually a sliding window across training sentences. For example, if the window “child ate cookies” occurs in the training data, this might be compared with a random ‘negative’ window that did not occur, such as  “child lovely cookies”. 
<!-- In this example, the word “lovely” was chosen as a randomly sampled “negative” example with about the same word frequency as “ate”.  -->
The tokens “child” and “cookies” are then assigned scores (vectors) that give a higher inner product with the “child” vector, and a smaller product with “lovely”. The word vectors of “ate” and “lovely” are similarly updated. Typically the embedding dimension is a few hundred, i.e. each word vector contains some two hundred scores. Note that if “cookies” previously co-occurred frequently with “spinach”, then the above also indirectly makes “ate” more similar to “spinach”, even if these two words have not been observed yet in the same context. Thus, the distributed representation learns something of the meaning of these words through their occurrence in similar contexts. D2V performs the above procedure while including a paragraph identifier, allowing for paragraph embeddings - or, in our case, embeddings for titles and abstracts. In short, D2V converts each abstract into a vector of a few hundred scores, which can be used to predict inclusion or exclusion.

### Query strategy
The active learning model can adopt different strategies in selecting the next publication to be screened by the reviewer. A strategy mentioned before is selecting the publication with the highest probability of being relevant. In the active learning literature this is referred to as certainty-based active learning [@Settles2012]. Another well-known strategy is uncertainty-based active learning, where the instances that are presented next are those instances on which the model's classifications are the least certain, i.e. close to 0.5 probability [@Settles2012]. Further strategies include selecting the next instance to optimize for various criteria, including: model fit (MLI), model change (MMC), parameter estimate accuracy (EVR), and expected (EER) or worst-case (MER) prediction accuracy [@Yang2018]. Although uncertainty sampling is not explicitly motivated by the optimization of any particular criterion, intuitively it can be seen as attempting to improve the model’s accuracy by reducing uncertainty about its parameter estimates. 

Simulation-based comparisons of these methods across different domains have yielded an ambiguous picture of their relative strengths [@Miwa2014; @Yang2018]. What has become clear from such studies is that the features of the task at hand determine the effectiveness of active learning strategies (“no free active lunch”). For example, if a linear classifier is used for a task that also happens to have a Bayes optimal linear decision boundary, a model-based approach such as Fisher information reduction can be expected to perform well, whereas the same technique can be disastrous when the model is misspecified - a fact that cannot be known in advance. Furthermore, the criteria mentioned above differ from the task of semi-automated abstract screening: here, the aim is not to obtain an accurate model, but rather to end up with a list of records belonging to the relevant class [@Fu2011]. This is the criterion corresponding intuitively to certainty-based sampling. For this reason, we choose to focus on certainty-based sampling strategies as the baseline strategy for active learning in systematic reviewing. However, different strategies may outperform our baseline in specific applications. 

## Simulation study
This section describes the simulation study that was carried out to answer the research questions.

### Set-up
To address RQ1, four models were investigated combining each classifier with TF-IDF feature extraction: 

 1. SVM + TF-IDF
 2. NB + TF-IDF
 3. RF + TF-IDF 
 4. LR + TF-IDF

To address RQ2, the classifiers were combined with D2V feature extraction, leading to the following three combinations:

 5. SVM + D2V
 6. RF + D2V
 7. LR + D2V

The combination NB + D2V could not be tested because the multinomial naive Bayes classifier
^[https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB] requires a feature matrix with positive values, whereas the D2V feature extraction approach^[https://radimrehurek.com/gensim/models/doc2vec.html] produces a feature matrix that can contain negative values. The performance of the seven models was evaluated by simulating every model on six systematic review datasets, addressing RQ3. Hence, 42 simulations were carried out, representing all model-dataset combinations.

Instead of having a human reviewer label publications manually, the screening process was simulated by retrieving the labels in the data. Each simulation started with an initial training set of one relevant and one irrelevant publication to represent a challenging scenario where the reviewer has very little prior knowledge on the publications in the data. The model was retrained each time after a publication had been labelled. A simulation ended after all publications in the dataset had been labelled. To account for sampling variance, every simulation was repeated 15 times. To account for bias due to the content of the initial publications, the initial training set was randomly sampled from the dataset for each of the 15 trials. Although varying over trials, the 15 initial training sets were kept constant for each dataset to allow for a direct comparison of models within datasets. A seed value was set to ensure reproducibility. The simulation study was carried out using the ASReview simulation extension [@ASReview2020]. For each simulation, hyperparameters were optimized through a Tree
of Parzen Estimators (TPE) algorithm [@Bergstra2013] to arrive at maximum model performance.
<!-- About a 1000 runs, retraining after labelling 20, after visual inspection.  --> 
<!-- Scripts for the hyperparaemter optimization and resulting hyperparameter values can be found at GitHub. -->

Simulations were carried out in ASReview version 0.9.3 [@ASReview2020]. Analyses were carried out using \texttt{R} version 3.6.1 [@RCoreTeam2019]. The simulations were carried out on Cartesius, the Dutch national supercomputer.

### Datasets 
The models were simulated on a convenience sample of six systematic review datasets. The data selection process was driven by two factors. Firstly, datasets are collected from various research areas to assess generalizability of the models across research contexts (RQ3). Secondly, all original data files have to be openly published with a CC-BY license. Datasets are available through ASReview's systematic review datasets GitHub^[https://github.com/asreview/systematic-review-datasets].

The Wilson dataset [@Appenzeller-Herzog2020] - from the field of medicine - is from a review on the effectiveness and safety of treatments of Wilson Disease, a rare genetic disorder of copper metabolism [@Appenzeller-Herzog2019]. From the same field, the ACE dataset contains publications on the efficacy of Angiotensin-converting enzyme (ACE) inhibitors, a treatment drug for heart disease [@Cohen2006]. Additionally, the Virus dataset is from a systematic review on studies that performed viral Metagenomic Next-Generation Sequencing (mNGS) in farm animals [@Kwok2020]. From the field of computer science, the Software dataset contains publications from a review on fault prediction in software engineering [@Hall2012]. The Nudging dataset [@Nagtegaal2019a] belongs to a systematic review on nudging healthcare professionals [@Nagtegaal2019], stemming from the social sciences. From the same research area, the PTSD dataset contains publications on studies applying latent trajectory analyses on posttraumatic stress after exposure to traumatic events [@vandeSchoot2017]. Of these six datasets, ACE and Software have been used for model simulations in previous studies on ML-aided title and abstract screening [@Cohen2006; @Yu2018]. 

Data were preprocessed from their original source into a dataset, containing title and abstract of the publications obtained in the initial search. Duplicates and publications with missing abstracts were removed from the data. 
<!-- Preprocessing scripts and resulting datasets can be found at GitHub repository for this paper.  -->
Datasets were labelled to indicate which candidate publications were included in the systematic review, thereby denoting relevant publications. All datasets consisted of thousands of candidate publications, of which only a fraction was deemed relevant to the systematic review. For the Virus and the Nudging dataset, this proportion was about 5 percent. For the remaining six datasets, the proportions of relevant publications were centered around 1-2 percent. (`r table_nums("datasets", display = "cite")`). 
<!-- The data preprocessing scripts can be found on the GitHub^[https://github.com/GerbrichFerdinands/asreview-thesis] -->


```{r, results = 'asis'}
# datasets table
drops <- readRDS(paste0(pathRdata, "drops.RDS"))
all <- readRDS(paste0(pathRdata, "all.RDS"))

nstudies <- function(all, datastage, col){
}

nostudies <- function(all, set, stage){
  sapply(all, function(x) x[set, stage])
}
inclrate <- function(all, set){
  sapply(all, function(x) round(x[set,"incl"]/x[set,"search"]*100,2))
}
datastats <- 
  tibble(dataset = names(all), 
       #Citation = NA, # maybe add footnote citation with kableExtra i.o. this.
       # paper 
       `candidates_paper` = nostudies(all, "paper", "search"), 
       #`fulltext_paper` = nostudies(all, "paper", "ftext"),
       `incl_paper` = nostudies(all, "paper", "incl"),
       `inclrate_paper` = inclrate(all, "paper"),
       # test set 
       `candidates_test` =  nostudies(all, "test", "search"), 
       #`fulltext_test` =  nostudies(all, "test", "ftext"), 
       `incl_test` =  nostudies(all, "test", "incl"), 
       `inclrate_test` = inclrate(all, "test")
       ) 

test <- 
datastats %>%
  select(dataset, candidates_test, incl_test) %>%
  mutate(excl = candidates_test - incl_test)

test <- rbind(test,test) %>%
  mutate(relevancy = c(rep("incl", 6), rep("excl", 6))) 
a <- c(test[1:6,"incl_test"], test[7:12, "excl"])
test[,"number"] <- c(a$incl_test, a$excl)

```


```{r, results = 'asis', caption = "Datasets obtained from six original systematic reviews.", eval = FALSE}
test %>%
  ggplot(aes(x=dataset, y = number, fill = relevancy)) +
  geom_col(position = "stack") +
  # scale_fill_brewer(palette="Set1") +
  scale_fill_manual(values= c("#CC6666","#66CC99"))+
  theme_bw() +
  theme(legend.position = "none") +
  labs(y="candidate publications", x = "") +
  coord_flip()

#ggsave("datasets.png")
```

\begin{center}
`r table_nums("datasets")` 
\end{center}
```{r, results = 'asis'}
datastats <- datastats %>%
  select(dataset, candidates_test, incl_test, inclrate_test) 
colnames(datastats) <- c("dataset", rep(c("Candidate publications", 
                                       #"Studies selected for fulltext screening", 
                                       "Relevant publications", 
                                       "Proportion relevant (%)"),1))
datastats <- datastats[c(2,3,4,1,5,6), ]
datastats[datastats == "Ace"] <- "ACE"
saveRDS(datastats, "tables/tab1_data.RDS")

print(xtable(datastats, digits = c(0,0,0,0,1)),
      include.rownames=FALSE, comment = FALSE, booktabs = TRUE,
      format.args = list(big.mark = ",", decimal.mark = "."))


# 
test <- xtable(datastats, digits = c(0,0,0,0,1))
# 
# library(pixiedust)
# dust(test) %>%
#    sprinkle(cols = 2, rows = 2, bold = TRUE) %>%
#    sprinkle_print_method("latex") %>%
#   print()
```


### Evaluating performance
Model performance was assessed by three different measures, Work Saved over Sampling (WSS), Relevant References Found (RRF), and Average Time to Discovery (ATD). 
WSS indicates the reduction in publications needed to be screened, at a given level of recall [@Cohen2006]. Typically measured at a recall level of 95%, WSS@95 yields an estimate of the amount of work that can be saved at the cost of failing to identify 5% of relevant publications. In the current study, WSS is computed at 95% recall. RRF@10 represents the proportion of relevant publications that are found after screening 10% of all publications.

Both RRF and WSS are sensitive to the position of the cutoff value and the distribution of the data.  Moreover, WSS makes assumptions about the acceptable recall level whereas this level might depend on the research question at hand [@OMara-Eves2015]. Therefore, we introduce the ATD, the average fraction of non-reviewed relevant publications during the review (except the relevant publications in the initial training set). The ATD is an indicator of performance throughout the entire screening process instead of performance at some arbitrary cutoff value. The ATD is computed by taking the average of the Time to Discovery (TD) of all relevant publications. The TD for a given relevant publication $i$ is computed as the fraction of publications needed to screen to detect $i$. Additional file 2 provides a detailed script to compute the ATD. 
<!-- proportion of publications needed to screen to find a relevant publication.  -->

Furthermore, model performance was visualized by plotting recall curves. Plotting recall as a function of the proportion of screened publications offers insight in model performance throughout the entire screening process [@Cormack2014; @Yu2018]. The curves give information in two directions. On the one hand they display the number of publications that need to be screened to achieve a certain level of recall, but on the other hand they present how many relevant publications are identified after screening a certain proportion of all publications (RRF).

For each simulation, the RRF@10, WSS@95, and ATD are reported as means over 15 trials. To indicate the spread of performance within simulations, the means are accompanied by an estimated standard deviation $\hat s$. To compare the overall performance across datasets, median performance is reported for every dataset, accompanied by the Median Absolute Deviation (MAD), indicating variability between models within a certain dataset. Recall curves are plotted for each simulation, representing the average recall over 15 trials $\pm$ the standard error of the mean. 

<!-- former footnote: -->
 <!-- The metrics for individual trials of one simulation deviate slightly from the overall mean for one simulation because of pre-averaging in the source code. As the analyses across all trials did not produce information on the 15 separate trials, the standard deviation of the mean was estimated by bootstrapping the standard deviation of the 15 trials. -->

# Results 
This section proceeds as follows: Firstly, as an example the results of the Nudging dataset are discussed in detail to provide a basis for answering the research questions. Secondly, the results are presented for each research question over all datasets. 


```{r, eval = TRUE}
# measures for averages over 15 runs 
results <- readRDS("../../results/output/results.RDS")
stabres <- readRDS("../../results/output/tabresults.RDS")
```

```{r}


# table for in manuscript

sdruns <- readRDS("../../results/output/sdruns.RDS")


nicetab <- function(results, statistic){
  test <- results %>% select(model, dataset, all_of(statistic))
  sdname <- paste0("sd", statistic)

  test <- left_join(test, sdruns[,c("model", "dataset", sdname)], by = c("model", "dataset"))
  
  test[,statistic] <- sprintf("%.1f", round(test[,statistic],1)) 
  test[,sdname] <-  sprintf("%.2f", round(test[,sdname],2)) 
  test$tab <- with(test, paste0(test[,statistic], " (", test[,sdname], ")"))
  
  tab <- test %>%
      select(model, dataset, tab) %>%
      pivot_wider(names_from = dataset, values_from = c("tab"))
  
  tab <- tab %>%
    select(model, nudging, ptsd, software, ace, virus, wilson)
  names(tab) <- c("", "Nudging", "PTSD", "Software", "ACE", "Virus", "Wilson")
  return(tab)
}

# rrf10tab <- tabruns %>%
#   select(dataset, model, mean_rrf10, sd_rrf10) %>%
#   pivot_wider(names_from = dataset, values_from = c("mean_rrf10","sd_rrf10"))
# 
# atdtab <- tabruns %>% 
#   select(dataset, model, mean_loss, sd_loss) %>%
#   pivot_wider(names_from = dataset, values_from = c("mean_loss","sd_loss"))
# tabruns %>%
#   select(model, starts_with(mean_wss95), starts_with(sd_wss95),
#          starts_with(mean_rrf10), starts_with(sd_rrf10), 
#          starts_with(mean_loss), starts_with(sd_loss))
```

## Evaluation on the Nudging dataset
Figure 1a shows the recall curves for all simulations on the Nudging dataset. As described in the previous section, these curves plot recall as a function of the proportion of publications screened. The curves represent the average recall over 15 trials $\pm$ the standard error of the mean in the direction of the y-axis. The x-axis is cut off at 40% since at this point in screening all models had already reached 95% recall. The dashed horizontal lines indicate the RRF@10 values, the dashed vertical lines the WSS@95 values. The dashed black diagonal line corresponds to the expected recall curve when publications are screened in a random order.
<!-- Desirable model performance was defined as maximizing recall while minimizing the number of publications needed to screen. -->

The recall curves were used to examine model performance throughout the entire screening process and to make a visual comparison between models within datasets. For example in Figure 1a, after screening about 30% of the publications all models had already found 95% of the relevant publications. Moreover, after screening 5% the green curve - representing the RF + TF-IDF model - splits away from the others and remains to be the lowest of all curves until about 30% of publications have been screened. Hence, from screening 5 to 30 percent of publications, the RF + TF- IDF model was the slowest in finding the relevant publications. The ordering of the remaining recall curves changes throughout the screening process, but maintain relatively similar performance at face value.

Figure 1b shows a subset of the recall curves in Figure 1a, namely the curves of the first four models to allow for a visual comparison across classification techniques adopting the TF-IDF feature extraction strategy. Figure 1c shows recall curves for the remaining three models to compare the models using D2V feature extraction. Figures 1d to 1f compare recall curves for models adopting the TF-IDF feature extraction strategy to recall curves for their D2V-using counterparts.

<!-- to allow for a comparison between models adopting TF-IDF and models adopting D2V feature extraction.  -->

```{r, eval = TRUE, fig-sub-nudging, fig.cap = "Recall curves of different models for the Nudging dataset, indicating how fast the model finds relevant publications during the process of screening publications. Figure a displays curves for all seven models at once. Figures b to f display curves for several subsets of those models to allow for a more detailed inspection of model performance.", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifier', "D2V vs TF-IDF for SVM classifier")}
# path
# nudgingplots <- list.files("../../results/one_seed/plots/nudging", full.names=TRUE)
f <- c("inclusion_all_nudging", "inclusion_tfidf_nudging", "inclusion_d2v_nudging", "inclusion_d2v_vs_tfidf_logistic_nudging", "inclusion_d2v_vs_tfidf_rf_nudging", "inclusion_d2v_vs_tfidf_svm_nudging")
nudgingplots <- paste0("../../results/one_seed/plots/nudging/", f, ".pdf")
include_graphics(nudgingplots)
```
<!-- is apparent that models adopting the TF-IDF feature extraction strategy are outperformed by their Doc2Vec-using counterparts:  -->
<!-- The best performing model in terms of ATD is NB + TF-IDF (9.25, 0.29), LR + TF-IDF (9.46, 0.19), SVM + TF-IDF (62.20),and RF + TF-IDF (11.6, 0.43). SVM + D2V (8.71, 0.33), LR + D2V (8.73, 0.47), RF + D2V (10.17, 0.88).  -->

<!-- provides the RRF@10 values for all model-dataset combinations.  -->
It can be seen from `r table_nums("atd", display = "cite")` that in terms of ATD, the best performing models on the Nudging dataset were SVM + D2V and LR + D2V, both with an ATD of 8.9%. This indicates that the average proportion of publications needed to screen to find a relevant publication was 8.9% for both models. In the SVM + D2V model, the standard deviation was 0.33, whereas for the LR + D2V model $\hat s =$ 0.47. This indicates that for the SVM + D2V model, the ATD values of individual trials were closer to the overall mean compared to the LR + D2V model, meaning that the SVM + D2V model performed more stable across different initial training datasets. Median ATD for this dataset was 9.6% with an MAD of 1.06, indicating that for half of the models, the ATD was within 1.06 percentage point distance from the median ATD. 

\begin{center}
`r table_nums("atd")`
\end{center}

```{r, results="asis", eval = FALSE}
tabatd <- nicetab(results, "loss") 
tabatd <- tabatd[c(7, 1, 5, 3, 6, 4, 2),]
# add range rows 
mad <- results %>% group_by(dataset) %>% summarise(median = sprintf("%.1f", round(median(loss), 1)), mad = sprintf("%.2f", round(mad(loss), 2)))

mad <- with(mad, paste0(median, " (", mad, ")"))

tabatd <- rbind(tabatd, (c("median (MAD)", mad[c(2:4, 1, 5,6)])))

saveRDS(tabatd, file = "tables/tab2_atd.RDS")

# print(xtable(tabatd, align = c("r", "r", rep("c", 6))), 
#       include.rownames=FALSE, comment = FALSE, booktabs = TRUE, hline.after = c(0,7))
```

\begin{table}[ht]
\centering
\begin{tabular}{rcccccc}
  & Nudging & PTSD & Software & ACE & Virus & Wilson \\ 
  \midrule
SVM + TF-IDF & 10.2 (0.19) & 2.1 (0.13) & 1.9 (0.04) & 7.3 (1.18) & 8.5 (0.17) & 4.2 (0.33) \\ 
  NB + TF-IDF & 9.4 (0.29) & 1.8 (0.11) & 1.5 (0.03) & \textbf{5.0 (0.53)} & \textbf{8.2 (0.22)} & \textbf{4.1 (0.37)} \\ 
  RF + TF-IDF & 11.8 (0.44) & 3.4 (0.27) & 2.0 (0.09) & 7.0 (0.76) & 10.6 (0.42) & 5.9 (1.20) \\ 
  LR + TF-IDF & 9.6 (0.19) & 1.7 (0.10) & \textbf{1.4 (0.02)} & 6.1 (1.20) & 8.4 (0.24) & 4.5 (0.34) \\ 
  SVM + D2V &  \textbf{8.9 (0.33)} & 2.1 (0.15) & \textbf{1.4 (0.05)} & 6.2 (0.34) & 8.5 (0.21) & 4.7 (0.31) \\ 
  RF + D2V & 10.4 (0.88) & 3.1 (0.34) & 1.6 (0.09) & 7.3 (1.29) & 9.3 (0.43) & 7.5 (1.56) \\ 
  LR + D2V & \textbf{8.9 (0.47)} &  \textbf{1.9 (0.17)} & \textbf{1.4 (0.04)} & 5.6 (0.18) & 8.4 (0.41) & 4.9 (0.32) \\ 
   \midrule
median (MAD) & 9.6 (1.06) & 2.1 (0.49) & 1.5 (0.12) & 6.2 (1.14) & 8.5 (0.18) & 4.7 (0.66) \\ 
  \end{tabular}
\end{table}

As `r table_nums("wss", display = "cite")` shows, the highest WSS@95 value on the Nudging dataset was achieved by the NB + TF-IDF model with a mean of 71.7%, meaning that this model reduced the number of publications needed to screen by 71.7% at the cost of losing 5% of relevant publications. The estimated standard deviation of 1.37 indicates that in terms of WSS@95, this model performed the most stable across trials. The model with the lowest WSS@95 value was RF + TF-IDF ($\bar x$ = 64.9%, $\hat s =$ 2.50). Median WSS@95 of these models was 66.9%, with a MAD of 3.05, indicating that of all datasets, the WSS@95 values of the models simulated on the Nudging dataset varied the most within the Nudging dataset.


\begin{center}
`r table_nums("wss")` 

```{r, eval = FALSE, results="asis"}
tabwss95 <- nicetab(results, "wss.95") 
tabwss95 <- tabwss95[c(7, 1, 5, 3, 6, 4, 2),]
# add range rows 
mad <- results %>% group_by(dataset) %>% summarise(median = sprintf("%.1f", round(median(wss.95), 1)), 
                                                   mad = sprintf("%.2f", round(mad(wss.95), 2)))
# insert mad 
mad <- with(mad, paste0(median, " (", mad, ")"))

tabwss95 <- rbind(tabwss95, (c("median (MAD)", mad[c(2:4, 1, 5,6)])))
# insert N per dataset 
saveRDS(tabwss95, file = "tables/tab3_wss95.RDS")

print(xtable(tabwss95, align = c("r", "r", rep("c", 6))), 
      include.rownames=FALSE, comment = FALSE, booktabs = TRUE, hline.after = c(0,7))


  # kable(format = "latex") %>%
  # kableExtra()
```
\end{center}

\begin{table}[ht]
\centering
\begin{tabular}{rcccccc}
  & Nudging & PTSD & Software & ACE & Virus & Wilson \\ 
  \midrule
SVM + TF-IDF & 66.2 (2.90) & 91.0 (0.41) & 92.0 (0.10) & 75.8 (1.95) & 69.7 (0.81) & 79.9 (2.09) \\ 
  NB + TF-IDF & \textbf{71.7 (1.37)} & \textbf{91.7 (0.27)} & \textbf{92.3 (0.08)} & \textbf{82.9 (0.99)} & \textbf{71.2 (0.62)} & 83.4 (0.89) \\ 
  RF + TF-IDF & 64.9 (2.50) & 84.5 (3.38) & 90.5 (0.34) & 71.3 (4.03) & 63.9 (3.54) & 81.6 (3.35) \\ 
  LR + TF-IDF & 66.9 (4.01) & \textbf{91.7 (0.18)} & 92.0 (0.10) & 81.1 (1.31) & 70.3 (0.65) & 80.5 (0.65) \\ 
  SVM + D2V & 70.9 (1.68) & 90.6 (0.73) & 92.0 (0.21) & 78.3 (1.92) & 70.7 (1.76) & 82.7 (1.44) \\ 
  RF + D2V & 66.3 (3.25) & 88.2 (3.23) & 91.0 (0.55) & 68.6 (7.11) & 67.2 (3.44) & 77.9 (3.43) \\ 
  LR + D2V & 71.6 (1.66) & 90.1 (0.63) & 91.7 (0.13) & 77.4 (1.03) & 70.4 (1.34) & \textbf{84.0 (0.77)} \\ 
   \midrule
median (MAD) & 66.9 (3.05) & 90.6 (1.53) & 92.0 (0.47) & 77.4 (5.51) & 70.3 (0.90) & 81.6 (2.48) \\ 
  \end{tabular}
\end{table}

As can be seen from the data in `r table_nums("rrf", display = "cite")`, LR + D2V was the best performing model in terms of RRF@10, with a mean of 67.5% indicating that after screening 10% of publications, on average 67.5% of all relevant publications had been identified, with a standard deviation of 2.59. The worst performing model was RF + TF-IDF ($\bar x =$ 53.6%, $\hat s =$ 2.71). Median performance was 62.6%, with an MAD of 3.89 indicating again that of all datasets, the RRF@10 values were most dispersed for models simulated on the Nudging dataset.

\begin{center}
`r table_nums("rrf")` 
\end{center}
```{r, results="asis", eval = FALSE}
tabrrf10 <- nicetab(results, "rrf.10") 

tabrrf10 <- tabrrf10[c(7, 1, 5, 3, 6, 4, 2),]
# add range rows 
mad <- results %>% group_by(dataset) %>% summarise(median = sprintf("%.1f", round(median(rrf.10), 1)), mad = sprintf("%.2f", round(mad(rrf.10), 2)))

mad <- with(mad, paste0(median, " (", mad, ")"))


tabrrf10 <- rbind(tabrrf10, (c("median (MAD)", mad[c(2:4, 1, 5,6)])))
saveRDS(tabrrf10, file = "tables/tab4_rrf10.RDS")

print(xtable(tabrrf10, align = c("r", "r", rep("c", 6))), 
      include.rownames=FALSE, comment = FALSE, booktabs = TRUE, hline.after = c(0,7))



```
<!-- (67.74, 2.40), followed by SVM + D2V (67.61, 2.82), NB + TF-IDF (65.59, 2.57), RF + D2V (62.83, 5.54), LR + TF-IDF (62.22, 2.72), SVM + TF-IDF (60.40, 3.29), AND RF + TF-IDF (53.74, 2.71) . -->

\begin{table}[ht]
\centering
\begin{tabular}{rcccccc}
  & Nudging & PTSD & Software & ACE & Virus & Wilson \\ 
  \midrule
SVM + TF-IDF & 60.2 (3.12) & 98.6 (1.40) & 99.0 (0.00) & 86.2 (5.25) & 73.4 (1.62) & 90.6 (1.17) \\ 
  NB + TF-IDF & 65.3 (2.61) & 99.6 (0.95) & 98.2 (0.34) & \textbf{90.5 (1.40)} & \textbf{73.9 (1.70)} & 87.3 (2.55) \\ 
  RF + TF-IDF & 53.6 (2.71) & 94.8 (1.60) & 99.0 (0.00) & 82.3 (2.75) & 62.1 (3.19) & 86.7 (5.82) \\ 
  LR + TF-IDF & 62.1 (2.59) & \textbf{99.8 (0.70)} & 99.0 (0.00) & 88.5 (5.16) & 73.7 (1.48) & 89.1 (2.30) \\ 
  SVM + D2V & 67.3 (3.00) & 97.8 (1.12) & \textbf{99.3 (0.44)} & 84.2 (2.78) & 73.6 (2.54) & \textbf{91.5 (4.16)} \\ 
  RF + D2V & 62.6 (5.47) & 97.1 (1.90) & 99.2 (0.34) & 80.8 (5.72) & 67.3 (3.19) & 75.5 (14.35) \\ 
  LR + D2V & \textbf{67.5 (2.59)} & 98.6 (1.40) & 99.0 (0.00) & 81.7 (1.81) & 70.6 (2.21) & 90.6 (5.00) \\ 
   \midrule
median (MAD) & 62.6 (3.89) & 98.6 (1.60) & 99.0 (0.00) & 84.2 (3.71) & 73.4 (0.70) & 89.1 (2.70) \\ 
  \end{tabular}
\end{table}

```{r, eval = FALSE}
stabres %>%
  select(model, starts_with("wss.95")) %>%
  flextable() %>%
  colformat_num(j = 2:7, big.mark = ",", digits = 2, na_str = "NA") %>%
  colformat_char(j=1) %>%
  set_header_labels(wss.95_ace = "ACE", wss.95_nudging = "Nudging", wss.95_ptsd = "PTSD", wss.95_software = "Software", wss.95_virus = "Virus", wss.95_wilson = "Wilson") %>%
  theme_booktabs() %>%
  autofit() 


stabres %>%
  select(model, starts_with("rrf.10")) %>%
  flextable() %>%
  colformat_num(j = 2:7, big.mark = ",", digits = 2, na_str = "NA") %>%
  colformat_char(j=1) %>%
    set_header_labels(rrf.10_ace = "ACE", rrf.10_nudging = "Nudging", rrf.10_ptsd = "PTSD", rrf.10_software = "Software", rrf.10_virus = "Virus", rrf.10_wilson = "Wilson") %>%
  theme_booktabs() %>%
  autofit()

stabres %>%
  select(model, starts_with("loss")) %>% 
  flextable() %>%
  colformat_num(j = 2:7, big.mark = ",", digits = 2, na_str = "NA") %>%
  colformat_char(j=1) %>%
  set_header_labels(loss_ace = "ACE", loss_nudging = "Nudging", loss_ptsd = "PTSD",loss_software = "Software", loss_virus = "Virus", loss_wilson = "Wilson") %>%
  theme_booktabs() %>%
  autofit() 
```


```{r, eval = FALSE}
atdrank <-
  results %>%
  group_by(dataset) %>%
  mutate(atd_rank = dense_rank(loss)) %>%
  select(dataset, model, loss, atd_rank)

# %>%
#   filter(model == "SVM + TF-IDF")

wssrank <- 
results %>%
  group_by(dataset) %>%
  mutate(wss95rank = dense_rank(-wss.95)) %>%
  select(dataset, model, wss95rank)

rrfrank <-
results %>%
  group_by(dataset) %>%
  mutate(rrf10rank = dense_rank(-rrf.10)) %>%
  select(dataset, model, rrf10rank)

tabruns %>%
  group_by(dataset) %>%
  mutate(atd_rank = dense_rank(mean_loss)) %>%
  filter(model == "RF + TF-IDF")

tabruns %>%
  mutate(wss95rank = dense_rank(-mean_wss95)) %>%
  filter(model == "RF + TF-IDF")

tabruns %>%
  mutate(rrf10rank = dense_rank(-mean_rrf10)) %>%
  filter(model == "RF + TF-IDF")

```


## Overall evaluation 
Recall curves for the simulations on the five remaining datasets are presented in Figure 2. For the sake of conciseness, recall curves are only plotted once per dataset, like in Figure 1a for the Nudging dataset. Please refer to Additional file 3 for figures presenting subsets of recall curves for the remaining datasets, like in Figure 1b-f.

First of all, as the recall curves exceed the expected recall at screening at random order by far for all datasets, the models were able to detect the relevant publications much faster compared to when screening publications at random order. Even the worst results outperform this reference condition. Across simulations, the ATD was at maximum 11.8% (in the Nudging dataset), the WSS@95 at least 63.9% (in the Virus dataset), and the lowest RRF@10 was 53.6% (in the Nudging dataset). Interestingly, all these values were achieved by the RF + TF-IDF model.

Similar to the simulations on the Nudging dataset (Figure 1a), the ordering of recall curves changes throughout the screening process, indicating that some models perform better at the start of the screening phase whereas others models take the lead later on.
<!-- model performance is dependent on the number of publications that have been screened.  -->
Moreover, the ordering of models in the Nudging dataset (Figure 1a) is not replicated in the remaining five datasets (Figure 2).

```{r, eval = TRUE, fig-sub-alldata, fig.cap = "Recall curves of all seven models for (a) the PTSD, (b) Software, (c) ACE, (d) Virus, and (e) Wilson dataset.", fig.subcap=c("PTSD","Software","ACE","Virus","Wilson")}
d <- c("ptsd", "software", "ace", "virus", "wilson")
files <- c("../../results/one_seed/plots/ptsd/inclusion_all_ptsd.pdf" ,paste0("../../results/one_seed/plots/", d[2:5], "/nolegend_inclusion_all_", d[2:5], ".pdf"))


include_graphics(files)
```


### RQ1 - Comparison across classification techniques
The first research question was aimed at evaluating the four models adopting either the NB, SVM, LR or RF classification technique combined with TF-IDF feature extraction. When comparing ATD-values of the models (`r table_nums("atd", display = "cite")`), the NB + TF-IDF model ranked first in the ACE, Nudging, PTSD, Virus and Wilson dataset, and second in the PTSD and the Software dataset in which the LR + TF-IDF model achieved the lowest ATD value. The RF + TF-IDF ranked last in all of the datasets except for the ACE dataset, in which the SVM + TF-IDF model achieved the highest ATD-value.

```{r, eval = FALSE}
atdrank %>% 
  filter(model %in% c("NB + TF-IDF", "RF + TF-IDF", "SVM + TF-IDF", "LR + TF-IDF")) %>% 
  arrange(dataset, atd_rank)
```

```{r, eval = FALSE}
wssrank %>% 
  filter(model %in% c("NB + TF-IDF", "RF + TF-IDF", "SVM + TF-IDF", "LR + TF-IDF")) %>% 
  arrange(dataset, wss95rank)
```

Additionally, in terms of WSS@95 (`r table_nums("wss", display = "cite")`) the ranking of models was strikingly similar across all datasets. In the ACE, Nudging, Software, and Virus dataset, the highest WSS@95 value was always achieved by the NB + TF-IDF model, followed by LR + TF-IDF, SVM + TF-IDF, and RF + TF-IDF. In the PTSD dataset this ranking applied as well, except that the LR + TF-IDF and NB + TF-IDF models showed the same WSS@95 value. The ordering of the models for the Wilson dataset was NB + TF-IDF, RF + TF-IDF, LR + TF-IDF and SVM + TF-IDF. 

```{r, eval = FALSE}
rrfrank %>% 
  filter(model %in% c("NB + TF-IDF", "RF + TF-IDF", "SVM + TF-IDF", "LR + TF-IDF")) %>% 
  arrange(dataset, rrf10rank)
```
Moreover, in terms of RRF@10 (`r table_nums("rrf", display = "cite")`) the NB + TF-IDF model achieved the highest RRF@10 value in the ACE, Nudging, and Wilson dataset. LR + TF-IDF ranked first in the PTSD dataset, SVM + TF-IDF was the best performing model within the Wilson dataset. The RF + TF-IDF model was again the worst performing model within all datasets, with an exception for the Software dataset. In this dataset, NB + TF-IDF ranked fourth, the remaining three models achieved an equal RRF@10 score.

Taken together, these results show that while all four models perform quite well, the NB + TF-IDF model demonstrates high performance on all measures across all datasets, whereas the RF + TF-IDF model never performed best on any of the measures across all datasets. 
<!-- The RF + TF-IDF model did not perform well on any of the datasets. This model was listed the lower half of the ranking across all datasets for both WSS@95, RRF@10^[With one exception for the RRF@10 in the Software dataset, however the range for this model-dataset combination is relatively small min(RRF@10) = 98.2, max(RRF@10) = 99.3.], and ATD, where the model ranked lowest within four datasets. -->

<!-- all datasets. Interestingly this model held it position when comparing ATD-values of all seven models at once, except for the Wilson dataset, where it ranked second, outperformed by   -->

<!-- the NB + TF-IDF belonged to the top four performing models in every dataset in terms of ATD, ranking first in the Wilson, Ace, and Virus dataset.  -->

<!-- For the PTSD dataset, the top performing classifiers are SVM + D2V (1.36, 0.05) AND LR + D2V () -->
<!-- For the Software dataset, models perform very similar with the -->

<!-- On the PTSD dataset  classification strategies had good performance as WSS@95%.  -->
<!-- No direct comparison between datasets can be made so we summarise model performance in terms of ranking. -->

### RQ2 - Comparison across feature extraction techniques
This section is concerned with the question of how models using different feature extraction strategies relate to each other. The recall curves for the Nudging dataset (Figure 1d-f) show a clear trend of the models adopting D2V feature extraction outperforming their TF-IDF counterparts. This trend also shows from the WSS@95 and RRF@10 values indicated by the vertical and horizontal lines in the figure. Likewise, the ATD values (`r table_nums("atd", display = "cite")`) indicate that for the models adopting a particular classification technique, the models adopting D2V feature extraction always achieved a lower ATD-value than the model adopting TF-IDF feature extraction. 

In contrast, this pattern of models adopting D2V outperforming their TF-IDF counterparts in the Nudging dataset is not replicated across other datasets. Whether evaluated in terms of recall curves, WSS@95, RRF@10, or ATD, the findings were mixed. Neither one of the feature extraction strategies showed superior performance within certain datasets nor within certain classification techniques. 

```{r, eval = FALSE}
wssrank %>% 
  filter(model != "NB + TF-IDF") %>% 
  arrange(dataset, wss95rank)

atdrank %>% 
  filter(model != "NB + TF-IDF") %>% 
  arrange(dataset, atd_rank)

atdrank %>% 
  arrange(model, atd_rank)

```

```{r, eval = FALSE}
wssrank %>% 
  arrange(wss95rank, model, dataset)

atdrank %>% 
  arrange(atd_rank, model, dataset)

rrfrank %>% 
  arrange(rrf10rank, model, dataset)

```

### RQ3 - Comparison across research contexts 
First of all, models showed much higher recall curves for some datasets than for others. While performance of the PTSD (Figure 2a) and Software datasets (Figure 2b) was quite high, performance was much lower across models for the Nudging (Figure 1a) and Virus (Figure 2d) datasets. The models simulated on the PTSD and Software datasets also demonstrated high performances in terms of the median ATD, WSS@95, and RRF@10 values for these models (Table 2, 3, and 4).

Secondly, variability of between-model performance differed across datasets. For the PTSD (Figure 2a), Software (Figure 2b), and the Virus (Figure 2d) datasets, recall curves form a tight group meaning that within these datasets, the models performed similarly In contrast, for the Nudging (Figure 1a), ACE (Figure 2c), and Wilson (Figure 2e) dataset, the recall curves are much further apart, indicating that model performance was more dependent on the adopted classification technique and feature extraction strategy. The MAD values of the ATD, WSS@95 and RRF@10 confirm that model performance is less spread out within the PTSD, Software, and Virus datasets than within the Nudging, ACE, and Wilson datasets. Moreover, the curves for the ACE (Figure 2c) and Wilson (Figure 2e) datasets show a larger standard error of the mean compared the other datasets. 

Taken together, although model performance is very data-dependent, there does not seem to be a distinction in performance between the datasets from the biomedical sciences (ACE, Virus, and Wilson) and datasets from other fields (Nudging, PTSD, and Software).
<!-- Thus, for these datasets model performance was more dependent on the initial training dataset compared to others. -->

<!-- For the PTSD dataset, WSS@95% ranges from 84.20% (RF + TF-IDF) to 93.75% (LR + TF-IDF).  -->

<!-- Overall these results indicate that first of all, model performance was mostly data dependent, however, the ordering of models is fairly similar.  -->
<!-- - the ordering of models is not the same in every dataset -->
<!-- - the spread between models performance differs over datasets, (high in ) -->
<!-- - social sciences vs medical sciences -->

<!-- #### plot:  -->
<!-- ## Overall Comparison -->
<!-- - models in general? -->
<!-- - which models perform better in which contexts? -->

<!-- all models perform better  -->
<!-- which models perform well overall.  -->

<!-- ranges in value: some datasets is matters more which models you choose, for some it matters less!  -->

<!-- #### across datasets  -->
<!-- For the SVM + TF-IDF model, WSS@95 ranged from 71.11 - 92.76 for the NB + TF-IDF model, WSS@95 ranged from 71.69 - 93.52, for -->


```{r, eval = FALSE}
metricsat <- c("WSS@95", "RRF@10", "ATD")
metrics <- c("WSS", "RRF", "ATD")
at <- c("@95", "@10", "")
Data <- c("ACE", "Nudging", "PTSD", "Software", "Virus", "Wilson")

typology <- data.frame(
  col_keys = names(stabres)[2:19],
  what = rep(Data, each = length(metrics)),
  #measure = rep(metricsat, each = 6),
  what = rep(c(rep("WSS@95"), rep("RRF@10"), rep("ATD")), 6),
  #measure = c(rep(c("@95", "@10", ""), each = 6)),
  stringsAsFactors = FALSE
)
num_keys <- names(stabres)[2:19]
test <- flextable(stabres)
### 

Data <- Data[1]
data <- "ace"
acetab <- select(stabres, 
                  model, 
                  ends_with(data))
typology <- data.frame(
  col_keys = names(acetab)[2:ncol(acetab)],
  what = rep(Data, each = length(metrics)),
  #measure = rep(metricsat, each = 6),
  what = rep(c(rep("WSS@95"), rep("RRF@10"), rep("ATD")), length(Data)),
  #measure = c(rep(c("@95", "@10", ""), each = 6)),
  stringsAsFactors = FALSE
)
num_keys <- names(acetab)[2:length(acetab)]
test <- flextable(acetab)
```

```{r, eval = FALSE}
# format numbers
test <- colformat_num(x = test, j = num_keys, big.mark = ",", digits = 2, na_str = "missing")
test <- colformat_char(x=test, j = "model")
# set all columns 
test <- set_header_df(test, mapping = typology, key = "col_keys")
test <- merge_h(test, part = "header")
#border_v = fp_border(color="purple")
#test <- border_inner_v(test, part="all", border = border_v )
test <- merge_v(test, part = "header")
test <- theme_booktabs(test)
test <- autofit(test, add_w = 0.2)
test <- fix_border_issues(test)
test <- flextable::align(test, align = "left", part = "header" )

plot(test)
```

# Discussion 
The current study evaluates the performance of active learning models for the purpose of identifying relevant publications in systematic review datasets. 
<!-- This study adds to existing literature by evaluating models across four different classification techniques and two different feature extraction strategies. Moreover, as previous studies have paid little attention to the generalizability of active learning models across different research contexts, models were evaluated across six systematic review datasets from various research areas.  -->
It has been one of the first attempts to examine different classification strategies and feature extraction strategies in active learning models for systematic reviews. Moreover, this study has provided a deeper insight into the performance of active learning models across research contexts. 

## Active learning-based screening prioritization
<!-- Overall, the findings confirm the great potential of active learning models to reduce the workload for systematic reviews.  -->
All models were able to detect 95% of the relevant publications after screening less than 40% of the total number of publications, indicating that active learning models can save more than half of the workload in the screening process. In a previous study, the ACE dataset was used to simulate a model that did not use active learning, finding a WSS@95 value of 56.61% [@Cohen2006], whereas the models in the current study achieved far superior WSS@95 values varying from 68.6% to 82.9% in this dataset. 
<!-- Active learning models clearly outperformed this model that did not use active learning.  -->
In another study [@Yu2018] that did use active learning, the Software dataset was used for simulation and a WSS@95 value of 91% was reached, strikingly similar to the values found in the current study which ranged from 90.5% to 92.3%. 

### Classification techniques
The first research question in this study sought to evaluate models adopting different classification techniques. The most important finding to emerge from these evaluations was that the NB + TF-IDF model consistently performed as one of the best models. Our results suggest that while SVM performed fairly well, the LR and NB classification techniques are good if not superior alternatives to this default classifier in software tools. Note that LR and NB were always good methods for text classification tasks [@Mitchell1997].
<!-- Across simulations the NB + TF-IDF model demonstrated high performance on  -->

### Feature extraction strategy 
The overall results on models adopting D2V versus TF-IDF feature extraction strategy remain inconclusive. According to our findings, models adopting D2V do not outperform models adopting the well-established TF-IDF feature extraction strategy. Given these results, preference goes out to the TF-IDF feature extraction technique as this relatively simple technique will lead to a model that is easier to interpret. Another advantage of this technique is its short computation time. 

### Research contexts 
Difficulty of applying active learning is not confined to any particular research area. The suggestion that active learning is more difficult for datasets from the social sciences compared to data from the medical sciences [@Miwa2014] does not seem to be the case.
<!-- Simulating models on a heterogenous collection of systematic review datasets revealed that model performance is very data-dependent. -->
A possible explanation for this is that this difficulty has to be attributed to factors more directly related to the systematic review at hand, such as the proportion of relevant publications or the complexity of inclusion criteria used to identify relevant publications [@Gates2018a; @Rathbone2015]. Although the current study did not investigate the inclusion criteria of systematic reviews, the datasets on which the active learning models performed worst, Nudging and Virus, were interestingly also the datasets with the highest proportion of relevant publications, 5.4% and 5.0%, respectively.
<!-- For some datasets, models achieved much higher overall performance than for other datasets. Moreover, within some datasets, differences between models were much larger than within other datasets.  -->
<!-- It has been suggested that active learning is more difficult for datasets from the social sciences compared to data from the medical sciences [@Miwa2014]. This does not appear to be the case as performance within the biomedical datasets was not in any way superior to performance within the datasets from the social sciences.  -->
<!-- An issue that emerges from our findings is that that difficulty of applying active learning is not confined to any particular research area.  -->
<!-- as differences in model performance could not be linked to non-biomedical datasets only from the biomedical areas (Nudging, Virus, and Wilson) did not perform did the datasets in which models pachieved highest performance  -->
<!-- although the nature of this study was rather exploratory, the results yield important insights on the ...  -->
<!-- Since models are more data dependent than model dependent (although for some datasets model performance varies more widely than for others). Maybe it doesn't really matter which model you pick although if you want to be save, the nb is a one size fits all model.  
## Strengths and limitations
<!-- The findings from this study have extened our knowledge on performance of different classification strategies We added to this emerging field of research by expanding model evaluations across classification techniques and feature extraction strategies. The findings from this study has extended our knowledge on  -->
<!-- what does this mean for reviewing?? save a lot of time ! -->
<!-- and if you've limited screening time then why not use such an prioritization model.  -->
<!-- This thesis adds to the existing knowledge on active learning models for sreening publications by evaluating  -->
<!-- We evaluted active learning models using established methods (certainty sampling, rebalancing, threw some new methods in the mix and evaluated them on heterogenous   -->
<!-- what did we add to other research  -->
<!-- We were able to reach WSS@95 of 63.9-91.7% (instead of 30-70 as stated in [@OMara-Eves2015]). Depending on the model-dataset combination.  -->
<!-- current study adds to this field by expanding -->
<!-- Across all classification techniques, feature extraction strategies and review contexts, active learning models exceeded screening at random order by far.  -->

## Limitations and future research
When applied to systematic reviews, the success of active learning models stands or falls with the generalizability of model performance across unseen datasets. In our study, is important to bear in mind that model hyperparameters were optimized for each model-dataset combination. Thus, the observed results reflect the maximum model performance for each presented datasets. The question remains whether model performance generalizes to datasets for which the hyperparameters are not optimized. Further research should be undertaken to determine the sensitivity of model performance to the hyperparameter values. 

Additionally, while the sample of datasets in the current study is diverse compared to previous studies, the sample size (n=6) does not allow for investigating how model performance relates to characteristics of the data, such as the proportion of relevant publications. To build more confidence in active learning models for screening publications, it is essential to identify how data characteristics affect model performance. Such a study requires more data on systematic reviews. Thus, a more thorough study depends on researchers to openly publish their systematic review datasets.

<!-- the datasets are by no means representative of the current body of systematic reviews.  -->

<!-- When models are applied in a real world setting there is no way to know of optimal parameter. Future research will focus on a cross-validation  -->
<!-- study did not seek to address results to differences in ... ?  -->

<!-- The current study was designed to detecting the final inclusions in a systematic review.  -->
<!-- It is interesting to note that studies use only abstract + title infor -->
<!-- Since the active learning models base their predictions only on metadata... -->
<!-- Screening publications in systematic reviews is typically a two-step process. First, titles and abstracts are screened to identify potentially relevant publications, called abstract inclusions. Second, the fulltexts of these publications are read to identify the relevant publications. This implies that the relevant publications are selected based on information that the models do not have. To truly assess the added value of active learning models in title-and-abstract screening, models should be evaluated on their capability of detecting the abstract inclusions instead of relevant publications only. However, this data is typically not available. Hence, greater efforts are needed to provide information on the abstract inclusions in openly published systematic review datasets.  -->

<!-- just exploratory, no fancy statistical analysis, however the goal was not to look for the best model. -->
<!-- strengths: -->

<!-- - open data -->
<!-- - different research areas -->
<!-- - different models on same dataset -->
<!-- - different datasets on same model -->

Moreover, the runtime of simulations varied widely across models, indicating that some models take longer to retrain after a publication has been labelled than other models. This has important implications for the practical application of such models, as an efficient model should be able to keep up with the decision-making speed of the reviewer. Further studies should take into account the retraining time of models. 

<!-- Now that we know that the potential of models/that the models show good performance, the next step is to identify factors that drive performance. When the only information available is an unlabeled dataset and we only want to try one model,  When applied in practice, it is  -->
<!-- - challenge is how to define BEFORE knowing the answers, what the model is.. In practice, knowing what the best mdoel is after labelling has no value whatsoever, the model + hyperparameters need to be defined a priori.  -->
<!-- - difficult to distinguish performance over datasets, especially when applied on a dataset of which no prior information is known (e.g. inclusions isn't known in practice).  -->


# Conclusions 
Overall, the findings confirm the great potential of active learning models to reduce the workload for systematic reviews. 
<!-- Overall, the findings of this study confirm that active learning models show great potential of retrieving relevant publications in a systematic review dataset, while minimizing the number of publications needed to screen.  -->
The results shed new light on the performance of different classification techniques, indicating that the NB classification technique is superior to the widely used SVM. As model performance differs vastly across datasets, this study raises the question which factors cause models to yield more workload savings for some systematic review datasets than for others. In order to facilitate the applicability of active learning models in systematic review practice, it is essential to identify how dataset characteristics relate to model performance. 

<!-- This study shows that active learning models are   -->
<!-- As all models exceeded screening at random order by far, they show great potential in reducing workload of systematic reviewers.  -->
<!-- of more precise estimate of  -->


# Declarations

### List of abbreviations
ATD - Average Time to Discovery

D2V - Doc2vec

LR - Logistic regression

MAD - Median Absolute Deviation

ML - Machine Learning

NB - Naive Bayes

PTSD - Post Traumatic Stress Disorder

RF - Random forest

RRF - Relevant References Found 

SD - Standard Deviation

SEM - Standard Error of the Mean

SVM - Support vector machines

TF-IDF - Term Frequency - Inverse Document Frequency

TPE - Tree of Parzen Estimators 

TD - Time to Discovery

WSS - Work Saved over Sampling

### Ethics approval and consent to participate
This study has been approved by the Ethics Committee of the Faculty of Social and Behavioural Sciences of Utrecht University, filed as an amendment under study 20-104. 

### Consent for publication
Not applicable.

### Availability of data and materials 
All data and materials are stored in the GitHub repository for this paper^[https://github.com/asreview/paper-evaluating-models-across-research-areas]. This repository contains all systematic review datasets used during this study and their preprocessing scripts, scripts for the hyperparameter optimization, the simulations, the processing and analysis of the results of the simulations, and for the figures and tables in this paper. The raw output files of the simulation study are stored on the Open Science Framework page of this paper, https://osf.io/7mr2g/ and https://osf.io/ag2xp/.

### Competing interests
The authors declare that they have no competing interests. 

### Funding
Access to the Cartesius supercomputer was granted by SURFsara (ID EINF-156). SURFsara had no role whatsoever in the design of the current study, nor in the data collection, analysis and interpretation, nor in writing the manuscript. 

### Author's contributions
RvdS, RS, JdB and GF designed the study. RS developed the DR balance strategy and ATD metric, and wrote the programs required for hyperparameter optimization and cloud computation. RS, JdB, and DO designed the architecture required for the simulation study. GF extracted and analyzed the data and drafted the manuscript. RvdS, AB, RS, DO, LT, and JdB assisted with writing the paper. LT, DO, AB, and RvdS provided domain knowledge. All authors read and approved the final manuscript.

## Acknowledgements
We are grateful for all researchers who have made great efforts to openly publish the data on their systematic reviews, special thanks go out to Rosanna Nagtegaal.
<!-- \newpage -->


<!-- # Appendix A - list of definitions -->

<!-- ### Feature Extraction Strategies  -->
<!-- split_ta = overall hyperparameter  -->

<!-- #### TF-IDF -->

<!-- ##### hyperparameters  -->
<!--     ngram_max: int -->
<!--             Can use up to ngrams up to ngram_max. For example in the case of -->
<!--             ngram_max=2, monograms and bigrams could be used. -->


<!-- #### Doc2vec -->
<!-- Predicts words from context.  -->
<!-- Aims at capturing the relations between word (man-woman, king-queen). -->
<!-- [@Le2014]. Using a neural network.  -->

<!-- using Continuous Bag-of-Words (CBOW), Skip-Gram model, .... -->
<!-- Word vector _W_ and extra: document vector _D_, trained to predict words in the text. -->


<!-- From gensim [@Rehurek2010]. -->

<!--         Arguments -->
<!--         --------- -->
<!--         vector_size: int -->
<!--             Output size of the vector. -->
<!--         epochs: int -->
<!--             Number of epochs to train the Doc2vec model. -->
<!--         min_count: int -->
<!--             Minimum number of occurences for a word in the corpus for it to -->
<!--             be included in the model. -->
<!--         workers: int -->
<!--             Number of threads to train the model with. -->
<!--         window: int -->
<!--             Maximum distance over which word vectors influence each other. -->
<!--         dm_concat: int -->
<!--             Whether to concatenate word vectors or not. -->
<!--             See paper for more detail. -->
<!--         dm: int -->
<!--             Model to use. -->
<!--             0: Use distribute bag of words (DBOW). -->
<!--             1: Use distributed memory (DM). -->
<!--             2: Use both of the above with half the vector size and concatenate -->
<!--             them. -->
<!--         dbow_words: int -->
<!--             Whether to train the word vectors using the skipgram metho -->


<!-- #### SBERT -->

<!-- BERT-base model with mean-tokens pooling [@Reimers2019] -->

<!-- #### embeddingIdf  -->
<!-- This model averages the weighted word vectors of all the words in the text, -->
<!-- in order to get a single feature vector for each text. The weights are provided by the inverse document frequencies -->

<!-- ### Models  -->
<!-- #### Naive Bayes  -->
<!-- Naive Bayes assumes all features are independent given the class value. [@Zhang2004]  -->

<!-- ASReview uses the ` MultinomialNB` from the scikit-learn package [@scikit-learn], that implements the naive Bayes algorithm for multinomially distributed data.  -->
<!-- ` nb` -->

<!-- Hyperparameters -->

<!--   * alpha - accounts for features not present in learning samples and prevents zero probabilities in further computations.  -->

<!-- #### Random Forests -->
<!-- A number of decision trees are fit on bootstrapped samples of the original data,  -->
<!-- [@Breiman2001] -->
<!-- RandomForestClassifier from sklearn  -->

<!-- Arguments -->
<!--         --------- -->
<!--         n_estimators: int -->
<!--             Number of estimators. -->
<!--         max_features: int -->
<!--             Number of features in the model. -->
<!--         class_weight: float -->
<!--             Class weight of the inclusions. -->
<!--         random_state: int, RandomState -->
<!--             Set the random state of the RNG. -->
<!--         """ -->

<!-- #### Support Vector Machine -->


<!--  Arguments -->
<!--         --------- -->
<!--         gamma: str -->
<!--             Gamma parameter of the SVM model. -->
<!--         class_weight: -->
<!--             class_weight of the inclusions. -->
<!--         C: -->
<!--             C parameter of the SVM model. -->
<!--         kernel: -->
<!--             SVM kernel type. -->
<!--         random_state: -->
<!--             State of the RNG. -->

<!-- #### Logistic Regression  -->

<!-- #### __Dense Neural Network__ -->

<!-- ### Query Strategies  -->
<!-- * Max - Choose the most likely samples to be included according to the model -->
<!-- * Uncertainty - choose the most uncertain samples according to the model (i.e. closest to 0.5 probability) [@Lewis1994]  -->
<!-- * Random - randomly selects abstracts with no regard to model assigned probabilities.  -->
<!-- * Cluster - Use clustering after feature extraction on the dataset. Then the highest probabilities within random clusters are sampled -->

<!-- The following combinations are simulated:  -->

<!-- * cluster -->
<!-- * max -->
<!-- * cluster * random  -->
<!-- * cluster * uncertainty -->
<!-- * max * cluster -->
<!-- * max * random -->
<!-- * max * uncertainty -->

<!-- ### Balance Strategies  -->

<!-- ### amount of training data -->

<!-- * n_instances = number of papers queried each query -->
<!-- * n_queries = number of queries -->
<!-- * n_prior_included: 5 -->
<!-- * n_prior_excluded: -->

<!-- # Combinations  -->

<!-- This leads to  `r nrow(combins)` combinations of configurations.  -->

<!-- - Naive bayes only goes with tfidf feature extraction. -->
<!-- - For the feature extraction strategies we will focus on Doc2vec and tfidf. (but will compute all 4) -->
<!-- - This leads to 3 * 7  * 4 * 3 + 1 * 7 * 1 * 3 =  273 combinations.  -->

<!-- See appendix A for a table containing all 273 combinations.  -->


<!-- ## Cross-validation -->
<!-- Should give an accurate estimate of maximum performance / future systematic reviews to be performed.  -->


<!-- # Appendix B - combinations -->

<!-- ```{r} -->
<!-- names(combins) <- c("Model", "Query Strategy", "Feature extraction strategy") #, "Training data [included/excluded]") -->
<!-- combins %>% -->
<!--   kable(row.names = FALSE, format = "latex", longtable = TRUE, booktabs = TRUE) %>% -->
<!--   kable_styling(latex_options = c("repeat_header"))  -->
<!-- ``` -->


<!-- # Appendix C - supercomputer Cartesius -->

<!-- 500,000 SBU -->

<!-- Running on Cartesius is charged in System Billing Units (SBUs), and charging is based on the wall clock time of a job. On fat and thin nodes, an SBU is equal to using 1 core for 1 hour (a core hour), or 1 core for 20 minutes on a GPU node. Since compute nodes are allocated exclusively to a single job at a time, you will be charged for all cores on that node - even if you are using less.  -->

<!-- In the current study, the classifier and the feature extraction strategy are varied, whereas the query and balance strategy remain fixed.  -->
<!-- In the current study only a fraction of all possible configurations are tested for the sake of brevity.  -->
<!-- There are many more options available and open to exploration. -->
\nolinenumbers

# References

<div id="refs"></div>
# Additional file 1
<!-- Input: -->
<!--     -  -->


<!-- Output: Train, a balanced training dataset, supersampling inclusions depending on the number of exclusions and total number of publications that have been labeled. -->

<!-- Parameters: -->
<!-- a - Governs the weight of the 1's. Higher values mean linearly more 1's -->
<!--     in your training sample. -->
<!-- alpha - Governs the scaling the weight of the 1's, as a function of the -->
<!--     ratio of ones to zeros. A positive value means that the lower the -->
<!--     ratio of zeros to ones, the higher the weight of the ones. -->
<!-- b - Governs how strongly we want to sample depending on the total -->
<!--     number of samples. A value of 1 means no dependence on the total -->
<!--     number of samples, while lower values mean increasingly stronger -->
<!--     dependence on the number of samples. -->
<!-- beta - Governs the scaling of the weight of the zeros depending on the -->
<!--     number of samples. Higher values means that larger samples are more -->
<!--     strongly penalizing zeros. By default 1. -->

<!-- ------------ -->

<!-- resample(read, a, alpha, b, beta) -->
<!--     # characteristics of the already labeled publications -->
<!--     n_read = len(read) -->
<!--     one_idx = where(read == 1) -->
<!--     zero_idx = where(read == 0) -->
<!--     n_one = len(one_idx) -->
<!--     n_zero: len(zero_idx) -->
<!--     n_train = n_one + n_zero -->
<!--     # Compute the weights. -->
<!--     one_weight =  a * (n_one / n_zero)**(-alpha) -->
<!--     zero_weight = weight = 1 - (1 - b) * (1 + log(n_read))**(-beta) -->
<!--     tot_zo_weight =  one_weight * n_one + zero_weight * n_zero -->

<!--     # number of inclusions to sample -->
<!--     n_one_train = one_weight * n_one * n_train / tot_zo_weight -->
<!--     # at least one inclusion, but always leave two spots for exclusions -->
<!--     n_one_train = max(1, min(n_train - 2, n_one_train)) -->
<!--     # the spots that are left are for exclusions -->
<!--     n_zero_train = n_train - n_one_train -->

<!--     # sample inclusions and exclusions -->
<!--     one_train_idx = sample(n_one_train, one_idx) -->
<!--     zero_train_idx = sample(n_zero_train, zero_idx) -->

<!--     train = append(one_train_idx, zero_train_idx) -->
<!--     return(shuffle(train)) -->


<!-- # Copy/sample until there are n_train indices sampled -->
<!-- sample(n_x_train, idx): -->
<!--     # n x train = desired size of training sample, nxread = how much you've read -->
<!--     # number of copies needed, rounded down. -->
<!--     n_copy = n_x_train / len(idx) -->
<!--     # for the remainder, sample -->
<!--     n_sample = n_x_train % len(idx) -->
<!--     # paste idx n_copy times -->
<!--     x_train = tile(idx, n_copy) -->
<!--     # add sample -->
<!--     x_train = append(x_train, random_sample(idx, n_sample)) -->
<!--     return(n_x_train) -->


<!-- \begin{algorithm}[H] -->
<!-- \DontPrintSemicolon -->
<!-- \SetAlgoLined -->
<!-- \KwResult{} -->
<!-- \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output} -->
<!-- \Input{read, all publications that have been labeled, a, alpha and b.} -->
<!-- \Output{train = A balanced training dataset} -->
<!-- \BlankLine -->
<!-- # characteristics of the already labeled publications -->
<!--     n_read = len(read) -->
<!--     one_idx = where(read == 1) -->
<!--     zero_idx = where(read == 0) -->
<!--     n_one = len(one_idx) -->
<!--     n_zero: len(zero_idx) -->
<!--     n_train = n_one + n_zero -->
<!--     # Compute the weights. -->
<!--     one_weight =  a * (n_one / n_zero)**(-alpha) -->
<!--     zero_weight = weight = 1 - (1 - b) * (1 + log(n_read))**(-beta) -->
<!--     tot_zo_weight =  one_weight * n_one + zero_weight * n_zero -->

<!-- \While{While condition}{ -->
<!--     instructions\; -->
<!--     \eIf{condition}{ -->
<!--         instructions1\; -->
<!--         instructions2\; -->
<!--     }{ -->
<!--         instructions3\; -->
<!--     } -->
<!-- } -->
<!-- \caption{Dynamic Resampling} -->
<!-- \end{algorithm} -->

\newpage

# Additional file 2

\newpage

# Additional file 3
Recall curves plot separately for the PTSD, Software, ACE, Virus and Wilson datasets. 

## PTSD
```{r, eval = TRUE, fig-sub-ptsd, fig.cap = "Recall curves for the PTSD dataset. ", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifier', "D2V vs TF-IDF for SVM classifier") }
# path
f <- c("inclusion_all_", "inclusion_tfidf_", "inclusion_d2v_", "inclusion_d2v_vs_tfidf_logistic_", "inclusion_d2v_vs_tfidf_rf_", "inclusion_d2v_vs_tfidf_svm_")

plots <- paste0("../../results/one_seed/plots/ptsd/", f, "ptsd.pdf")
include_graphics(plots)
```

## Software
```{r, fig-sub-software, fig.cap = "Recall curves for the Software dataset. ", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifier', "D2V vs TF-IDF for SVM classifier") }
# path
plots <- paste0("../../results/one_seed/plots/software/", f, "software.pdf")
include_graphics(plots)
```

## ACE
```{r, fig-sub-ace, fig.cap = "Recall curves for the ACE dataset. ", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifier', "D2V vs TF-IDF for SVM classifier") }
# path
plots <- paste0("../../results/one_seed/plots/ace/", f, "ace.pdf")
include_graphics(plots)
```

## Virus
```{r, fig-sub-virus, fig.cap = "Recall curves for the Virus dataset. ", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifaier', "D2V vs TF-IDF for SVM classifier") }
# path
plots <- paste0("../../results/one_seed/plots/virus/", f, "virus.pdf")
include_graphics(plots)

```

## Wilson
```{r, eval = TRUE, fig-sub-wilson, fig.cap = "Recall curves for the Wilson dataset. ", fig.subcap=c('All seven models',  "Models adopting TF-IDF feature extraction", "Models adopting D2V feature extraction",'D2V vs TF-IDF for LR classifier', 'D2V vs TF-IDF for RF classifier', "D2V vs TF-IDF for SVM classifier") }
# path
plots <- paste0("../../results/one_seed/plots/wilson/", f, "wilson.pdf")
include_graphics(plots)
```

