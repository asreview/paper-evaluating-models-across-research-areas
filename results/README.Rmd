---
title: "Processing simulation output"
output: github_document
bibliography: ../manuscript/manuscript/asreview.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjson)
library(tidyverse)
library(knitr)
library(flextable)
library(officer)

set.seed(42)
```

This directory stores all files used to extract figures and statistics from the simulation study output.
The directory contains the following:

- `extract_plots.ipynb`, a jupyter notebook producing recall curves from the simulation output (Figure 1 and 2 in the manuscript)
- `extract_results.ipynb`, a jupyter notebook extracting `.json` files containing statistics from the `.h5` simulation output (WSS, RRF, ATD in table 2, 3, and 4 in the manuscript)
- `one_seed`, containing all plots and `.json` statistics files produced by the two jupyter notebooks above. The plots are used in the manuscript, the data are further processed by this readme into tables before they are manuscript-ready.
- `README.Rmd` containing R-code to transform the `.json` files into readable tables for the manuscript.
- `output` contains the abovementioned tables, stored as `.RDS` files.  
- the `datastats.RDS` is used in the analyses to compute adjusted ATD values (see code below)

# Requirements
Extracing data from the simulation output requires having several packages installed, like ASReview version 0.9.3 [@ASReview2020]. All these requirements are listed in the `requirements.txt` file. If you've already installed this file in the `simulation_study` step, please skip this. If not, you can run the following in your terminal to install all requirements: 

```{bash, eval = FALSE}
pip install -r ../simulation_study/requirements.txt
```

Additionally, to create the plots and statistics in the manuscript you will need to install a specific branch of the asreview visualization package. Run the following in your terminal: 

```{bash, eval = FALSE}
# clone visualization package from GitHub
git clone https://github.com/GerbrichFerdinands/asreview-thesis-visualization.git
```

And then, within the newly created directory, the following:

```{bash, eval = FALSE}
# install visualization package 
pip install . 
```

# Reproduce data extraction
To reproduce the results, follow the steps below. If you do not want to download the raw simulation output, start at step 3:

1. Run all code in the `extract_plots.ipynb` notebook to create all plots. This requires having the raw simulation data, to be found on the OSF (https://osf.io/7mr2g/ and https://osf.io/ag2xp/). Note that you will need to adjust the paths to where you've stored the simulation output on your local computer. Also, note that creating figures can take quite some time, depending on your computer. Mine took from 30 minutes to 5 hours per figure The final figures can be found in the `one_seed/plots` directory. 
2. Run all code in the `extract_results.ipynb` to extract the metrics WSS, RRF and ATD from the raw simulation data. Note that you will need to adjust the paths to where you've stored the simulation output on your local computer. Also, note that extracting all results will take quite some time, depending on your computer. Mine took 48 hours. The results are stored in the `one_seed/plots` directory. 
3. Follow the preprocessing steps in the `README.Rmd` files to create tables for in the manuscript, stored in the `output` directory. 

## Define functions for reading simulation output 
```{r}
data <- readRDS("../simulation_study/R/00_datasets.RDS")
models <-c("BCTD", "LCDD", "LCTD", "RCDD", "RCTD", "SCDD", "SCTD")
names(models) <- c("NB + TF-IDF", "LR + D2V", "LR + TF-IDF", "RF + D2V", "RF + TF-IDF", "SVM + D2V", "SVM + TF-IDF" )

# function that reads results for all 15 runs
read_results <- function(m){
   files = list.files(paste0("one_seed/statistics/", m), pattern = "all.json", recursive = TRUE)
  # names of the files are the data
  names(files) <- str_split(files, "/", simplify = TRUE)[,1]
  # read data
  dat <- lapply(files, function(x) fromJSON(file = paste0("one_seed/statistics/", m, "/", x)))
  
  # extract wss, rrf, and loss
  dat <- map(dat, `[`, c("wss", "rrf", "loss"))

  # transorm into dataframe
  dat <- map_dfr(dat, ~ as.data.frame(.x), .id = "dataset")
  
  # add model name
  dat <- dat %>% 
    mutate(model = names(models[models == m]))

  return(dat)
}

# function for extracting all separate runs 
read_trials <- function(m){
   files <- list.files(paste0("one_seed/statistics/", m), pattern = "results_", recursive = TRUE, full.names=TRUE)
  # names of the files are the data
  names(files) <- str_split(files, "/", simplify = TRUE)[,4]
  
  # read data
  dat <- lapply(files, function(x) fromJSON(file = x))
  
  # extract wss, rrf, and loss
  dat <- map(dat, `[`, c("wss", "rrf", "loss"))

  # transorm into dataframe
  dat <- map_dfr(dat, ~ as.data.frame(.x), .id = "dataset")
  
  # add model name
  dat <- dat %>% 
    mutate(model = names(models[models == m]))

  return(dat)
}
```

The ATD values need to be adjusted for prior inclusions and exclusions as the computation does not take into account that prior to the active learning cycle, already 1 inclusion and 1 exclusion have been labelled 'for free'. 
```{r}
# ATD needs to be adjusted for 1 prior inclusion and 1 prior exclusion! 
# compute adjusted ATD to make it equal to the area above the curve
datastats <- readRDS("datastats.RDS") %>%
  select(Dataset, candidates_test, incl_test) %>%
  # account for 1 prior exclusion and 1 prior inclusion
  mutate(n_1 = incl_test, n_excl = candidates_test-n_1, n_1_noprior = incl_test-1, candidates_noprior = candidates_test -1) %>%
  mutate(ratio = (n_1_noprior/n_1)/(candidates_noprior/candidates_test))

datastats$dataset <- c("ace", "nudging", "ptsd", "software", "virus", "wilson")
atdratio <- datastats %>%
  select(dataset, ratio)
```

## Load results for 15 separate trials
```{r}
# read all 15 trials separately
runs <- lapply(models, FUN = read_trials)

# all in one dataframe
runs <- do.call("rbind", runs)

# convert loss (ttd) to percentage 
runs$loss <- runs$loss*100
# adjust loss to N_1 : N_1-1 (to the prior inclusions)
runs <- left_join(runs, atdratio, by = "dataset")
runs <- runs %>%
  mutate(loss = loss/ratio) %>%
  select(-ratio)

# save results file 
saveRDS(runs, "output/runs.RDS")
```

Compute standarad deviation from the 15 separate trials.
```{r}
# compute standard deviation (bootstrapped)
sdruns <- 
  runs %>%
  select(dataset, model, wss.95, rrf.10, loss) %>% 
  group_by(model, dataset) %>%
  summarise(sdwss.95 = sd(wss.95),
            sdrrf.10 = sd(rrf.10),
            sdloss = sd(loss))
library(boot)
# but now boostrap
sdruns <- 
  runs %>%
  select(dataset, model, wss.95, rrf.10, loss) %>% 
  group_by(model, dataset) 

# for ace/nb+tf-idf
test <- sdruns %>% 
  ungroup()%>%
  filter(dataset == "ace", model == "NB + TF-IDF") 

samplemean <- function(x, d) {
  wss <- x[d,1]$wss.95
  rrf <- x[d,2]$rrf.10
  atd <- x[d,3]$loss
  result <- c(mean(wss), mean(rrf), mean(atd))
  return(result)
}

samplesd <- function(x,d){
  # indices 
  wss <- x[d,1]$wss.95
  rrf <- x[d,2]$rrf.10
  atd <- x[d,3]$loss
  result <- c(sd(wss), sd(rrf), sd(atd))
  return(result)
}

test <- tibble(test %>% select(wss.95, rrf.10, loss))
         
bs <- boot(test,statistic = samplemean, R = 1000)
bs <- boot(test, statistic = samplesd, R=1000)
bs# standard deviation 
sd(bs$t[,1])

samples <- bs$t


saveRDS(sdruns, "output/sdruns.RDS")

var <- apply(samples,2,mean)^2 - (apply(samples,2,mean))^2 

ms <- samples[,1]
ms <- samples[,2]
ms <- samples[,3]

#samples = de gemiddelde ATD van 1000 resamples (1000 waardes dus)
lhs <- sum(samples^2)/1000
rhs <- mean(samples)^2
# standard deviation estimated by bootstrap
sqrt(lhs - rhs)

# variance in means
# but I want standard deviation within samples estimated

samplesd <- function(x,d){
  # indices 
  wss <- x[d,1]$wss.95
  rrf <- x[d,2]$rrf.10
  atd <- x[d,3]$loss
  result <- c(sd(wss), sd(rrf), sd(atd))
  return(result)
}


```

## Load results as means over all 15 trials  
```{r}
# extract results for all models
# list for models separately
results <- lapply(models, read_results)

# all in one dataframe
results <- do.call("rbind", results)

# convert loss (ttd) to percentage 
results$loss <- results$loss*100
# adjust loss to N_1 : N_1-1
results <- left_join(results, atdratio, by = "dataset")

results <- results %>%
  mutate(loss = loss/ratio) %>%
  select(-ratio)

# save results file 
saveRDS(results, "output/results.RDS")
```

Create table for manuscript (all means over 15 runs)
```{r, echo = FALSE, results = "asis"}
tabres <- 
  results %>%
  pivot_wider(names_from = dataset, values_from = c("wss.95", "wss.99", "wss.100", "rrf.5", "rrf.10", "rrf.20", "loss"))

# table of mean statistics over all runs 
stabres <- 
tabres %>%
  # select statistics
  select(model,
         starts_with("wss.95"),
         starts_with("rrf.10"), 
         starts_with("loss")) %>%
  # reorder datasets
  select(model,
         ends_with("nudging"),
         ends_with("ptsd"),
         ends_with("software"),
         ends_with("ace"),
         ends_with("virus"),
         ends_with("wilson")
         )

saveRDS(stabres, "output/tabresults.RDS")

knitr::kable(stabres, format = "markdown", digits = 1)
```

# References